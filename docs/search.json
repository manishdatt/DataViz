[
  {
    "objectID": "posts/xkcd_colors/xkcd_colors.html",
    "href": "posts/xkcd_colors/xkcd_colors.html",
    "title": "The xkcd color survey",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport colorsys\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import pairwise_distances_argmin_min\nimport textwrap\n\n\nanswers = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/answers.csv')\ncolor_ranks = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/color_ranks.csv')\nusers = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/users.csv')\n\n\nanswers\n\n\n\n\n\n\n\n\nuser_id\nhex\nrank\n\n\n\n\n0\n1\n#8240EA\n1\n\n\n1\n2\n#4B31EA\n3\n\n\n2\n2\n#584601\n5\n\n\n3\n2\n#DA239C\n4\n\n\n4\n2\n#B343E5\n1\n\n\n...\n...\n...\n...\n\n\n1058206\n152397\n#7238F0\n1\n\n\n1058207\n152398\n#8E14CD\n1\n\n\n1058208\n152398\n#0A49E7\n3\n\n\n1058209\n152400\n#38A30E\n2\n\n\n1058210\n152401\n#4D004B\n1\n\n\n\n\n1058211 rows × 3 columns\n\n\n\n\nanswers['rank'].unique()\n\narray([1, 3, 5, 4, 2])\n\n\n\ncolor_ranks\n\n\n\n\n\n\n\n\ncolor\nrank\nhex\n\n\n\n\n0\npurple\n1\n#7e1e9c\n\n\n1\ngreen\n2\n#15b01a\n\n\n2\nblue\n3\n#0343df\n\n\n3\npink\n4\n#ff81c0\n\n\n4\nbrown\n5\n#653700\n\n\n...\n...\n...\n...\n\n\n944\nfresh green\n945\n#69d84f\n\n\n945\nelectric lime\n946\n#a8ff04\n\n\n946\ndust\n947\n#b2996e\n\n\n947\ndark pastel green\n948\n#56ae57\n\n\n948\ncloudy blue\n949\n#acc2d9\n\n\n\n\n949 rows × 3 columns\n\n\n\n\ndef hex_to_hsl(hex_color):\n    rgb = colors.to_rgb(hex_color)  # Returns RGB as floats (0-1)\n    h, l, s = colorsys.rgb_to_hls(*rgb)  # Note: HLS order\n#    return round(h * 360, 1), round(s * 100, 1), round(l * 100, 1)\n    return h, s, l\n\nprint(hex_to_hsl(\"#ff5733\"))\n\n(0.02941176470588236, 1.0, 0.6)\n\n\n\n# merge answers and color_rank based on rank column\nmerged_data = pd.merge(answers, color_ranks, on='rank', how='left')\nmerged_data[['h', 's', 'l']] = merged_data['hex_x'].apply(hex_to_hsl).apply(pd.Series)\n\n\nmerged_data\n\n\n\n\n\n\n\n\nuser_id\nhex_x\nrank\ncolor\nhex_y\nh\ns\nl\n\n\n\n\n0\n1\n#8240EA\n1\npurple\n#7e1e9c\n0.731373\n0.801887\n0.584314\n\n\n1\n2\n#4B31EA\n3\nblue\n#0343df\n0.690090\n0.814978\n0.554902\n\n\n2\n2\n#584601\n5\nbrown\n#653700\n0.132184\n0.977528\n0.174510\n\n\n3\n2\n#DA239C\n4\npink\n#ff81c0\n0.889800\n0.723320\n0.496078\n\n\n4\n2\n#B343E5\n1\npurple\n#7e1e9c\n0.781893\n0.757009\n0.580392\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1058206\n152397\n#7238F0\n1\npurple\n#7e1e9c\n0.719203\n0.859813\n0.580392\n\n\n1058207\n152398\n#8E14CD\n1\npurple\n#7e1e9c\n0.776577\n0.822222\n0.441176\n\n\n1058208\n152398\n#0A49E7\n3\nblue\n#0343df\n0.619155\n0.917012\n0.472549\n\n\n1058209\n152400\n#38A30E\n2\ngreen\n#15b01a\n0.286353\n0.841808\n0.347059\n\n\n1058210\n152401\n#4D004B\n1\npurple\n#7e1e9c\n0.837662\n1.000000\n0.150980\n\n\n\n\n1058211 rows × 8 columns\n\n\n\n\nmerged_data.groupby('color').count()\n\n\n\n\n\n\n\n\nuser_id\nhex_x\nrank\nhex_y\nh\ns\nl\n\n\ncolor\n\n\n\n\n\n\n\n\n\n\n\nblue\n288015\n288015\n288015\n288015\n288015\n288015\n288015\n\n\nbrown\n75812\n75812\n75812\n75812\n75812\n75812\n75812\n\n\ngreen\n314172\n314172\n314172\n314172\n314172\n314172\n314172\n\n\npink\n131013\n131013\n131013\n131013\n131013\n131013\n131013\n\n\npurple\n249199\n249199\n249199\n249199\n249199\n249199\n249199\n\n\n\n\n\n\n\n\nf\"{merged_data.groupby('color')['hex_x'].nunique().loc['blue']:,}\"\n\n'275,337'\n\n\n\ncluster_results = {}\nfor color, group_df in merged_data.groupby('color'):\n    kmeans = KMeans(n_clusters=100, random_state=2025)\n    kmeans.fit(group_df[['h', 's', 'l']])\n\n    # Store the result: labels and cluster centers\n    cluster_results[color] = {\n        'labels': kmeans.labels_,\n        'centers': kmeans.cluster_centers_,\n        'data': group_df.copy()\n    }\n#    cluster_results[color]['data']['cluster'] = kmeans.labels_\n\n#print(cluster_results[\"blue\"][\"data\"])\n\nfor color in cluster_results:\n    group_data = cluster_results[color]['data']\n    centers = cluster_results[color]['centers']\n    closest_idxs, _ = pairwise_distances_argmin_min(centers, group_data[['h', 's', 'l']].values)\n    closest_points = group_data.iloc[closest_idxs]\n    cluster_results[color]['closest_points'] = closest_points\n\n\nprint(cluster_results['blue']['closest_points'][['h', 's', 'l']].head(10).values)\n\n[[0.56581741 0.74407583 0.58627451]\n [0.6374269  0.89528796 0.3745098 ]\n [0.50673401 0.52380952 0.62941176]\n [0.59259259 0.54418605 0.42156863]\n [0.66666667 0.88516746 0.59019608]\n [0.66333333 0.20661157 0.4745098 ]\n [0.68726592 0.78070175 0.44705882]\n [0.57042254 0.63963964 0.21764706]\n [0.59777778 0.78947368 0.81372549]\n [0.53159041 0.76884422 0.39019608]]\n\n\n\nfrom scipy.optimize import root_scalar\ndef hsl_to_rgb(h, s, l):\n    # colorsys expects H, L, S in [0,1]\n    r, g, b = colorsys.hls_to_rgb(h, l, s)\n    return r, g, b\n    \n# Arc length function of theta\ndef arc_length(theta, b):\n    return (b/2) * (theta * np.sqrt(1 + theta**2) + np.arcsinh(theta))\n\n# Inverse function: find theta given s (arc length)\ndef theta_for_s(s, b):\n    # Use root finding to solve arc_length(theta) - s = 0\n    sol = root_scalar(lambda t: arc_length(t, b) - s, bracket=[0, 100], method='bisect')\n    return sol.root\n\nd = 4  # distance from center to corner\n# Define corner offsets using Cartesian product of [-d, d]\ncorners = np.array(np.meshgrid([-d, d], [-d, d])).T.reshape(-1, 2)\noffsets = np.vstack([[0, 0],corners])\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\nfor ind, color_name in enumerate(cluster_results):\n    cp = cluster_results[color_name]['closest_points'][['h', 's', 'l']].values\n    \n    rgb_colors = [hsl_to_rgb(*hsl) for hsl in cp]\n    \n    b = 0.1  # spiral parameter\n    num_points = len(cp)\n    desired_sep = 0.3  # desired arc length between points\n        \n    # Compute theta values for uniform arc length steps\n    arc_lengths = np.arange(num_points) * desired_sep\n    theta_vals = np.array([theta_for_s(s, b) for s in arc_lengths])\n    \n    # Compute spiral coords\n    r = b * theta_vals\n    x = r * np.cos(theta_vals)\n    y = r * np.sin(theta_vals)\n    \n    #plt.scatter(x, y, color=rgb_colors, s=200, marker=\"|\", linewidths=4)\n    plt.scatter(x+offsets[ind][0], y+offsets[ind][1], color=rgb_colors, s=100) \n    ax.text(offsets[ind][0], offsets[ind][1]-3.25, f\"{color_name}\\n{ merged_data.groupby('color')['hex_x'].nunique().loc[color_name]:,}\", fontsize=14, ha='center', va='center', color=color_ranks[color_ranks['color'] == color_name]['hex'].values[0], fontfamily=\"Consolas\")\n\nax.axis(\"off\")\nplt.ylim(-7, 7)\nplt.xlim(-7, 7)\nlong_title = \"Top 100 of the specified number of color variants selected via K-means clustering for the top five colors in the xkcd color survey.\"\nwrapped_title = \"\\n\".join(textwrap.wrap(long_title, width=60))\n\nplt.title(wrapped_title, fontsize=16, fontfamily=\"Consolas\", loc='left')\nplt.tight_layout()\n#plt.ylabel(\"Random value\")\nplt.savefig(\"xkcd_colors.png\", dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "posts/xkcd_colors/xkcd_colors.html#tidytuesday-data-for-2025-07-08",
    "href": "posts/xkcd_colors/xkcd_colors.html#tidytuesday-data-for-2025-07-08",
    "title": "The xkcd color survey",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport colorsys\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import pairwise_distances_argmin_min\nimport textwrap\n\n\nanswers = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/answers.csv')\ncolor_ranks = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/color_ranks.csv')\nusers = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/users.csv')\n\n\nanswers\n\n\n\n\n\n\n\n\nuser_id\nhex\nrank\n\n\n\n\n0\n1\n#8240EA\n1\n\n\n1\n2\n#4B31EA\n3\n\n\n2\n2\n#584601\n5\n\n\n3\n2\n#DA239C\n4\n\n\n4\n2\n#B343E5\n1\n\n\n...\n...\n...\n...\n\n\n1058206\n152397\n#7238F0\n1\n\n\n1058207\n152398\n#8E14CD\n1\n\n\n1058208\n152398\n#0A49E7\n3\n\n\n1058209\n152400\n#38A30E\n2\n\n\n1058210\n152401\n#4D004B\n1\n\n\n\n\n1058211 rows × 3 columns\n\n\n\n\nanswers['rank'].unique()\n\narray([1, 3, 5, 4, 2])\n\n\n\ncolor_ranks\n\n\n\n\n\n\n\n\ncolor\nrank\nhex\n\n\n\n\n0\npurple\n1\n#7e1e9c\n\n\n1\ngreen\n2\n#15b01a\n\n\n2\nblue\n3\n#0343df\n\n\n3\npink\n4\n#ff81c0\n\n\n4\nbrown\n5\n#653700\n\n\n...\n...\n...\n...\n\n\n944\nfresh green\n945\n#69d84f\n\n\n945\nelectric lime\n946\n#a8ff04\n\n\n946\ndust\n947\n#b2996e\n\n\n947\ndark pastel green\n948\n#56ae57\n\n\n948\ncloudy blue\n949\n#acc2d9\n\n\n\n\n949 rows × 3 columns\n\n\n\n\ndef hex_to_hsl(hex_color):\n    rgb = colors.to_rgb(hex_color)  # Returns RGB as floats (0-1)\n    h, l, s = colorsys.rgb_to_hls(*rgb)  # Note: HLS order\n#    return round(h * 360, 1), round(s * 100, 1), round(l * 100, 1)\n    return h, s, l\n\nprint(hex_to_hsl(\"#ff5733\"))\n\n(0.02941176470588236, 1.0, 0.6)\n\n\n\n# merge answers and color_rank based on rank column\nmerged_data = pd.merge(answers, color_ranks, on='rank', how='left')\nmerged_data[['h', 's', 'l']] = merged_data['hex_x'].apply(hex_to_hsl).apply(pd.Series)\n\n\nmerged_data\n\n\n\n\n\n\n\n\nuser_id\nhex_x\nrank\ncolor\nhex_y\nh\ns\nl\n\n\n\n\n0\n1\n#8240EA\n1\npurple\n#7e1e9c\n0.731373\n0.801887\n0.584314\n\n\n1\n2\n#4B31EA\n3\nblue\n#0343df\n0.690090\n0.814978\n0.554902\n\n\n2\n2\n#584601\n5\nbrown\n#653700\n0.132184\n0.977528\n0.174510\n\n\n3\n2\n#DA239C\n4\npink\n#ff81c0\n0.889800\n0.723320\n0.496078\n\n\n4\n2\n#B343E5\n1\npurple\n#7e1e9c\n0.781893\n0.757009\n0.580392\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1058206\n152397\n#7238F0\n1\npurple\n#7e1e9c\n0.719203\n0.859813\n0.580392\n\n\n1058207\n152398\n#8E14CD\n1\npurple\n#7e1e9c\n0.776577\n0.822222\n0.441176\n\n\n1058208\n152398\n#0A49E7\n3\nblue\n#0343df\n0.619155\n0.917012\n0.472549\n\n\n1058209\n152400\n#38A30E\n2\ngreen\n#15b01a\n0.286353\n0.841808\n0.347059\n\n\n1058210\n152401\n#4D004B\n1\npurple\n#7e1e9c\n0.837662\n1.000000\n0.150980\n\n\n\n\n1058211 rows × 8 columns\n\n\n\n\nmerged_data.groupby('color').count()\n\n\n\n\n\n\n\n\nuser_id\nhex_x\nrank\nhex_y\nh\ns\nl\n\n\ncolor\n\n\n\n\n\n\n\n\n\n\n\nblue\n288015\n288015\n288015\n288015\n288015\n288015\n288015\n\n\nbrown\n75812\n75812\n75812\n75812\n75812\n75812\n75812\n\n\ngreen\n314172\n314172\n314172\n314172\n314172\n314172\n314172\n\n\npink\n131013\n131013\n131013\n131013\n131013\n131013\n131013\n\n\npurple\n249199\n249199\n249199\n249199\n249199\n249199\n249199\n\n\n\n\n\n\n\n\nf\"{merged_data.groupby('color')['hex_x'].nunique().loc['blue']:,}\"\n\n'275,337'\n\n\n\ncluster_results = {}\nfor color, group_df in merged_data.groupby('color'):\n    kmeans = KMeans(n_clusters=100, random_state=2025)\n    kmeans.fit(group_df[['h', 's', 'l']])\n\n    # Store the result: labels and cluster centers\n    cluster_results[color] = {\n        'labels': kmeans.labels_,\n        'centers': kmeans.cluster_centers_,\n        'data': group_df.copy()\n    }\n#    cluster_results[color]['data']['cluster'] = kmeans.labels_\n\n#print(cluster_results[\"blue\"][\"data\"])\n\nfor color in cluster_results:\n    group_data = cluster_results[color]['data']\n    centers = cluster_results[color]['centers']\n    closest_idxs, _ = pairwise_distances_argmin_min(centers, group_data[['h', 's', 'l']].values)\n    closest_points = group_data.iloc[closest_idxs]\n    cluster_results[color]['closest_points'] = closest_points\n\n\nprint(cluster_results['blue']['closest_points'][['h', 's', 'l']].head(10).values)\n\n[[0.56581741 0.74407583 0.58627451]\n [0.6374269  0.89528796 0.3745098 ]\n [0.50673401 0.52380952 0.62941176]\n [0.59259259 0.54418605 0.42156863]\n [0.66666667 0.88516746 0.59019608]\n [0.66333333 0.20661157 0.4745098 ]\n [0.68726592 0.78070175 0.44705882]\n [0.57042254 0.63963964 0.21764706]\n [0.59777778 0.78947368 0.81372549]\n [0.53159041 0.76884422 0.39019608]]\n\n\n\nfrom scipy.optimize import root_scalar\ndef hsl_to_rgb(h, s, l):\n    # colorsys expects H, L, S in [0,1]\n    r, g, b = colorsys.hls_to_rgb(h, l, s)\n    return r, g, b\n    \n# Arc length function of theta\ndef arc_length(theta, b):\n    return (b/2) * (theta * np.sqrt(1 + theta**2) + np.arcsinh(theta))\n\n# Inverse function: find theta given s (arc length)\ndef theta_for_s(s, b):\n    # Use root finding to solve arc_length(theta) - s = 0\n    sol = root_scalar(lambda t: arc_length(t, b) - s, bracket=[0, 100], method='bisect')\n    return sol.root\n\nd = 4  # distance from center to corner\n# Define corner offsets using Cartesian product of [-d, d]\ncorners = np.array(np.meshgrid([-d, d], [-d, d])).T.reshape(-1, 2)\noffsets = np.vstack([[0, 0],corners])\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\nfor ind, color_name in enumerate(cluster_results):\n    cp = cluster_results[color_name]['closest_points'][['h', 's', 'l']].values\n    \n    rgb_colors = [hsl_to_rgb(*hsl) for hsl in cp]\n    \n    b = 0.1  # spiral parameter\n    num_points = len(cp)\n    desired_sep = 0.3  # desired arc length between points\n        \n    # Compute theta values for uniform arc length steps\n    arc_lengths = np.arange(num_points) * desired_sep\n    theta_vals = np.array([theta_for_s(s, b) for s in arc_lengths])\n    \n    # Compute spiral coords\n    r = b * theta_vals\n    x = r * np.cos(theta_vals)\n    y = r * np.sin(theta_vals)\n    \n    #plt.scatter(x, y, color=rgb_colors, s=200, marker=\"|\", linewidths=4)\n    plt.scatter(x+offsets[ind][0], y+offsets[ind][1], color=rgb_colors, s=100) \n    ax.text(offsets[ind][0], offsets[ind][1]-3.25, f\"{color_name}\\n{ merged_data.groupby('color')['hex_x'].nunique().loc[color_name]:,}\", fontsize=14, ha='center', va='center', color=color_ranks[color_ranks['color'] == color_name]['hex'].values[0], fontfamily=\"Consolas\")\n\nax.axis(\"off\")\nplt.ylim(-7, 7)\nplt.xlim(-7, 7)\nlong_title = \"Top 100 of the specified number of color variants selected via K-means clustering for the top five colors in the xkcd color survey.\"\nwrapped_title = \"\\n\".join(textwrap.wrap(long_title, width=60))\n\nplt.title(wrapped_title, fontsize=16, fontfamily=\"Consolas\", loc='left')\nplt.tight_layout()\n#plt.ylabel(\"Random value\")\nplt.savefig(\"xkcd_colors.png\", dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "posts/WHO_TB/WHO_TB.html",
    "href": "posts/WHO_TB/WHO_TB.html",
    "title": "WHO TB Burden Data",
    "section": "",
    "text": "TidyTuesday dataset of November 11, 2025\n\nimport marimo as mo\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom matplotlib.ticker import FuncFormatter\n\n\nwho_tb_data = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-11/who_tb_data.csv')\nwho_tb_data\n\n\n\n\n\n\n\n\ncountry\ng_whoregion\niso_numeric\niso2\niso3\nyear\nc_cdr\nc_newinc_100k\ncfr\ne_inc_100k\ne_inc_num\ne_mort_100k\ne_mort_exc_tbhiv_100k\ne_mort_exc_tbhiv_num\ne_mort_num\ne_mort_tbhiv_100k\ne_mort_tbhiv_num\ne_pop_num\n\n\n\n\n0\nAfghanistan\nEastern Mediterranean\n4\nAF\nAFG\n2000\n19.0\n35.0\n0.37\n190.0\n38000\n68.0\n68.0\n14000.0\n14000.0\n0.17\n34.0\n20130323\n\n\n1\nAfghanistan\nEastern Mediterranean\n4\nAF\nAFG\n2001\n26.0\n50.0\n0.35\n189.0\n38000\n63.0\n63.0\n13000.0\n13000.0\n0.30\n61.0\n20284311\n\n\n2\nAfghanistan\nEastern Mediterranean\n4\nAF\nAFG\n2002\n34.0\n65.0\n0.31\n189.0\n40000\n57.0\n57.0\n12000.0\n12000.0\n0.27\n58.0\n21378110\n\n\n3\nAfghanistan\nEastern Mediterranean\n4\nAF\nAFG\n2003\n32.0\n61.0\n0.32\n189.0\n43000\n58.0\n58.0\n13000.0\n13000.0\n0.29\n66.0\n22733047\n\n\n4\nAfghanistan\nEastern Mediterranean\n4\nAF\nAFG\n2004\n41.0\n78.0\n0.28\n189.0\n44000\n52.0\n51.0\n12000.0\n12000.0\n0.29\n67.0\n23560660\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5112\nZimbabwe\nAfrica\n716\nZW\nZWE\n2019\n69.0\n138.0\n0.22\n199.0\n30000\n43.0\n11.0\n1700.0\n6500.0\n31.00\n4800.0\n15271376\n\n\n5113\nZimbabwe\nAfrica\n716\nZW\nZWE\n2020\n54.0\n101.0\n0.24\n188.0\n29000\n44.0\n13.0\n2000.0\n6800.0\n31.00\n4800.0\n15526885\n\n\n5114\nZimbabwe\nAfrica\n716\nZW\nZWE\n2021\n52.0\n103.0\n0.25\n199.0\n31000\n49.0\n15.0\n2300.0\n7800.0\n34.00\n5400.0\n15797209\n\n\n5115\nZimbabwe\nAfrica\n716\nZW\nZWE\n2022\n54.0\n113.0\n0.26\n209.0\n34000\n52.0\n16.0\n2500.0\n8400.0\n37.00\n5900.0\n16069054\n\n\n5116\nZimbabwe\nAfrica\n716\nZW\nZWE\n2023\n56.0\n118.0\n0.24\n211.0\n35000\n48.0\n14.0\n2400.0\n7900.0\n34.00\n5500.0\n16340822\n\n\n\n\n5117 rows × 18 columns\n\n\n\n\nwho_tb_data['g_whoregion'].unique()\n\narray(['Eastern Mediterranean', 'Europe', 'Africa', 'Western Pacific',\n       'Americas', 'South-East Asia'], dtype=object)\n\n\n\nwho_tb_data.groupby('country')['c_newinc_100k'].sum().sort_values(ascending=False)\n\ncountry\nEswatini                                                    12982.00\nNamibia                                                     12055.00\nSouth Africa                                                11692.00\nLesotho                                                     10858.00\nBotswana                                                     8117.00\n                                                              ...   \nCuraçao                                                        23.85\nAnguilla                                                       21.90\noccupied Palestinian territory, including east Jerusalem       19.51\nMonaco                                                         11.00\nSan Marino                                                     10.90\nName: c_newinc_100k, Length: 215, dtype: float64\n\n\n\nsns.lineplot(data=who_tb_data,x='year',y='cfr',hue='g_whoregion')\nplt.show()\n\n\n\n\n\n\n\n\n\nbg_color=\"#FFF0FD\"\nfg_color=\"#740C08\"#\"#333333\"\nfg_color2=\"#E484AC\"\npalette='tab20'\nfig, ax  = plt.subplots(1,2, figsize=(12,5), sharey=True)\n#sns.lineplot(data=who_tb_data,x='year',y='c_newinc_100k',hue='g_whoregion',ax=ax[0], legend=False, errorbar='se', estimator=np.sum, palette=['black'] * who_tb_data['g_whoregion'].nunique(), lw=2.5)\nsns.lineplot(data=who_tb_data,x='year',y='c_newinc_100k',hue='g_whoregion',ax=ax[0], legend=False, errorbar='se', estimator=np.sum, palette=palette, lw=2)\n#sns.lineplot(data=who_tb_data,x='year',y='e_mort_100k',hue='g_whoregion',ax=ax[1], errorbar='se', estimator=np.sum, palette=['black'] * who_tb_data['g_whoregion'].nunique(), lw=2.5, legend=False)\nsns.lineplot(data=who_tb_data,x='year',y='e_mort_100k',hue='g_whoregion',ax=ax[1], errorbar='se', estimator=np.sum, palette=palette, lw=2)\n\nsns.despine()\n\n\ndef thousands(x, pos):\n    return f'{int(x/1000)}K'\n\n# Apply to y-axis\nax[1].yaxis.set_major_formatter(FuncFormatter(thousands))\nfor a in ax:\n    a.tick_params(axis='both', colors=fg_color)\n    for spine in a.spines.values():\n        spine.set_color(fg_color2) \n\nax[0].tick_params(axis='y', length=10, direction='inout')\nax[1].tick_params(axis='y', length=10, direction='inout')\nax[1].legend(frameon=False, ncol=2, loc='upper center', bbox_to_anchor=(0.5, 0.9))\n#ax[0].set_title(\"Number of cases\", color=fg_color)\n#ax[1].set_title(\"Number of deaths\", color=fg_color)\nax[0].set_ylabel(\"Number per 100K\", color=fg_color)\nax[0].set_xlabel(\"\")\nax[1].set_xlabel(\"\")\nplt.subplots_adjust(wspace=0)  \nfig.patch.set_facecolor(bg_color)\nax[0].set_facecolor(bg_color)\nax[1].set_facecolor(bg_color)\nfig.suptitle(\"Trends in Tuberculosis Cases (left) and Deaths (right) by WHO Region (2000-2023)\", color=fg_color, fontsize=16, family='serif')\nplt.savefig('WHO_TB.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "posts/Weather_attribution/weather_attribution.html",
    "href": "posts/Weather_attribution/weather_attribution.html",
    "title": "Extreme Weather Attribution Studies",
    "section": "",
    "text": "TidyTuesday dataset of 2025-08-12\n\nimport pandas as pd\nimport plotly.express as px\nimport plotly.subplots as sp\nimport textwrap\n\n\nattribution_studies = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-12/attribution_studies.csv')\nattribution_studies_raw = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-12/attribution_studies_raw.csv')\n\n\nattribution_studies\n\n\n\n\n\n\n\n\nevent_name\nevent_period\nevent_year\nstudy_focus\niso_country_code\ncb_region\nevent_type\nclassification\nsummary_statement\npublication_year\ncitation\nsource\nrapid_study\nlink\n\n\n\n\n0\nEuropean summer heatwave\n2003\n2003\nEvent\nNaN\nEurope\nHeat\nMore severe or more likely to occur\n\"We estimate it is very likely (confidence lev...\n2004.0\nStott, P. et al., 2004: Human contribution to ...\nNature\nNo\nhttps://www.nature.com/nature/journal/v432/n70...\n\n\n1\nGlobal temperature extremes\nsince 1950\nNaN\nTrend\nNaN\nGlobal\nHeat\nMore severe or more likely to occur\n\"Comparing these observations with climate mod...\n2005.0\nChristidis, N. et al., 2005: Detection of chan...\nGeophysical Research Letters\nNo\nhttps://agupubs.onlinelibrary.wiley.com/doi/fu...\n\n\n2\nRecord warm autumn in Europe\n2006\n2006\nEvent\nNaN\nEurope\nHeat\nMore severe or more likely to occur\n\"Global warming has made a warm autumn like th...\n2007.0\nVan Oldenborgh, G-J. et al., 2007: How unusual...\nClimate of the Past\nNo\nhttp://www.clim-past.net/3/659/2007/cp-3-659-2...\n\n\n3\nIncreasing frequency of 'very warm' Northern h...\n1860-2009\nNaN\nTrend\nNaN\nNorthern hemisphere\nHeat\nMore severe or more likely to occur\n\"We detect the dominant influence of anthropog...\n2007.0\nJones, G. et al., 2007: Human contribution to ...\nJournal of Geophysical Research: Atmospheres\nNo\nhttps://agupubs.onlinelibrary.wiley.com/doi/fu...\n\n\n4\nMoscow summer heatwave\n2010\n2010\nEvent\nRUS\nEurope\nHeat\nMore severe or more likely to occur\n\"For July temperature in Moscow, we estimate.....\n2011.0\nRahmstorf, S. & Coumou, D. 2011: Increase of e...\nProceedings of the National Academy of Sciences\nNo\nhttp://www.pnas.org/content/108/44/17905.abstract\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n739\nChina growing season hot and dry events\nmid 1990s\nMid-1990s\nEvent\nCHN\nEastern and south-eastern Asia\nCompound\nMore severe or more likely to occur\n\"Anthropogenic forcing changes can explain 60%...\n2024.0\nSu Q. et al., 2024: Anthropogenic Influence on...\nAdvances in Atmospheric Sciences\nNo\nhttps://doi.org/10.1007/s00376-023-2319-z\n\n\n740\nChina hot-wet events\n1979-2014\nNaN\nTrend\nCHN\nEastern and south-eastern Asia\nCompound\nDecrease, less severe or less likely to occur\n\"Over the past 40 years, anthropogenic activit...\n2024.0\nYao H. et al., 2024: Changes caused by human a...\nCommunications Earth & Environment\nNo\nhttps://doi.org/10.1038/s43247-024-01625-y\n\n\n741\nHeat extremes behind bleaching of the Great Ba...\n2020, 2024\n2017, 2020, 2024\nEvent\nAUS\nAustralia and New Zealand\nImpact\nMore severe or more likely to occur\n\"Climate model analysis confirms that human in...\n2024.0\nHenley B.J. et al., 2024: Highest ocean heat i...\nNature\nNo\nhttps://doi.org/10.1038/s41586-024-07672-x\n\n\n742\nHeat-related neonatal deaths in low- and middl...\n2001-2019\nNaN\nTrend\nNaN\nGlobal\nImpact\nMore severe or more likely to occur\n\"Climate change was responsible for 32% (range...\n2024.0\nDimitrova. A. et al., 2024: Temperature-relate...\nNature Communications\nNo\nhttps://www.nature.com/articles/s41467-024-498...\n\n\n743\nHospitalisations in China due to heat\n2000-2019\nNaN\nTrend\nCHN\nEastern and south-eastern Asia\nImpact\nMore severe or more likely to occur\n\"In the 2010s, the heat-related attributable f...\n2024.0\nZhou. L. et al,. 2024: Quantification of the H...\nEnvironmental Health Perspectives\nNo\nhttps://ehp.niehs.nih.gov/doi/10.1289/EHP14057\n\n\n\n\n744 rows × 14 columns\n\n\n\n\nattribution_studies['event_type'].value_counts()\n\nevent_type\nHeat                207\nRain & flooding     177\nDrought             106\nStorm                61\nCold, snow & ice     57\nImpact               33\nCompound             33\nWildfire             31\nOceans               25\nAtmosphere            7\nSunshine              4\nRiver flow            3\nName: count, dtype: int64\n\n\n\nattribution_studies['event_name'].value_counts()\n\nevent_name\nEurope heatwave                          13\nCalifornia drought                        7\nNorthern Europe summer heatwave           7\nDamages from UK flooding                  4\nGlobal temperature extremes               4\n                                         ..\nYangtze river extreme rainfall            1\nExtreme flooding in Missouri              1\nFrance floods                             1\nStorm Desmond heavy rains                 1\nHospitalisations in China due to heat     1\nName: count, Length: 678, dtype: int64\n\n\n\nattribution_studies['classification'].value_counts()\n\nclassification\nMore severe or more likely to occur              554\nNo discernible human influence                    71\nDecrease, less severe or less likely to occur     66\nInsufficient data/inconclusive                    53\nName: count, dtype: int64\n\n\n\nclassification_labels = {\n    'More severe or more likely to occur': 'More Severe/Likely',\n    'Decrease, less severe or less likely to occur': 'Less Severe/Likely',\n    'No discernible human influence': 'No human influence',\n    'Insufficient data/inconclusive': 'Insufficient Data'\n}\n\n# sunburst plots\nsunburst_data = attribution_studies.groupby(['study_focus', 'classification', 'event_type']).size().reset_index(name='count')\nsunburst_data['classification'] = sunburst_data['classification'].map(classification_labels)\n\nsunburst_data = sunburst_data.dropna(subset=['classification'])\n\nevent_focus_count = sunburst_data[sunburst_data['study_focus'] == 'Event']['count'].sum()\ntrend_focus_count = sunburst_data[sunburst_data['study_focus'] == 'Trend']['count'].sum()\n\nfig = sp.make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]],\n                       subplot_titles=(f'Event Focus ({event_focus_count})', f'Trend Focus ({trend_focus_count})'))\n\nfig.add_trace(px.sunburst(sunburst_data[sunburst_data['study_focus'] == 'Event'], \n                          path=['classification', 'event_type'], values='count').data[0],\n              row=1, col=1)\n\nfig.add_trace(px.sunburst(sunburst_data[sunburst_data['study_focus'] == 'Trend'], \n                          path=['classification', 'event_type'], values='count').data[0],\n              row=1, col=2)\n\nfig.update_layout(title_text='Effect of climate change on extreme weather attributions &#8211; &lt;br&gt;&lt;span style=\"color:blue\"&gt;More severe&lt;/span&gt;, &lt;span style=\"color:green\"&gt;Less severe&lt;/span&gt;, &lt;span style=\"color:red\"&gt;No human influence&lt;/span&gt;, and &lt;span style=\"color:purple\"&gt;Insufficient data&lt;/span&gt;.')\n\n# Save figure\nfig.write_html('sunburst_plots_by_study_focus.html')\n#fig.write_image(\"weather_attribution.png\")\nfig.show()\n\n                                                \n\n\nwrite_image didn’t work. HTML file was processed in inkscape to generate high resolution image."
  },
  {
    "objectID": "posts/useR2025/useR2025.html",
    "href": "posts/useR2025/useR2025.html",
    "title": "useR2025 conference data analysis",
    "section": "",
    "text": "library(tidyverse)\nlibrary(uk2us)\nuser2025 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-29/user2025.csv')\nuser2025\n\n# A tibble: 128 × 11\n      id session date       time  room   title  content video_recording keywords\n   &lt;dbl&gt; &lt;chr&gt;   &lt;date&gt;     &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;           &lt;chr&gt;   \n 1   170 Virtual 2025-08-01 TBD   Online A Rob… In R p… ✅              statist…\n 2    79 Virtual 2025-08-01 TBD   Online A fir… Positr… ✅              ide, wo…\n 3    30 Virtual 2025-08-01 TBD   Online Analy… This t… ✅              demogra…\n 4    31 Virtual 2025-08-01 TBD   Online Autom… Webhoo… ✅              automat…\n 5    39 Virtual 2025-08-01 TBD   Online Beyon… In a w… ✅              marketi…\n 6   169 Virtual 2025-08-01 TBD   Online CSV t… CSV is… ✅              data pr…\n 7    94 Virtual 2025-08-01 TBD   Online Data … Explor… ✅              factor …\n 8   163 Virtual 2025-08-01 TBD   Online Don’t… The fa… ✅              testing…\n 9    13 Virtual 2025-08-01 TBD   Online Exper… Large … ✅              automat…\n10    51 Virtual 2025-08-01 TBD   Online From … Data S… ✅              quarto,…\n# ℹ 118 more rows\n# ℹ 2 more variables: speakers &lt;chr&gt;, co_authors &lt;chr&gt;\nnormalize_keywords_uk2us &lt;- function(phrases) {\n  phrases %&gt;%\n    str_split(\"\\\\s+\") %&gt;%                                \n    map(~ uk2us::convert_uk2us(.x)) %&gt;%                  \n    map_chr(str_c, collapse = \" \")                       \n}"
  },
  {
    "objectID": "posts/useR2025/useR2025.html#plotting",
    "href": "posts/useR2025/useR2025.html#plotting",
    "title": "useR2025 conference data analysis",
    "section": "Plotting",
    "text": "Plotting\n\nuser2025 %&gt;%\n  separate_rows(keywords, sep = \",\") %&gt;%\n  mutate(keywords = str_trim(keywords)) %&gt;%\n  mutate(keywords = normalize_keywords_uk2us(keywords)) %&gt;% \n  group_by(keywords) %&gt;% \n  summarise(count = n()) %&gt;% \n  arrange(desc(count)) %&gt;% \n  slice(1:20) %&gt;% \n  ggplot(aes(x = reorder(keywords, count), y = count, fill=count)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = reorder(keywords, count), y=count/2), hjust = 0, \n            size = 3.5, color=rep(c(\"white\", \"#555\"), times=c(8,12))) + \n  coord_flip() +\n  scale_fill_viridis_c(option=\"magma\", direction = -1)+\n  labs(title = \"Top 20 Keywords in useR! 2025\") +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.title.y = element_blank(),\n    legend.position = \"none\",\n    panel.grid.major.y = element_blank()\n  )\n\n\n\n\n\n\n\nggsave(\"user2025_keywords.png\", width = 8, height = 6, dpi = 300)"
  },
  {
    "objectID": "posts/Swim_sites_Sydney/swim_sites.html",
    "href": "posts/Swim_sites_Sydney/swim_sites.html",
    "title": "Water Quality at Sydney Beaches",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggtext)\n\n\nwater_quality &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-20/water_quality.csv')\nwater_quality\n\n# A tibble: 123,530 × 10\n   region         council       swim_site date       time  enterococci_cfu_100ml\n   &lt;chr&gt;          &lt;chr&gt;         &lt;chr&gt;     &lt;date&gt;     &lt;tim&gt;                 &lt;dbl&gt;\n 1 Western Sydney Hawkesbury C… Windsor … 2025-04-28 11:00                   620\n 2 Sydney Harbour North Sydney… Hayes St… 2025-04-28 11:40                    64\n 3 Sydney Harbour Willoughby C… Northbri… 2025-04-28 10:54                   160\n 4 Sydney Harbour Northern Bea… Fairligh… 2025-04-28 09:28                    54\n 5 Western Sydney Hawkesbury C… Yarramun… 2025-04-28 10:35                   720\n 6 Sydney Harbour Northern Bea… Little M… 2025-04-28 09:19                   230\n 7 Sydney Harbour City of Cana… Chiswick… 2025-04-28 13:06                   120\n 8 Sydney Harbour Inner West C… Dawn Fra… 2025-04-28 08:04                   280\n 9 Sydney Harbour Woollahra Mu… Rose Bay… 2025-04-28 08:50                    60\n10 Sydney Harbour Woollahra Mu… Camp Cove 2025-04-28 09:09                   100\n# ℹ 123,520 more rows\n# ℹ 4 more variables: water_temperature_c &lt;dbl&gt;, conductivity_ms_cm &lt;dbl&gt;,\n#   latitude &lt;dbl&gt;, longitude &lt;dbl&gt;\n\n\n\nglimpse(water_quality)\n\nRows: 123,530\nColumns: 10\n$ region                &lt;chr&gt; \"Western Sydney\", \"Sydney Harbour\", \"Sydney Harb…\n$ council               &lt;chr&gt; \"Hawkesbury City Council\", \"North Sydney Council…\n$ swim_site             &lt;chr&gt; \"Windsor Beach\", \"Hayes Street Beach\", \"Northbri…\n$ date                  &lt;date&gt; 2025-04-28, 2025-04-28, 2025-04-28, 2025-04-28,…\n$ time                  &lt;time&gt; 11:00:00, 11:40:00, 10:54:00, 09:28:00, 10:35:0…\n$ enterococci_cfu_100ml &lt;dbl&gt; 620, 64, 160, 54, 720, 230, 120, 280, 60, 100, 1…\n$ water_temperature_c   &lt;dbl&gt; 20, 21, 21, 21, 18, 21, 21, 21, 22, 22, 20, 20, …\n$ conductivity_ms_cm    &lt;dbl&gt; 248, 45250, 48930, 52700, 64, 39140, 4845, 50600…\n$ latitude              &lt;dbl&gt; -33.60448, -33.84172, -33.80604, -33.80073, -33.…\n$ longitude             &lt;dbl&gt; 150.8170, 151.2194, 151.2228, 151.2748, 150.6979…"
  },
  {
    "objectID": "posts/Swim_sites_Sydney/swim_sites.html#tidytuesday-data-for-2025-05-20",
    "href": "posts/Swim_sites_Sydney/swim_sites.html#tidytuesday-data-for-2025-05-20",
    "title": "Water Quality at Sydney Beaches",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggtext)\n\n\nwater_quality &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-20/water_quality.csv')\nwater_quality\n\n# A tibble: 123,530 × 10\n   region         council       swim_site date       time  enterococci_cfu_100ml\n   &lt;chr&gt;          &lt;chr&gt;         &lt;chr&gt;     &lt;date&gt;     &lt;tim&gt;                 &lt;dbl&gt;\n 1 Western Sydney Hawkesbury C… Windsor … 2025-04-28 11:00                   620\n 2 Sydney Harbour North Sydney… Hayes St… 2025-04-28 11:40                    64\n 3 Sydney Harbour Willoughby C… Northbri… 2025-04-28 10:54                   160\n 4 Sydney Harbour Northern Bea… Fairligh… 2025-04-28 09:28                    54\n 5 Western Sydney Hawkesbury C… Yarramun… 2025-04-28 10:35                   720\n 6 Sydney Harbour Northern Bea… Little M… 2025-04-28 09:19                   230\n 7 Sydney Harbour City of Cana… Chiswick… 2025-04-28 13:06                   120\n 8 Sydney Harbour Inner West C… Dawn Fra… 2025-04-28 08:04                   280\n 9 Sydney Harbour Woollahra Mu… Rose Bay… 2025-04-28 08:50                    60\n10 Sydney Harbour Woollahra Mu… Camp Cove 2025-04-28 09:09                   100\n# ℹ 123,520 more rows\n# ℹ 4 more variables: water_temperature_c &lt;dbl&gt;, conductivity_ms_cm &lt;dbl&gt;,\n#   latitude &lt;dbl&gt;, longitude &lt;dbl&gt;\n\n\n\nglimpse(water_quality)\n\nRows: 123,530\nColumns: 10\n$ region                &lt;chr&gt; \"Western Sydney\", \"Sydney Harbour\", \"Sydney Harb…\n$ council               &lt;chr&gt; \"Hawkesbury City Council\", \"North Sydney Council…\n$ swim_site             &lt;chr&gt; \"Windsor Beach\", \"Hayes Street Beach\", \"Northbri…\n$ date                  &lt;date&gt; 2025-04-28, 2025-04-28, 2025-04-28, 2025-04-28,…\n$ time                  &lt;time&gt; 11:00:00, 11:40:00, 10:54:00, 09:28:00, 10:35:0…\n$ enterococci_cfu_100ml &lt;dbl&gt; 620, 64, 160, 54, 720, 230, 120, 280, 60, 100, 1…\n$ water_temperature_c   &lt;dbl&gt; 20, 21, 21, 21, 18, 21, 21, 21, 22, 22, 20, 20, …\n$ conductivity_ms_cm    &lt;dbl&gt; 248, 45250, 48930, 52700, 64, 39140, 4845, 50600…\n$ latitude              &lt;dbl&gt; -33.60448, -33.84172, -33.80604, -33.80073, -33.…\n$ longitude             &lt;dbl&gt; 150.8170, 151.2194, 151.2228, 151.2748, 150.6979…"
  },
  {
    "objectID": "posts/Swim_sites_Sydney/swim_sites.html#data-wrangling",
    "href": "posts/Swim_sites_Sydney/swim_sites.html#data-wrangling",
    "title": "Water Quality at Sydney Beaches",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nwq_grp &lt;- water_quality %&gt;%\n  filter(!is.na(enterococci_cfu_100ml)) %&gt;% \n  mutate(date_y = format(date, \"%Y\")) %&gt;% \n  group_by(date_y, swim_site) %&gt;% \n  summarise(mean_enterococci = mean(enterococci_cfu_100ml, na.rm = TRUE)) %&gt;% \n  slice_min(order_by = mean_enterococci, n = 5)\n\nsite_freq &lt;- wq_grp %&gt;%\n  ungroup() %&gt;% \n  count(swim_site, sort = TRUE)"
  },
  {
    "objectID": "posts/Swim_sites_Sydney/swim_sites.html#plotting",
    "href": "posts/Swim_sites_Sydney/swim_sites.html#plotting",
    "title": "Water Quality at Sydney Beaches",
    "section": "Plotting",
    "text": "Plotting\n\nwq_grp%&gt;% \n  mutate(swim_site = fct_rev(factor(swim_site, levels = site_freq$swim_site))) %&gt;% \n  ggplot(aes(x = date_y, y=swim_site, fill=mean_enterococci)) +\n  scale_fill_gradient(low = \"lightblue\", high = \"blue\") +\n  geom_tile(color=\"white\") +\n  labs(\n    title = \"Year-wise top five swim-sites based on the lowest *Enterococci* concentration\",\n    x = element_blank(),\n    y = element_blank(),\n    fill = \"Average *Enterococci* &lt;br&gt; CFU/100ml\",\n    caption = \"Colony Forming Units (CFU) indicate number of viable bacteria that can form colonies when grown in a lab.\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(angle = 90, hjust = 1),\n    plot.title = element_textbox_simple(\n      padding = margin(5.5, 5.5, 5.5, 5.5),\n      margin = margin(5, 0, 10, 0)),\n    legend.title = element_markdown(hjust = 0.5),\n    legend.position = \"inside\",\n    legend.position.inside = c(0.80, 0.1),\n    legend.title.position = \"top\",\n    plot.caption = element_textbox_simple(\n      margin = margin(0, 0, 10, 175),\n      padding = margin(5.5, 0, 0, 0),\n      hjust = 0,\n      size = 8,\n      color = \"gray40\",\n      lineheight = 1.2\n    ),\n    axis.title.x = element_text(family = \"Consolas\"),\n    ) +\n  guides(fill = guide_colorbar(direction = \"horizontal\", barheight=.5)) +\n   coord_fixed(ratio = 1) \n\n\n\n\n\n\n\n#ggsave(\"swim_sites2.png\", width = 8, height = 6, dpi = 300, bg = \"white\")"
  },
  {
    "objectID": "posts/Seismic_Vesuvius/Mt_vesuvius.html",
    "href": "posts/Seismic_Vesuvius/Mt_vesuvius.html",
    "title": "Seismic events at Mount Vesuvius",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nvesuvius = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-13/vesuvius.csv')\n\n\nvesuvius\n\n\n\n\n\n\n\n\nevent_id\ntime\nlatitude\nlongitude\ndepth_km\nduration_magnitude_md\nmd_error\narea\ntype\nreview_level\nyear\n\n\n\n\n0\n4251\n2011-04-20T00:27:24Z\n40.818000\n14.430000\n0.42\n1.2\n0.3\nMount Vesuvius\nearthquake\nrevised\n2011\n\n\n1\n4252\n2012-06-19T21:29:48Z\n40.808833\n14.427167\n1.31\n0.7\n0.3\nMount Vesuvius\nearthquake\nrevised\n2012\n\n\n2\n22547\n2013-01-01T07:34:46Z\n40.822170\n14.428000\n0.06\n2.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n\n\n3\n22546\n2013-01-03T16:06:48Z\nNaN\nNaN\nNaN\n0.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n\n\n4\n22545\n2013-01-03T16:07:37Z\nNaN\nNaN\nNaN\n0.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n12022\n40738\n2024-12-29T23:56:51Z\n40.823000\n14.428333\n0.34\n-0.1\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n12023\n40741\n2024-12-30T07:52:43Z\n40.823333\n14.423500\n0.56\n-0.1\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n12024\n40743\n2024-12-30T12:52:24Z\nNaN\nNaN\nNaN\n-0.1\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n12025\n40744\n2024-12-30T15:11:28Z\n40.819000\n14.424500\n0.55\n-0.4\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n12026\n40802\n2024-12-31T17:02:32Z\n40.822000\n14.409833\n0.41\n0.0\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n\n\n12027 rows × 11 columns\n\n\n\n\nsns.histplot(data=vesuvius,x=\"depth_km\")\n\n\n\n\n\n\n\n\n\nvesuvius[\"time\"] = pd.to_datetime(vesuvius[\"time\"])\n\n\nvesuvius[\"hour\"] = vesuvius[\"time\"].dt.hour\ntime_bins = [0, 5, 12, 17, 21, 24]  # 0-4 -&gt; Night, 5-11 -&gt; Morning, etc.\ntime_labels = [\"Night\", \"Morning\", \"Afternoon\", \"Evening\", \"Night\"]\n\n\nvesuvius[\"time_of_day\"] = pd.cut(vesuvius['hour'], bins=time_bins, labels=time_labels, right=False, ordered=False)\n\n\nvesuvius[\"time_of_day\"]\n\n0            Night\n1            Night\n2          Morning\n3        Afternoon\n4        Afternoon\n           ...    \n12022        Night\n12023      Morning\n12024    Afternoon\n12025    Afternoon\n12026      Evening\nName: time_of_day, Length: 12027, dtype: category\nCategories (4, object): ['Afternoon', 'Evening', 'Morning', 'Night']\n\n\n\nvesuvius[\"year\"].unique()\n\narray([2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021,\n       2022, 2023, 2024], dtype=int64)\n\n\n\nbin_edges = [-float('inf'), 1, 2, 3, 5, 7, float('inf')]\nvesuvius[\"depth_bins\"] = pd.cut(vesuvius['depth_km'], bins=bin_edges)\n\n\nvesuvius[\"depth_bins\"].value_counts()\n\ndepth_bins\n(-inf, 1.0]    7777\n(1.0, 2.0]      648\n(2.0, 3.0]      142\n(3.0, 5.0]       25\n(5.0, 7.0]        1\n(7.0, inf]        1\nName: count, dtype: int64\n\n\n\nvesuvius.head()\n\n\n\n\n\n\n\n\nevent_id\ntime\nlatitude\nlongitude\ndepth_km\nduration_magnitude_md\nmd_error\narea\ntype\nreview_level\nyear\nhour\ntime_of_day\ndepth_bins\n\n\n\n\n0\n4251\n2011-04-20 00:27:24+00:00\n40.818000\n14.430000\n0.42\n1.2\n0.3\nMount Vesuvius\nearthquake\nrevised\n2011\n0\nNight\n(-inf, 1.0]\n\n\n1\n4252\n2012-06-19 21:29:48+00:00\n40.808833\n14.427167\n1.31\n0.7\n0.3\nMount Vesuvius\nearthquake\nrevised\n2012\n21\nNight\n(1.0, 2.0]\n\n\n2\n22547\n2013-01-01 07:34:46+00:00\n40.822170\n14.428000\n0.06\n2.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n7\nMorning\n(-inf, 1.0]\n\n\n3\n22546\n2013-01-03 16:06:48+00:00\nNaN\nNaN\nNaN\n0.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n16\nAfternoon\nNaN\n\n\n4\n22545\n2013-01-03 16:07:37+00:00\nNaN\nNaN\nNaN\n0.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n16\nAfternoon\nNaN"
  },
  {
    "objectID": "posts/Seismic_Vesuvius/Mt_vesuvius.html#tidytuesday-data-for-2025-05-13",
    "href": "posts/Seismic_Vesuvius/Mt_vesuvius.html#tidytuesday-data-for-2025-05-13",
    "title": "Seismic events at Mount Vesuvius",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nvesuvius = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-13/vesuvius.csv')\n\n\nvesuvius\n\n\n\n\n\n\n\n\nevent_id\ntime\nlatitude\nlongitude\ndepth_km\nduration_magnitude_md\nmd_error\narea\ntype\nreview_level\nyear\n\n\n\n\n0\n4251\n2011-04-20T00:27:24Z\n40.818000\n14.430000\n0.42\n1.2\n0.3\nMount Vesuvius\nearthquake\nrevised\n2011\n\n\n1\n4252\n2012-06-19T21:29:48Z\n40.808833\n14.427167\n1.31\n0.7\n0.3\nMount Vesuvius\nearthquake\nrevised\n2012\n\n\n2\n22547\n2013-01-01T07:34:46Z\n40.822170\n14.428000\n0.06\n2.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n\n\n3\n22546\n2013-01-03T16:06:48Z\nNaN\nNaN\nNaN\n0.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n\n\n4\n22545\n2013-01-03T16:07:37Z\nNaN\nNaN\nNaN\n0.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n12022\n40738\n2024-12-29T23:56:51Z\n40.823000\n14.428333\n0.34\n-0.1\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n12023\n40741\n2024-12-30T07:52:43Z\n40.823333\n14.423500\n0.56\n-0.1\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n12024\n40743\n2024-12-30T12:52:24Z\nNaN\nNaN\nNaN\n-0.1\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n12025\n40744\n2024-12-30T15:11:28Z\n40.819000\n14.424500\n0.55\n-0.4\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n12026\n40802\n2024-12-31T17:02:32Z\n40.822000\n14.409833\n0.41\n0.0\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n\n\n12027 rows × 11 columns\n\n\n\n\nsns.histplot(data=vesuvius,x=\"depth_km\")\n\n\n\n\n\n\n\n\n\nvesuvius[\"time\"] = pd.to_datetime(vesuvius[\"time\"])\n\n\nvesuvius[\"hour\"] = vesuvius[\"time\"].dt.hour\ntime_bins = [0, 5, 12, 17, 21, 24]  # 0-4 -&gt; Night, 5-11 -&gt; Morning, etc.\ntime_labels = [\"Night\", \"Morning\", \"Afternoon\", \"Evening\", \"Night\"]\n\n\nvesuvius[\"time_of_day\"] = pd.cut(vesuvius['hour'], bins=time_bins, labels=time_labels, right=False, ordered=False)\n\n\nvesuvius[\"time_of_day\"]\n\n0            Night\n1            Night\n2          Morning\n3        Afternoon\n4        Afternoon\n           ...    \n12022        Night\n12023      Morning\n12024    Afternoon\n12025    Afternoon\n12026      Evening\nName: time_of_day, Length: 12027, dtype: category\nCategories (4, object): ['Afternoon', 'Evening', 'Morning', 'Night']\n\n\n\nvesuvius[\"year\"].unique()\n\narray([2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021,\n       2022, 2023, 2024], dtype=int64)\n\n\n\nbin_edges = [-float('inf'), 1, 2, 3, 5, 7, float('inf')]\nvesuvius[\"depth_bins\"] = pd.cut(vesuvius['depth_km'], bins=bin_edges)\n\n\nvesuvius[\"depth_bins\"].value_counts()\n\ndepth_bins\n(-inf, 1.0]    7777\n(1.0, 2.0]      648\n(2.0, 3.0]      142\n(3.0, 5.0]       25\n(5.0, 7.0]        1\n(7.0, inf]        1\nName: count, dtype: int64\n\n\n\nvesuvius.head()\n\n\n\n\n\n\n\n\nevent_id\ntime\nlatitude\nlongitude\ndepth_km\nduration_magnitude_md\nmd_error\narea\ntype\nreview_level\nyear\nhour\ntime_of_day\ndepth_bins\n\n\n\n\n0\n4251\n2011-04-20 00:27:24+00:00\n40.818000\n14.430000\n0.42\n1.2\n0.3\nMount Vesuvius\nearthquake\nrevised\n2011\n0\nNight\n(-inf, 1.0]\n\n\n1\n4252\n2012-06-19 21:29:48+00:00\n40.808833\n14.427167\n1.31\n0.7\n0.3\nMount Vesuvius\nearthquake\nrevised\n2012\n21\nNight\n(1.0, 2.0]\n\n\n2\n22547\n2013-01-01 07:34:46+00:00\n40.822170\n14.428000\n0.06\n2.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n7\nMorning\n(-inf, 1.0]\n\n\n3\n22546\n2013-01-03 16:06:48+00:00\nNaN\nNaN\nNaN\n0.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n16\nAfternoon\nNaN\n\n\n4\n22545\n2013-01-03 16:07:37+00:00\nNaN\nNaN\nNaN\n0.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n16\nAfternoon\nNaN"
  },
  {
    "objectID": "posts/Seismic_Vesuvius/Mt_vesuvius.html#annual-number-of-tremors-vs-duration",
    "href": "posts/Seismic_Vesuvius/Mt_vesuvius.html#annual-number-of-tremors-vs-duration",
    "title": "Seismic events at Mount Vesuvius",
    "section": "Annual number of tremors vs duration",
    "text": "Annual number of tremors vs duration\n\nfig, ax = plt.subplots()\nhue_order = [\"Morning\", \"Afternoon\", \"Evening\", \"Night\"]\ncolor_map = [\"lightblue\", \"dodgerblue\", \"lightgrey\", \"grey\"]\ncolors = {x: color_map[ind] for ind, x in enumerate(hue_order)}\n     \nsns.stripplot(data=vesuvius, x=\"year\", y=\"duration_magnitude_md\", \\\n              hue='time_of_day', hue_order=hue_order, palette=\"plasma\")\nplt.xticks(rotation=45)\nplt.xlabel(\"\")\n\nplt.legend(\n    title='',\n    loc='upper center',     \n    bbox_to_anchor=(0.5, 1.1), \n    ncol=4,\n    frameon=False\n)\nsns.despine()"
  },
  {
    "objectID": "posts/Seismic_Vesuvius/Mt_vesuvius.html#earthquakes-at-different-times-of-the-day",
    "href": "posts/Seismic_Vesuvius/Mt_vesuvius.html#earthquakes-at-different-times-of-the-day",
    "title": "Seismic events at Mount Vesuvius",
    "section": "Earthquakes at different times of the day",
    "text": "Earthquakes at different times of the day\n\nvesuvius[\"hour12\"] = vesuvius[\"time\"].dt.strftime(\"%I\").astype(int)    \nvesuvius[\"AMPM\"] = vesuvius[\"time\"].dt.strftime(\"%p\")              \n\n\nvesuvius_grp = vesuvius.groupby([\"hour12\", \"AMPM\"]).size().unstack(fill_value=0)\nvesuvius_grp\n\n\n\n\n\n\n\nAMPM\nAM\nPM\n\n\nhour12\n\n\n\n\n\n\n1\n691\n339\n\n\n2\n694\n384\n\n\n3\n618\n335\n\n\n4\n591\n440\n\n\n5\n510\n732\n\n\n6\n472\n555\n\n\n7\n352\n610\n\n\n8\n287\n592\n\n\n9\n284\n624\n\n\n10\n380\n598\n\n\n11\n334\n627\n\n\n12\n646\n332\n\n\n\n\n\n\n\n\nvesuvius_grp.plot(kind=\"bar\", stacked=True, color=['dodgerblue', 'salmon'])"
  },
  {
    "objectID": "posts/Seismic_Vesuvius/Mt_vesuvius.html#polar-coordinates",
    "href": "posts/Seismic_Vesuvius/Mt_vesuvius.html#polar-coordinates",
    "title": "Seismic events at Mount Vesuvius",
    "section": "Polar coordinates",
    "text": "Polar coordinates\n\nimport plotly.graph_objects as go\nimport numpy as np\nimport roman\n\n\nvesuvius_grp['angle'] = vesuvius_grp.index * 30\n\n# Create polar plot with two bar traces: AM and PM\nfig = go.Figure()\n\n# AM bars\nfig.add_trace(go.Barpolar(\n    r=vesuvius_grp['AM'],\n    theta=vesuvius_grp['angle'],\n    name='AM',\n    marker_color='dodgerblue',\n    hovertemplate='count = %{r}&lt;br&gt;time = %{theta} AM&lt;extra&gt;&lt;/extra&gt;'\n))\n\n# PM bars\nfig.add_trace(go.Barpolar(\n    r=vesuvius_grp['PM'],\n    theta=vesuvius_grp['angle'],\n    name='PM',\n    marker_color='salmon',\n    hovertemplate='count = %{r}&lt;br&gt;time = %{theta} PM&lt;extra&gt;&lt;/extra&gt;'\n))\n\n# Layout\nfig.update_layout(\n    title={\n        'text': f'Distribution of &lt;b&gt;{vesuvius_grp[\"AM\"].sum() + vesuvius_grp[\"PM\"].sum():,.0f}&lt;/b&gt; earthquakes during &lt;br&gt;&lt;span style=\"color:dodgerblue;\"&gt;AM&lt;/span&gt; and &lt;span style=\"color:red;\"&gt;PM&lt;/span&gt; at Mount Vesuvius (2011-24)',\n        'font': {\n            'size': 16  \n        },\n        \"x\" : 0.5,\n    },\n    polar=dict(\n        angularaxis=dict(direction='clockwise', rotation=90, tickmode='array',\n                         tickvals=np.arange(30, 361, 30),\n                         ticktext=[f\"&lt;b&gt;{roman.toRoman(h)}&lt;/b&gt;\" for h in range(1, 13)], \n                         showline=False, showgrid=False),\n        radialaxis=dict(showticklabels=False, ticks='', showline=False, showgrid=False)\n    ),\n    showlegend=False,\n    template='plotly_white',\n    width=500,\n    height=500,\n    margin=dict(l=0, r=0, t=100, b=20)\n)\n#fig.write_image(\"Vesuvius.png\")\nfig.show()\n\n                                                \n\n\nThere is no direct way to add labels to bars in radial axis. As a workaround, add scatterplot and show only text.\n\nvesuvius_grp['angle'] = vesuvius_grp.index * 30\n\n# Create polar plot with two bar traces: AM and PM\nfig = go.Figure()\n\n# AM bars\nfig.add_trace(go.Barpolar(\n    r=vesuvius_grp['AM'],\n    theta=vesuvius_grp['angle'],\n    name='AM',\n    marker_color='dodgerblue',\n    hovertemplate='count = %{r}&lt;br&gt;time = %{theta} AM&lt;extra&gt;&lt;/extra&gt;'\n))\n\n# PM bars\nfig.add_trace(go.Barpolar(\n    r=vesuvius_grp['PM'],\n    theta=vesuvius_grp['angle'],\n    name='PM',\n    marker_color='salmon',\n    hovertemplate='count = %{r}&lt;br&gt;time = %{theta} PM&lt;extra&gt;&lt;/extra&gt;'\n))\n\n# Layout\nfig.update_layout(\n    title={\n        'text': f'Distribution of &lt;b&gt;{vesuvius_grp[\"AM\"].sum() + vesuvius_grp[\"PM\"].sum():,.0f}&lt;/b&gt; earthquakes during &lt;br&gt;&lt;span style=\"color:dodgerblue;\"&gt;AM&lt;/span&gt; and &lt;span style=\"color:red;\"&gt;PM&lt;/span&gt; at Mount Vesuvius (2011-24)',\n        'font': {\n            'size': 16  \n        },\n        \"x\" : 0.5,\n    },\n    polar=dict(\n        angularaxis=dict(direction='clockwise', rotation=90, tickmode='array',\n                         tickvals=np.arange(30, 361, 30),\n                         ticktext=[f\"&lt;b&gt;{roman.toRoman(h)}&lt;/b&gt;\" for h in range(1, 13)], \n                         showline=False, showgrid=False),\n        radialaxis=dict(showticklabels=False, ticks='', showline=False, showgrid=False)\n    ),\n    showlegend=False,\n    template='plotly_white',\n    width=500,\n    height=500,\n    margin=dict(l=0, r=0, t=100, b=20)\n)\nfig.add_trace(go.Scatterpolar(\n    r=vesuvius_grp['AM']+50,  \n    theta=vesuvius_grp['angle'],  \n    mode=\"text\",  \n    text=vesuvius_grp['AM'],  \n    textposition=\"middle center\",  \n    textfont=dict(size=14, color=\"white\"),\n    name=\"AM Labels\",\n    hoverinfo=\"skip\",  \n    showlegend=False\n))\n\nfig.add_trace(go.Scatterpolar(\n    r=vesuvius_grp['AM']+vesuvius_grp['PM']+50,  \n    theta=vesuvius_grp['angle'],  \n    mode=\"text\",\n    text=vesuvius_grp['PM'],\n    textposition=\"middle center\",\n    name=\"PM Labels\",\n    hoverinfo=\"skip\",\n    showlegend=False\n))\n#fig.write_image(\"Vesuvius_labels.png\")\nfig.show()"
  },
  {
    "objectID": "posts/recipes_ratings/recipes_ratings.html",
    "href": "posts/recipes_ratings/recipes_ratings.html",
    "title": "Recipes and Cuisines data from allrecipes.com",
    "section": "",
    "text": "TidyTuesday dataset of September 16, 2025\n\nimport pandas as pd\nimport numpy as np\nimport flagpy as fp\nfrom great_tables import GT, style,loc, md, html, nanoplot_options\nimport io\nimport base64\nimport matplotlib.pyplot as plt\n\n\nall_recipes = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-16/all_recipes.csv')\ncuisines = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-16/cuisines.csv')\n\n\nall_recipes\n\n\n\n\n\n\n\n\nname\nurl\nauthor\ndate_published\ningredients\ncalories\nfat\ncarbs\nprotein\navg_rating\ntotal_ratings\nreviews\nprep_time\ncook_time\ntotal_time\nservings\n\n\n\n\n0\nChewy Whole Wheat Peanut Butter Brownies\nhttps://www.allrecipes.com/recipe/140717/chewy...\nDMOMMY\n2020-06-18\n⅓ cup margarine, softened, ⅔ cup white sugar, ...\n222.0\n13.0\n24.0\n6.0\n4.4\n47.0\n36.0\n20\n35\n55\n16.0\n\n\n1\nPumpkin Pie Eggnog\nhttps://www.allrecipes.com/recipe/269204/pumpk...\nBobbie Susan\n2022-09-26\n12 egg yolks, 2 cups heavy whipping cream, ½ ...\n477.0\n31.0\n43.0\n8.0\n5.0\n1.0\n1.0\n10\n5\n495\n8.0\n\n\n2\nEggs Poached in Tomato Sauce\nhttps://www.allrecipes.com/recipe/238054/eggs-...\nBren\n2018-06-08\n2 tablespoons olive oil, or to taste, ½ onion...\n354.0\n18.0\n32.0\n20.0\n4.8\n4.0\n4.0\n10\n75\n85\n4.0\n\n\n3\nMinestrone Casserole\nhttps://www.allrecipes.com/minestrone-casserol...\nSarah Brekke\n2025-03-03\n4 cups dried mafalda pasta (mini lasagna noodl...\n356.0\n9.0\n53.0\n19.0\n4.3\n14.0\n13.0\n20\n40\n60\n8.0\n\n\n4\nYummy Stuffed Peppers\nhttps://www.allrecipes.com/recipe/241937/yummy...\nProcrastigirl\n2024-12-11\n4 green bell peppers, halved lengthwise and se...\n366.0\n22.0\n23.0\n19.0\n4.7\n84.0\n67.0\n30\n95\n125\n8.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n14421\nCheesy Kale Quiche\nhttps://www.allrecipes.com/recipe/244249/chees...\nally-gator\n2022-01-14\n1 (9 inch) pie crust pastry, ½ (8 ounce) packa...\n338.0\n25.0\n15.0\n14.0\n4.8\n20.0\n17.0\n10\n45\n55\n8.0\n\n\n14422\nFabulous Chicken Cordon Bleu Casserole\nhttps://www.allrecipes.com/fabulous-chicken-co...\nLindsay Breeze\n2024-01-08\n8 ounces egg noodles, 1/2 cup butter, 1/2 cup ...\n572.0\n37.0\n28.0\n30.0\n4.9\n14.0\n14.0\n30\n40\n70\n6.0\n\n\n14423\nAbsolutely Wonderful Cheesy, Creamy Spinach Ar...\nhttps://www.allrecipes.com/recipe/262412/absol...\nShamrock Farms\n2022-01-14\n½ cup Shamrock Farms® Premium Sour Cream, ½ cu...\n244.0\n13.0\n22.0\n11.0\nNaN\nNaN\nNaN\n10\n25\n35\n8.0\n\n\n14424\nGluten-Free Fruitcake\nhttps://www.allrecipes.com/recipe/268501/glute...\nBuckwheat Queen\n2023-01-22\n¼ cup raisins, ¼ cup golden raisins, ¼ cup dri...\n401.0\n20.0\n45.0\n5.0\n5.0\n6.0\n6.0\n40\n90\n1585\n12.0\n\n\n14425\nChef John's Irish Pork Stew\nhttps://www.allrecipes.com/recipe/236988/chef-...\nJohn Mitzewich\n2024-11-16\n1 (2 ½ pound) boneless pork shoulder, cut into...\n401.0\n19.0\n33.0\n20.0\n4.8\n333.0\n274.0\n25\n145\n170\n6.0\n\n\n\n\n14426 rows × 16 columns\n\n\n\n\ncuisines\n\n\n\n\n\n\n\n\nname\ncountry\nurl\nauthor\ndate_published\ningredients\ncalories\nfat\ncarbs\nprotein\navg_rating\ntotal_ratings\nreviews\nprep_time\ncook_time\ntotal_time\nservings\n\n\n\n\n0\nSaganaki (Flaming Greek Cheese)\nGreek\nhttps://www.allrecipes.com/recipe/263750/flami...\nJohn Mitzewich\n2024-02-07\n1 (4 ounce) package kasseri cheese, 1 tablespo...\n391.0\n25.0\n15.0\n16.0\n4.8\n25.0\n22.0\n10\n5\n15\n2.0\n\n\n1\nConey Island Knishes\nJewish\nhttps://www.allrecipes.com/recipe/272334/coney...\nJohn Mitzewich\n2024-11-26\n2 ¾ cups all-purpose flour, or more as needed,...\n301.0\n17.0\n31.0\n7.0\n4.6\n10.0\n9.0\n30\n75\n180\n16.0\n\n\n2\nDiana's Hawaiian Bread Rolls\nAustralian and New Zealander\nhttps://www.allrecipes.com/recipe/22797/dianas...\nCHIPPENDALE\n2022-07-14\n1 ½ cups warm water (110 degrees F/45 degrees ...\n64.0\n3.0\n9.0\n1.0\n4.3\n126.0\n104.0\n20\n15\n180\n12.0\n\n\n3\nChilean Pebre\nChilean\nhttps://www.allrecipes.com/recipe/273763/chile...\nHeidi\n2025-01-31\n½ cup chopped cilantro, ¼ cup olive oil, ¼ cup...\n106.0\n9.0\n7.0\n1.0\n5.0\n1.0\n1.0\n10\n0\n10\n6.0\n\n\n4\nTex-Mex Sheet Cake\nTex-Mex\nhttps://www.allrecipes.com/recipe/22388/tex-me...\nAnn\n2025-02-18\n2 cups all-purpose flour, 1 ½ cups brown sugar...\n449.0\n23.0\n58.0\n7.0\n3.8\n13.0\n11.0\n30\n15\n45\n15.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2213\nChicken Satay Bowls with Spicy Peanut Sauce\nThai\nhttps://www.allrecipes.com/chicken-satay-bowls...\nLaDonna Langwell\n2025-06-02\n1/4 cup coconut milk, 1 tablespoon brown sugar...\n716.0\n42.0\n41.0\n48.0\n5.0\n3.0\n3.0\n30\n15\n165\n4.0\n\n\n2214\nThe Best Ricotta Pancakes\nCanadian\nhttps://www.allrecipes.com/recipe/242225/the-b...\nWestCoastMom\n2025-03-11\n1 cup ricotta cheese plus, 2 tablespoons ricot...\n86.0\n4.0\n8.0\n5.0\n4.5\n41.0\n35.0\n15\n10\n25\n12.0\n\n\n2215\nBlack Pepper Chicken\nChinese\nhttps://www.allrecipes.com/black-pepper-chicke...\nBarrett Heald\n2024-03-05\n1/4 cup cornstarch, 1 tablespoon reduced-sodiu...\n484.0\n20.0\n44.0\n32.0\n4.4\n11.0\n10.0\n15\n15\n45\n4.0\n\n\n2216\nChicken Florentine\nFrench\nhttps://www.allrecipes.com/chicken-florentine-...\nRenu Dhar\n2024-01-18\n1/4 cup all-purpose flour, 3/4 teaspoon kosher...\n571.0\n33.0\n12.0\n50.0\n4.8\n17.0\n17.0\n15\n20\n35\n4.0\n\n\n2217\nIskender Kebab\nPersian\nhttps://www.allrecipes.com/recipe/88080/iskend...\nGATOULA\n2024-11-17\n4 pita bread rounds, 1 tablespoon olive oil, ...\n667.0\n36.0\n49.0\n37.0\n4.5\n17.0\n12.0\n15\n15\n30\n4.0\n\n\n\n\n2218 rows × 17 columns\n\n\n\n\ncuisines_filtered = cuisines[(cuisines['avg_rating']==5) & (cuisines['total_time']&lt;=10) & (cuisines['total_time']&gt;0)]\ncuisines_filtered\n\n\n\n\n\n\n\n\nname\ncountry\nurl\nauthor\ndate_published\ningredients\ncalories\nfat\ncarbs\nprotein\navg_rating\ntotal_ratings\nreviews\nprep_time\ncook_time\ntotal_time\nservings\n\n\n\n\n3\nChilean Pebre\nChilean\nhttps://www.allrecipes.com/recipe/273763/chile...\nHeidi\n2025-01-31\n½ cup chopped cilantro, ¼ cup olive oil, ¼ cup...\n106.0\n9.0\n7.0\n1.0\n5.0\n1.0\n1.0\n10\n0\n10\n6.0\n\n\n9\nPan con Tomate (Spanish Tomato Bread)\nSpanish\nhttps://www.allrecipes.com/pan-con-tomate-span...\nLuis Luna\n2025-06-02\n1 large tomato, chopped, 2 slices crusty bread...\n322.0\n16.0\n39.0\n7.0\n5.0\n2.0\n2.0\n5\n5\n10\n1.0\n\n\n243\nVietnamese Egg Coffee\nVietnamese\nhttps://www.allrecipes.com/recipe/283318/vietn...\nYoly\n2021-02-09\n1 large egg yolk, 2 tablespoons sweetened cond...\n175.0\n8.0\n21.0\n6.0\n5.0\n1.0\n1.0\n5\n0\n5\n1.0\n\n\n330\nSpanish Gin and Tonic (Gin Tonica)\nSpanish\nhttps://www.allrecipes.com/spanish-gin-and-ton...\nJohn Mitzewich\n2023-08-17\nwhole spices, such as juniper berries, pink pe...\n366.0\n2.0\n74.0\n7.0\n5.0\n1.0\n1.0\n9\n0\n9\n1.0\n\n\n507\nHomemade Za'atar\nLebanese\nhttps://www.allrecipes.com/recipe/262666/homem...\nSerena\n2022-08-22\n3 tablespoons sesame seeds, 3 tablespoons fres...\n29.0\n2.0\n2.0\n1.0\n5.0\n3.0\n2.0\n5\n3\n8\n6.0\n\n\n824\nCuban Crunchwrap\nCuban\nhttps://www.allrecipes.com/cuban-crunchwrap-re...\nNicole McLaughlin\n2025-04-03\n1 extra large flour tortilla (12 inches or lar...\n2010.0\n71.0\n264.0\n76.0\n5.0\n1.0\n1.0\n5\n5\n10\n1.0\n\n\n1061\nCoconut Rum Brazilian Lemonade\nBrazilian\nhttps://www.allrecipes.com/coconut-rum-brazili...\nNicole McLaughlin\n2024-06-13\n2 limes, 2 cups water, 1/2 cup coconut rum, 1/...\n169.0\n2.0\n28.0\n2.0\n5.0\n2.0\n2.0\n5\n0\n5\n3.0\n\n\n1216\nItalian Cherry Margarita\nItalian\nhttps://www.allrecipes.com/italian-cherry-marg...\nNicole McLaughlin\n2025-07-26\n2 fresh cherries, pitted, or more to taste, pl...\n222.0\n0.0\n16.0\n0.0\n5.0\n1.0\n1.0\n5\n0\n5\n1.0\n\n\n1226\nIndian Summer Raspberry Peach Sangria\nSpanish\nhttps://www.allrecipes.com/recipe/233339/india...\nRobert Farabaugh\n2025-07-24\n1 (750 milliliter) bottle red wine, 24 fluid o...\n175.0\n0.0\n22.0\n1.0\n5.0\n8.0\n4.0\n10\n0\n10\n8.0\n\n\n1516\nChocolate Santafereño (Colombian-Style Hot Cho...\nColombian\nhttps://www.allrecipes.com/chocolate-santafere...\nDevon O'Brien\n2024-10-07\n1 cup whole milk, 2 ounces bittersweet or dark...\n544.0\n32.0\n47.0\n17.0\n5.0\n1.0\n1.0\n5\n5\n10\n1.0\n\n\n1581\nLebanese 7 Spices\nLebanese\nhttps://www.allrecipes.com/recipe/269331/leban...\nBigDaddy\n2018-11-30\n1 tablespoon ground nutmeg, 1 tablespoon groun...\n15.0\n1.0\n3.0\n1.0\n5.0\n2.0\n2.0\n5\n0\n5\n10.0\n\n\n1654\nEasy Mojitos\nCuban\nhttps://www.allrecipes.com/recipe/229649/easy-...\nJennifer\n2025-05-12\n12 leaves mint, 2 lime slices, 1 teaspoon whi...\n121.0\n0.0\n7.0\n0.0\n5.0\n24.0\n20.0\n5\n0\n5\n1.0\n\n\n1822\nDave Matthews\nCanadian\nhttps://www.allrecipes.com/recipe/201058/dave-...\nlooloo2\n2022-09-25\n1 fluid ounce coconut-flavored rum, 1 fluid ou...\nNaN\nNaN\nNaN\nNaN\n5.0\n11.0\n8.0\n5\n0\n5\n1.0\n\n\n1996\nAuthentic Chimichurri\nArgentinian\nhttps://www.allrecipes.com/recipe/278453/authe...\nAvon- status quo PRO\n2024-06-27\n1 cup fresh parsley, ½ cup extra-virgin olive ...\n133.0\n14.0\n1.0\n0.0\n5.0\n5.0\n4.0\n10\n0\n10\n8.0\n\n\n\n\n\n\n\n\ncuisines_filtered.columns\n\nIndex(['name', 'country', 'url', 'author', 'date_published', 'ingredients',\n       'calories', 'fat', 'carbs', 'protein', 'avg_rating', 'total_ratings',\n       'reviews', 'prep_time', 'cook_time', 'total_time', 'servings'],\n      dtype='object')\n\n\n\ncuisines_final = cuisines_filtered[['name', 'country', 'ingredients',\n       'calories', 'fat', 'carbs', 'protein', \n       'prep_time', 'cook_time']].sort_values(by='country').reset_index(drop=True)\ncuisines_final\n\n\n\n\n\n\n\n\nname\ncountry\ningredients\ncalories\nfat\ncarbs\nprotein\nprep_time\ncook_time\n\n\n\n\n0\nAuthentic Chimichurri\nArgentinian\n1 cup fresh parsley, ½ cup extra-virgin olive ...\n133.0\n14.0\n1.0\n0.0\n10\n0\n\n\n1\nCoconut Rum Brazilian Lemonade\nBrazilian\n2 limes, 2 cups water, 1/2 cup coconut rum, 1/...\n169.0\n2.0\n28.0\n2.0\n5\n0\n\n\n2\nDave Matthews\nCanadian\n1 fluid ounce coconut-flavored rum, 1 fluid ou...\nNaN\nNaN\nNaN\nNaN\n5\n0\n\n\n3\nChilean Pebre\nChilean\n½ cup chopped cilantro, ¼ cup olive oil, ¼ cup...\n106.0\n9.0\n7.0\n1.0\n10\n0\n\n\n4\nChocolate Santafereño (Colombian-Style Hot Cho...\nColombian\n1 cup whole milk, 2 ounces bittersweet or dark...\n544.0\n32.0\n47.0\n17.0\n5\n5\n\n\n5\nCuban Crunchwrap\nCuban\n1 extra large flour tortilla (12 inches or lar...\n2010.0\n71.0\n264.0\n76.0\n5\n5\n\n\n6\nEasy Mojitos\nCuban\n12 leaves mint, 2 lime slices, 1 teaspoon whi...\n121.0\n0.0\n7.0\n0.0\n5\n0\n\n\n7\nItalian Cherry Margarita\nItalian\n2 fresh cherries, pitted, or more to taste, pl...\n222.0\n0.0\n16.0\n0.0\n5\n0\n\n\n8\nHomemade Za'atar\nLebanese\n3 tablespoons sesame seeds, 3 tablespoons fres...\n29.0\n2.0\n2.0\n1.0\n5\n3\n\n\n9\nLebanese 7 Spices\nLebanese\n1 tablespoon ground nutmeg, 1 tablespoon groun...\n15.0\n1.0\n3.0\n1.0\n5\n0\n\n\n10\nPan con Tomate (Spanish Tomato Bread)\nSpanish\n1 large tomato, chopped, 2 slices crusty bread...\n322.0\n16.0\n39.0\n7.0\n5\n5\n\n\n11\nSpanish Gin and Tonic (Gin Tonica)\nSpanish\nwhole spices, such as juniper berries, pink pe...\n366.0\n2.0\n74.0\n7.0\n9\n0\n\n\n12\nIndian Summer Raspberry Peach Sangria\nSpanish\n1 (750 milliliter) bottle red wine, 24 fluid o...\n175.0\n0.0\n22.0\n1.0\n10\n0\n\n\n13\nVietnamese Egg Coffee\nVietnamese\n1 large egg yolk, 2 tablespoons sweetened cond...\n175.0\n8.0\n21.0\n6.0\n5\n0\n\n\n\n\n\n\n\n\n# merge calories, fat, carbs, and protein cols into one col as a list of four elements.\ncuisines_final['nutrition'] = cuisines_final[['fat', 'carbs', 'protein']].values.tolist()\n# in calories, fat, protein, carbs, replace nan with 0.0\ncuisines_final[['calories', 'fat', 'carbs', 'protein']] = cuisines_final[['calories', 'fat', 'carbs', 'protein']].fillna(0.0)\n# convert nutrition col to string\ncuisines_final['nutrition'] = cuisines_final['nutrition'].apply(\n    lambda x: ', '.join([str(0.0 if pd.isna(i) else i) for i in x])\n)\ncuisines_final\n\n\n\n\n\n\n\n\nname\ncountry\ningredients\ncalories\nfat\ncarbs\nprotein\nprep_time\ncook_time\nnutrition\n\n\n\n\n0\nAuthentic Chimichurri\nArgentinian\n1 cup fresh parsley, ½ cup extra-virgin olive ...\n133.0\n14.0\n1.0\n0.0\n10\n0\n14.0, 1.0, 0.0\n\n\n1\nCoconut Rum Brazilian Lemonade\nBrazilian\n2 limes, 2 cups water, 1/2 cup coconut rum, 1/...\n169.0\n2.0\n28.0\n2.0\n5\n0\n2.0, 28.0, 2.0\n\n\n2\nDave Matthews\nCanadian\n1 fluid ounce coconut-flavored rum, 1 fluid ou...\n0.0\n0.0\n0.0\n0.0\n5\n0\n0.0, 0.0, 0.0\n\n\n3\nChilean Pebre\nChilean\n½ cup chopped cilantro, ¼ cup olive oil, ¼ cup...\n106.0\n9.0\n7.0\n1.0\n10\n0\n9.0, 7.0, 1.0\n\n\n4\nChocolate Santafereño (Colombian-Style Hot Cho...\nColombian\n1 cup whole milk, 2 ounces bittersweet or dark...\n544.0\n32.0\n47.0\n17.0\n5\n5\n32.0, 47.0, 17.0\n\n\n5\nCuban Crunchwrap\nCuban\n1 extra large flour tortilla (12 inches or lar...\n2010.0\n71.0\n264.0\n76.0\n5\n5\n71.0, 264.0, 76.0\n\n\n6\nEasy Mojitos\nCuban\n12 leaves mint, 2 lime slices, 1 teaspoon whi...\n121.0\n0.0\n7.0\n0.0\n5\n0\n0.0, 7.0, 0.0\n\n\n7\nItalian Cherry Margarita\nItalian\n2 fresh cherries, pitted, or more to taste, pl...\n222.0\n0.0\n16.0\n0.0\n5\n0\n0.0, 16.0, 0.0\n\n\n8\nHomemade Za'atar\nLebanese\n3 tablespoons sesame seeds, 3 tablespoons fres...\n29.0\n2.0\n2.0\n1.0\n5\n3\n2.0, 2.0, 1.0\n\n\n9\nLebanese 7 Spices\nLebanese\n1 tablespoon ground nutmeg, 1 tablespoon groun...\n15.0\n1.0\n3.0\n1.0\n5\n0\n1.0, 3.0, 1.0\n\n\n10\nPan con Tomate (Spanish Tomato Bread)\nSpanish\n1 large tomato, chopped, 2 slices crusty bread...\n322.0\n16.0\n39.0\n7.0\n5\n5\n16.0, 39.0, 7.0\n\n\n11\nSpanish Gin and Tonic (Gin Tonica)\nSpanish\nwhole spices, such as juniper berries, pink pe...\n366.0\n2.0\n74.0\n7.0\n9\n0\n2.0, 74.0, 7.0\n\n\n12\nIndian Summer Raspberry Peach Sangria\nSpanish\n1 (750 milliliter) bottle red wine, 24 fluid o...\n175.0\n0.0\n22.0\n1.0\n10\n0\n0.0, 22.0, 1.0\n\n\n13\nVietnamese Egg Coffee\nVietnamese\n1 large egg yolk, 2 tablespoons sweetened cond...\n175.0\n8.0\n21.0\n6.0\n5\n0\n8.0, 21.0, 6.0\n\n\n\n\n\n\n\n\ncuisines_final['country'].unique()\n\narray(['Argentinian', 'Brazilian', 'Canadian', 'Chilean', 'Colombian',\n       'Cuban', 'Italian', 'Lebanese', 'Spanish', 'Vietnamese'],\n      dtype=object)\n\n\n\ncountry_list = ['Argentina', 'Brazil', 'Canada', 'Chile', 'Colombia', 'Cuba', 'Italy', 'Lebanon', 'Spain', 'Vietnam']\n\n\n# save flag for each country\nfor country in country_list:\n    img = fp.get_flag_img(country)\n    img.save(f'{country}_flag.png')\n\n\ncountry_image_paths = {\n    'Argentinian': 'Argentina_flag.png',\n    'Brazilian': 'Brazil_flag.png',\n    'Canadian': 'Canada_flag.png',\n    'Chilean': 'Chile_flag.png',\n    'Colombian': 'Colombia_flag.png',\n    'Cuban': 'Cuba_flag.png',\n    'Italian': 'Italy_flag.png',\n    'Lebanese': 'Lebanon_flag.png',\n    'Spanish': 'Spain_flag.png',\n    'Vietnamese': 'Vietnam_flag.png'\n}\n\ncuisines_final['flag']=cuisines_final['country'].map(country_image_paths)\n\n\ncuisines_final.columns\n\nIndex(['name', 'country', 'ingredients', 'calories', 'fat', 'carbs', 'protein',\n       'prep_time', 'cook_time', 'nutrition', 'flag'],\n      dtype='object')\n\n\n\ndef create_tally_image(n, color='black'):\n    \"\"\"Create tally mark image with diagonal going from bottom-left to top-right\"\"\"\n    groups = n // 5\n    remainder = n % 5\n    \n    # Calculate figure width based on number of marks\n    # Each group takes 0.7 width (0.5 for marks + 0.2 space)\n    # Each remainder takes 0.1 width\n    total_width = max(1.0, 0.7 * groups + 0.1 * remainder + 0.2)\n    \n    fig, ax = plt.subplots(figsize=(total_width, 0.5))\n    ax.set_xlim(0, total_width)\n    ax.set_ylim(0, 1)\n    ax.axis('off')\n    \n    x_start = 0.1\n    for i in range(groups):\n        # Four vertical bars\n        for j in range(4):\n            ax.plot([x_start + j*0.1, x_start + j*0.1], [0.2, 0.8], color=color, lw=2)\n        \n        # Diagonal from bottom-left to top-right\n        # Starts below and left of first line, ends above and right of fourth line\n        ax.plot([x_start - 0.05, x_start + 0.35], [0.15, 0.85], color=color, lw=2)\n        \n        # Add space after group\n        x_start += 0.7  # 0.5 for marks + 0.2 space\n    \n    # Draw remaining marks\n    for j in range(remainder):\n        ax.plot([x_start + j*0.1, x_start + j*0.1], [0.2, 0.8], color=color, lw=2)\n    \n    # Save to memory\n    buf = io.BytesIO()\n    plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0.1, transparent=True)\n    buf.seek(0)\n    img_str = base64.b64encode(buf.read()).decode('utf-8')\n    plt.close(fig)\n    \n    return f'&lt;img src=\"data:image/png;base64,{img_str}\" style=\"height:30px\"&gt;'\n\ncuisines_final[\"prep_Tally\"] = cuisines_final[\"prep_time\"].apply(create_tally_image, args=('purple',))\ncuisines_final[\"cook_Tally\"] = cuisines_final[\"cook_time\"].apply(create_tally_image, args=('dodgerblue',))\n\n\ngt_table = (\nGT(cuisines_final[['name', 'flag', 'ingredients', 'prep_Tally', 'cook_Tally', 'nutrition','calories']], rowname_col='name')\n.cols_width({\n      'name': '15%',\n      'flag': '8%',\n      'ingredients': '45%',\n      'calories': '6%',\n#      'prep_time': '5%',\n#      'cook_time': '6%',\n      'nutrition': '15%',\n      'prep_Tally': '5%',\n      'cook_Tally': '6%'\n  })\n.tab_spanner(label=\"Time (mins)\", columns=[\"prep_Tally\", \"cook_Tally\"])\n.tab_spanner(label=\"Nutrition\", columns=[\"calories\",\"nutrition\"])\n.tab_style(\n    style=style.text(weight=\"bold\"),\n    locations=loc.spanner_labels([\"Time (mins)\", \"Nutrition\"])\n  )\n.tab_header(\n    title=md(\"**Five star** cuisines having preparation time upto **ten minutes**.\"),\n#    subtitle=\"Recipes with 5 star ratings and total time less than 10 minutes\"\n)\n.cols_label(\n    prep_Tally=\"Prep\",\n    cook_Tally=\"Cook\",\n    calories=\"Calories\",\n    flag=\"Country\",\n    ingredients=\"Ingredients\",\n    nutrition=\"Fat, Carbs, Protein\"\n)\n.fmt_number(columns=[\"calories\"], decimals=0)\n.fmt_nanoplot(columns=\"nutrition\", plot_type='bar',options=nanoplot_options(data_bar_fill_color=\"green\"))\n.fmt_markdown(columns='ingredients')\n.cols_align('center')\n.cols_align(align=\"left\", columns=[\"ingredients\"])\n.tab_style(\n    style=style.text(align=\"center\"),                # Center header text\n    locations=loc.column_labels(columns=[\"ingredients\"])\n    )\n.tab_style(\n    style=style.text(weight=\"bold\"),                # Center header text\n    locations=loc.column_labels(columns=[\"flag\",\"ingredients\"])\n    )\n.data_color(\n        columns=['calories'],\n        palette='Reds',\n        domain=[cuisines_final['calories'].min(), cuisines_final['calories'].max()]\n        ) \n.tab_style(\nstyle=[\n        style.text(whitespace='normal'),  # Text wrapping\n        style.fill(color='papayawhip')     # Background fill\n    ],\nlocations=loc.body(columns='ingredients'))\n.tab_style(\n    style=[\n        style.text(whitespace='normal'),\n        style.fill(color='aliceblue')\n    ],\n    locations=loc.stub()\n  )\n.tab_style(\n    style=style.text(color=\"purple\"), #, weight=\"bold\", size=\"16px\"),\n    locations=loc.column_labels(columns=[\"prep_Tally\"])\n)\n.tab_style(\n    style=style.text(color=\"dodgerblue\"),\n    locations=loc.column_labels(columns=[\"cook_Tally\"])\n)\n.tab_options(table_width='100%')\n.fmt_image(\n    columns=['flag'], path=\".\"  \n)\n.tab_source_note(\n    source_note=\"Data: allrecipies.com\"\n)\n.tab_style(\n    style=style.css(\"border:0px\"),\n    locations=loc.body(columns=[\"flag\"])\n)\n.tab_style(\n    style=style.css(\"border-radius: 50px;\"),\n    locations=loc.body(columns=[\"calories\"])\n)\n\n\n)\n\n#gt_table.save(\"table.html\")\n\n# Save html without selenium\nhtml = gt_table.as_raw_html()\nwith open(\"table.html\", \"w\") as f:\n    f.write(html)\n\ngt_table\n\n#gt_table.save(\n#    file=\"my_table.png\",\n#    selector=\"table\",       # HTML selector to capture\n#    scale=1.0,               # Zoom level\n#    expand=5,                # Padding around the image\n#    window_size=(1000, 800)  # Size of the browser window used for rendering\n#)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFive star cuisines having preparation time upto ten minutes.\n\n\n\nCountry\nIngredients\nTime (mins)\nNutrition\n\n\nPrep\nCook\nCalories\nFat, Carbs, Protein\n\n\n\n\nAuthentic Chimichurri\n\n1 cup fresh parsley, ½ cup extra-virgin olive oil, 2 tablespoons fresh oregano, 2 tablespoons red wine vinegar, 1 tablespoon water, 1 jalapeño chile pepper, seeded, 2 cloves garlic, chopped, ½ teaspoon salt, ½ teaspoon ground black pepper\n\n\n133\n\n\n\n\n1401410\n\n\n\n\nCoconut Rum Brazilian Lemonade\n\n2 limes, 2 cups water, 1/2 cup coconut rum, 1/4 cup sweetened condensed milk, 2 cups ice\n\n\n169\n\n\n\n\n2802282\n\n\n\n\nDave Matthews\n\n1 fluid ounce coconut-flavored rum, 1 fluid ounce amaretto liqueur, ¼ fluid ounce fresh lime juice, ¾ fluid ounce pineapple juice, ½ fluid ounce cranberry juice\n\n\n0\n\n\n\n\n5-5000\n\n\n\n\nChilean Pebre\n\n½ cup chopped cilantro, ¼ cup olive oil, ¼ cup red wine vinegar, 1 tomato, chopped, 1 small onion, chopped, 1 lemon, juiced, 2 chile peppers, seeded and chopped, 2 tablespoons minced garlic, salt and pepper to taste\n\n\n106\n\n\n\n\n90971\n\n\n\n\nChocolate Santafereño (Colombian-Style Hot Chocolate)\n\n1 cup whole milk, 2 ounces bittersweet or dark chocolate(85% cacao), 1 pinch cinnamon, 1 pinch ground cloves, 1 (1-ounce) mozzarella cube\n\n\n544\n\n\n\n\n470324717\n\n\n\n\nCuban Crunchwrap\n\n1 extra large flour tortilla (12 inches or larger), 2 ounces sliced ham, 2 ounces cooked pork, 2 teaspoons mustard, 1/2 cup kettle potato chips, 1/4 cup shredded Swiss cheese, 4 dill pickle slices, 1 tablespoon butter\n\n\n2,010\n\n\n\n\n26407126476\n\n\n\n\nEasy Mojitos\n\n12 leaves mint, 2 lime slices, 1 teaspoon white sugar, or more to taste, ¼ cup ice cubes, or as needed, 1 (1.5 fluid ounce) jigger rum (such as Bacardi), 4 ½ ounces diet lemon-lime soda (such as Diet Sprite)\n\n\n121\n\n\n\n\n70070\n\n\n\n\nItalian Cherry Margarita\n\n2 fresh cherries, pitted, or more to taste, plus more for garnish, 2 teaspoons fresh lime juice, 1 1/2 fluid ounces reposado tequila, 1 fluid ounce amaretto liqueur\n\n\n222\n\n\n\n\n1600160\n\n\n\n\nHomemade Za'atar\n\n3 tablespoons sesame seeds, 3 tablespoons fresh thyme leaves, 1 tablespoon sumac powder, ½ teaspoon salt\n\n\n29\n\n\n\n\n20221\n\n\n\n\nLebanese 7 Spices\n\n1 tablespoon ground nutmeg, 1 tablespoon ground ginger, 1 tablespoon ground allspice, 1 tablespoon fenugreek seeds, 1 tablespoon freshly ground black pepper, 1 teaspoon ground cloves, 1 teaspoon cinnamon\n\n\n15\n\n\n\n\n30131\n\n\n\n\nPan con Tomate (Spanish Tomato Bread)\n\n1 large tomato, chopped, 2 slices crusty bread, 1 clove garlic, halved, 1 tablespoon extra-virgin olive oil, or to taste, sea salt to taste\n\n\n322\n\n\n\n\n39016397\n\n\n\n\nSpanish Gin and Tonic (Gin Tonica)\n\nwhole spices, such as juniper berries, pink peppercorns, star anise, cardamom pods, cloves, or cinnamon sticks, 2 fluid ounces gin, 4 to 6 fluid ounces tonic water, to taste, sliced fresh citrus fruit, such as lemon, orange, lime, or blood orange, fresh herbs, such as basil, thyme, mint, rosemary, or tarragon\n\n\n366\n\n\n\n\n7402747\n\n\n\n\nIndian Summer Raspberry Peach Sangria\n\n1 (750 milliliter) bottle red wine, 24 fluid ounces raspberry-flavored soda water, ½ cup peach schnapps, ½ cup pomegranate juice, ½ cup fresh lemon juice, 2 peaches, sliced, 2 lemons, sliced, 1 cup fresh raspberries, 1 orange, sliced, 4 cups ice cubes, or as desired\n\n\n175\n\n\n\n\n2200221\n\n\n\n\nVietnamese Egg Coffee\n\n1 large egg yolk, 2 tablespoons sweetened condensed milk, ½ cup hot strong coffee\n\n\n175\n\n\n\n\n2108216\n\n\n\n\n\nData: allrecipies.com"
  },
  {
    "objectID": "posts/Netflix_viewing/Netflix_viewing.html",
    "href": "posts/Netflix_viewing/Netflix_viewing.html",
    "title": "Netflix viewing data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport re\n\n\nmovies = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-29/movies.csv')\nshows = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-29/shows.csv')\n\n\nmovies\n\n\n\n\n\n\n\n\nsource\nreport\ntitle\navailable_globally\nrelease_date\nhours_viewed\nruntime\nviews\n\n\n\n\n0\n1_What_We_Watched_A_Netflix_Engagement_Report_...\n2025Jan-Jun\nBack in Action\nYes\n2025-01-17\n313000000.0\n1H 54M 0S\n164700000.0\n\n\n1\n1_What_We_Watched_A_Netflix_Engagement_Report_...\n2025Jan-Jun\nSTRAW\nYes\n2025-06-06\n185200000.0\n1H 48M 0S\n102900000.0\n\n\n2\n1_What_We_Watched_A_Netflix_Engagement_Report_...\n2025Jan-Jun\nThe Life List\nYes\n2025-03-28\n198900000.0\n2H 5M 0S\n95500000.0\n\n\n3\n1_What_We_Watched_A_Netflix_Engagement_Report_...\n2025Jan-Jun\nExterritorial\nYes\n2025-04-30\n159000000.0\n1H 49M 0S\n87500000.0\n\n\n4\n1_What_We_Watched_A_Netflix_Engagement_Report_...\n2025Jan-Jun\nHavoc\nYes\n2025-04-25\n154900000.0\n1H 47M 0S\n86900000.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n36116\n4_What_We_Watched_A_Netflix_Engagement_Report_...\n2023Jul-Dec\nالرجل الرابع\nNo\nNaN\n100000.0\n1H 32M 0S\n100000.0\n\n\n36117\n4_What_We_Watched_A_Netflix_Engagement_Report_...\n2023Jul-Dec\n두근두근 내 인생\nNo\nNaN\n100000.0\n1H 52M 0S\n100000.0\n\n\n36118\n4_What_We_Watched_A_Netflix_Engagement_Report_...\n2023Jul-Dec\n라디오 스타\nNo\nNaN\n100000.0\n1H 56M 0S\n100000.0\n\n\n36119\n4_What_We_Watched_A_Netflix_Engagement_Report_...\n2023Jul-Dec\n선생 김봉두\nNo\nNaN\n100000.0\n1H 57M 0S\n100000.0\n\n\n36120\n4_What_We_Watched_A_Netflix_Engagement_Report_...\n2023Jul-Dec\n표적\nNo\nNaN\n100000.0\n1H 38M 0S\n100000.0\n\n\n\n\n36121 rows × 8 columns\n\n\n\n\nshows\n\n\n\n\n\n\n\n\nsource\nreport\ntitle\navailable_globally\nrelease_date\nhours_viewed\nruntime\nviews\n\n\n\n\n0\n1_What_We_Watched_A_Netflix_Engagement_Report_...\n2025Jan-Jun\nAdolescence: Limited Series\nYes\n2025-03-13\n555100000.0\n3H 50M 0S\n144800000.0\n\n\n1\n1_What_We_Watched_A_Netflix_Engagement_Report_...\n2025Jan-Jun\nSquid Game: Season 2 // 오징어 게임: 시즌 2\nYes\n2024-12-26\n840300000.0\n7H 10M 0S\n117300000.0\n\n\n2\n1_What_We_Watched_A_Netflix_Engagement_Report_...\n2025Jan-Jun\nSquid Game: Season 3 // 오징어 게임: 시즌 3\nYes\n2025-06-27\n438600000.0\n6H 8M 0S\n71500000.0\n\n\n3\n1_What_We_Watched_A_Netflix_Engagement_Report_...\n2025Jan-Jun\nZero Day: Limited Series\nYes\n2025-02-20\n315800000.0\n5H 9M 0S\n61300000.0\n\n\n4\n1_What_We_Watched_A_Netflix_Engagement_Report_...\n2025Jan-Jun\nMissing You: Limited Series\nYes\n2025-01-01\n218600000.0\n3H 46M 0S\n58000000.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n27798\n4_What_We_Watched_A_Netflix_Engagement_Report_...\n2023Jul-Dec\nWe Are Black and British: Season 1\nNo\nNaN\n100000.0\n1H 57M 0S\n100000.0\n\n\n27799\n4_What_We_Watched_A_Netflix_Engagement_Report_...\n2023Jul-Dec\nWhitney Cummings: Can I Touch It?\nYes\n2019-07-30\n100000.0\n59M 0S\n100000.0\n\n\n27800\n4_What_We_Watched_A_Netflix_Engagement_Report_...\n2023Jul-Dec\nWhitney Cummings: Jokes\nNo\n2022-07-26\n100000.0\n1H 0M 0S\n100000.0\n\n\n27801\n4_What_We_Watched_A_Netflix_Engagement_Report_...\n2023Jul-Dec\nWhose Vote Counts, Explained: Limited Series\nYes\n2020-09-28\n100000.0\n1H 15M 0S\n100000.0\n\n\n27802\n4_What_We_Watched_A_Netflix_Engagement_Report_...\n2023Jul-Dec\nZach Galifianakis: Live at the Purple Onion\nNo\nNaN\n100000.0\n1H 1M 0S\n100000.0\n\n\n\n\n27803 rows × 8 columns\n\n\n\n\nmovies[\"release_date\"] = pd.to_datetime(movies[\"release_date\"])\nshows[\"release_date\"] = pd.to_datetime(shows[\"release_date\"])\n\n\nprint(movies[\"hours_viewed\"].sum() / (24 * 365))\nprint(shows[\"hours_viewed\"].sum() / (24 * 365))\n\n11504006.849315068\n31113367.579908676\n\n\n\nmovies.groupby('title').views.sum().sort_values(ascending=False).head(10)\n\ntitle\nThe Boss Baby                  221300000.0\nThe Super Mario Bros. Movie    209900000.0\nLeo                            207200000.0\nLeave the World Behind         205100000.0\nDamsel                         194500000.0\nMinions                        190900000.0\nCarry-On                       185000000.0\nLift                           175800000.0\nSing (2016)                    173700000.0\nDr. Seuss' The Grinch          168300000.0\nName: views, dtype: float64\n\n\n\nshows.groupby('title').views.sum().sort_values(ascending=False).head(10)\n\ntitle\nSquid Game: Season 2 // 오징어 게임: 시즌 2    203800000.0\nAdolescence: Limited Series             144800000.0\nFool Me Once: Limited Series            129000000.0\nBridgerton: Season 3                    123300000.0\nBaby Reindeer: Limited Series           100400000.0\nONE PIECE: Season 1                      98400000.0\nThe Gentlemen: Season 1                  97900000.0\nPeppa Pig: Season 6                      92600000.0\nAvatar The Last Airbender: Season 1      86900000.0\nThe Perfect Couple: Season 1             86400000.0\nName: views, dtype: float64\n\n\n\nmovies = movies.loc[~movies[\"available_globally\"].isin([\"Available Globally?\"])]\nshows = shows.loc[~shows[\"available_globally\"].isin([\"Available Globally?\"])]\n\n\nfig, ax = plt.subplots(1,2, figsize=(10,5), sharey=True)\n\n# Removed 'line_kws' argument as it is not valid for sns.scatterplot\nsns.scatterplot(data=movies, x=\"release_date\", y=\"views\", ax=ax[0], alpha=0.5, hue=\"available_globally\")\n# plot median line\nax[0].axhline(movies[\"views\"].quantile(0.99), color=\"orange\", linestyle=\"--\")\nsns.scatterplot(data=shows, x=\"release_date\", y=\"views\", ax=ax[1], alpha=0.6, hue=\"available_globally\")\nax[1].axhline(shows[\"views\"].quantile(0.99), color=\"orange\", linestyle=\"--\")\n\n# add labels to top 5 movies and shows\ntop5_m = movies.groupby('title').agg({'views': 'sum', 'release_date': 'first'}).sort_values(by='views', ascending=False).head(5)\ntop5_s = shows.groupby('title').agg({'views': 'sum', 'release_date': 'first'}).sort_values(by='views', ascending=False).head(5)\n\n# Filter out rows with None in 'release_date'\ntop5_m = top5_m[top5_m['release_date'].notna()]\ntop5_s = top5_s[top5_s['release_date'].notna()]\n\n#print(top5_m)\n#print(top5_s)\n\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n# make word cloud of movie titles\n\nmovies[\"title_mod\"] = movies[\"title\"].str.replace(\"Movie\", \"\")\nmovies_title = \" \".join([str(x) for x in movies[\"title_mod\"]])\nmovies_wordcloud = WordCloud().generate(movies_title)\nplt.imshow(movies_wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# remove non-english characters\npattern = r'[^a-zA-Z0-9\\s]' \nshows[\"title_mod\"] = shows[\"title\"].str.replace(pattern, '', regex=True)\n\n\nst = shows[\"title_mod\"].str.replace(r\"(Season|Limited Series|Temporada|Serie)\", \"\", regex=True)\n\nshows_title = \" \".join([str(x) for x in st])\nshows_title.replace(\"Season\", \"\")\nshows_wordcloud = WordCloud().generate(shows_title)\nplt.imshow(shows_wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig,ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].imshow(movies_wordcloud, interpolation='bilinear')\nax[1].imshow(shows_wordcloud, interpolation='bilinear')\n\nax[0].axis('off')\nax[1].axis('off')\nplt.tight_layout()\n\nfig.suptitle(\"Word cloud of movie (left) and show (right) titles in the Netflix viewing data \\nfrom late 2023 through the first half of 2025.\", \n                ha=\"left\", x=0.025, y=0.85, fontsize=16, color='white')\nfig.set_facecolor('black')\n#plt.savefig(\"wordcloud.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\n\nwith open(\"wordcloud.svg\", \"w\", encoding=\"utf-8\") as f:\n            f.write(movies_wordcloud.to_svg())"
  },
  {
    "objectID": "posts/Netflix_viewing/Netflix_viewing.html#word-cloud",
    "href": "posts/Netflix_viewing/Netflix_viewing.html#word-cloud",
    "title": "Netflix viewing data",
    "section": "",
    "text": "# make word cloud of movie titles\n\nmovies[\"title_mod\"] = movies[\"title\"].str.replace(\"Movie\", \"\")\nmovies_title = \" \".join([str(x) for x in movies[\"title_mod\"]])\nmovies_wordcloud = WordCloud().generate(movies_title)\nplt.imshow(movies_wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# remove non-english characters\npattern = r'[^a-zA-Z0-9\\s]' \nshows[\"title_mod\"] = shows[\"title\"].str.replace(pattern, '', regex=True)\n\n\nst = shows[\"title_mod\"].str.replace(r\"(Season|Limited Series|Temporada|Serie)\", \"\", regex=True)\n\nshows_title = \" \".join([str(x) for x in st])\nshows_title.replace(\"Season\", \"\")\nshows_wordcloud = WordCloud().generate(shows_title)\nplt.imshow(shows_wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig,ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].imshow(movies_wordcloud, interpolation='bilinear')\nax[1].imshow(shows_wordcloud, interpolation='bilinear')\n\nax[0].axis('off')\nax[1].axis('off')\nplt.tight_layout()\n\nfig.suptitle(\"Word cloud of movie (left) and show (right) titles in the Netflix viewing data \\nfrom late 2023 through the first half of 2025.\", \n                ha=\"left\", x=0.025, y=0.85, fontsize=16, color='white')\nfig.set_facecolor('black')\n#plt.savefig(\"wordcloud.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\n\nwith open(\"wordcloud.svg\", \"w\", encoding=\"utf-8\") as f:\n            f.write(movies_wordcloud.to_svg())"
  },
  {
    "objectID": "posts/Measles_cases/Measles_cases.html",
    "href": "posts/Measles_cases/Measles_cases.html",
    "title": "Measles cases across the world",
    "section": "",
    "text": "TidyTuesday data for 2025-06-24\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncases_month = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-24/cases_month.csv')\ncases_year = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-24/cases_year.csv')\ncases_month\n\n\n\n\n\n\n\n\nregion\ncountry\niso3\nyear\nmonth\nmeasles_suspect\nmeasles_clinical\nmeasles_epi_linked\nmeasles_lab_confirmed\nmeasles_total\nrubella_clinical\nrubella_epi_linked\nrubella_lab_confirmed\nrubella_total\ndiscarded\n\n\n\n\n0\nAFR\nAlgeria\nDZA\n2012\n1\n8.0\n6.0\n0.0\n2.0\n8.0\nNaN\nNaN\nNaN\nNaN\n0.0\n\n\n1\nAFR\nAlgeria\nDZA\n2012\n2\n10.0\n10.0\n0.0\n0.0\n10.0\nNaN\nNaN\nNaN\nNaN\n0.0\n\n\n2\nAFR\nAlgeria\nDZA\n2012\n3\n17.0\n17.0\n0.0\n0.0\n17.0\nNaN\nNaN\nNaN\nNaN\n0.0\n\n\n3\nAFR\nAlgeria\nDZA\n2012\n4\n7.0\n5.0\n0.0\n0.0\n5.0\n0.0\n0.0\n1.0\n1.0\n2.0\n\n\n4\nAFR\nAlgeria\nDZA\n2012\n5\n14.0\n11.0\n0.0\n0.0\n11.0\n0.0\n0.0\n3.0\n3.0\n3.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n22775\nWPR\nViet Nam\nVNM\n2024\n10\n379.0\n19.0\n56.0\n256.0\n331.0\n5.0\n0.0\n5.0\n10.0\n48.0\n\n\n22776\nWPR\nViet Nam\nVNM\n2024\n11\n584.0\n37.0\n125.0\n347.0\n509.0\n0.0\n0.0\n1.0\n1.0\n75.0\n\n\n22777\nWPR\nViet Nam\nVNM\n2024\n12\n588.0\n56.0\n134.0\n338.0\n528.0\n0.0\n0.0\n1.0\n1.0\n60.0\n\n\n22778\nWPR\nViet Nam\nVNM\n2025\n1\n156.0\n7.0\n0.0\n124.0\n131.0\nNaN\nNaN\nNaN\nNaN\n25.0\n\n\n22779\nWPR\nViet Nam\nVNM\n2025\n2\n22.0\n0.0\n0.0\n20.0\n20.0\nNaN\nNaN\nNaN\nNaN\n2.0\n\n\n\n\n22780 rows × 15 columns\n\n\n\n\ncases_year\n\n\n\n\n\n\n\n\nregion\ncountry\niso3\nyear\ntotal_population\nannualized_population_most_recent_year_only\ntotal_suspected_measles_rubella_cases\nmeasles_total\nmeasles_lab_confirmed\nmeasles_epi_linked\nmeasles_clinical\nmeasles_incidence_rate_per_1000000_total_population\nrubella_total\nrubella_lab_confirmed\nrubella_epi_linked\nrubella_clinical\nrubella_incidence_rate_per_1000000_total_population\ndiscarded_cases\ndiscarded_non_measles_rubella_cases_per_100000_total_population\n\n\n\n\n0\nAFRO\nAlgeria\nDZA\n2012\n37646166\n37646166\n76.0\n55\n2\n0\n53\n1.46\n13\n13\n0\n0\n0.35\n8.0\n0.02\n\n\n1\nAFRO\nAlgeria\nDZA\n2013\n38414172\n38414172\n85.0\n0\n0\n0\n0\n0.00\n29\n29\n0\n0\n0.75\n56.0\n0.15\n\n\n2\nAFRO\nAlgeria\nDZA\n2014\n39205031\n39205031\n49.0\n0\n0\n0\n0\n0.00\n3\n3\n0\n0\n0.08\n46.0\n0.12\n\n\n3\nAFRO\nAlgeria\nDZA\n2015\n40019529\n40019529\n109.0\n62\n2\n60\n0\n1.55\n2\n2\n0\n0\n0.05\n45.0\n0.11\n\n\n4\nAFRO\nAlgeria\nDZA\n2016\n40850721\n40850721\n93.0\n49\n21\n27\n1\n1.20\n11\n11\n0\n0\n0.27\n33.0\n0.08\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2377\nWPRO\nViet Nam\nVNM\n2021\n98935099\n98935099\n346.0\n180\n33\n0\n147\n1.82\n6\n5\n0\n1\n0.06\n160.0\n0.16\n\n\n2378\nWPRO\nViet Nam\nVNM\n2022\n99680655\n99680655\n585.0\n38\n10\n1\n27\n0.38\n25\n21\n0\n4\n0.25\n522.0\n0.52\n\n\n2379\nWPRO\nViet Nam\nVNM\n2023\n100352192\n100352192\n499.0\n98\n8\n2\n88\n0.98\n42\n34\n0\n8\n0.42\n359.0\n0.36\n\n\n2380\nWPRO\nViet Nam\nVNM\n2024\n100987687\n100987687\n2807.0\n2105\n1500\n412\n193\n20.84\n41\n22\n0\n19\n0.41\n661.0\n0.65\n\n\n2381\nWPRO\nViet Nam\nVNM\n2025\n101598527\n42332720\n178.0\n151\n144\n0\n7\n3.57\n0\n0\n0\n0\n0.00\n27.0\n0.06\n\n\n\n\n2382 rows × 19 columns\n\n\n\n\ncases_month.groupby(\"country\").size().sort_values(ascending=False)\n\ncountry\nPakistan                            162\nAfghanistan                         161\nThailand                            161\nDemocratic Republic of the Congo    161\nCôte d'Ivoire                       161\n                                   ... \nCabo Verde                            7\nSamoa                                 6\nDominica                              5\nTonga                                 3\nMonaco                                3\nLength: 193, dtype: int64\n\n\n\ndf_grp = cases_month.groupby([\"month\",\"year\"])[[\"measles_total\", \"rubella_total\"]].sum().reset_index()\ndf_grp\n\n\n\n\n\n\n\n\nmonth\nyear\nmeasles_total\nrubella_total\n\n\n\n\n0\n1\n2012\n15412.0\n897.0\n\n\n1\n1\n2013\n18572.0\n3990.0\n\n\n2\n1\n2014\n38442.0\n4378.0\n\n\n3\n1\n2015\n26097.0\n2788.0\n\n\n4\n1\n2016\n22264.0\n3596.0\n\n\n...\n...\n...\n...\n...\n\n\n157\n12\n2020\n1631.0\n352.0\n\n\n158\n12\n2021\n3593.0\n639.0\n\n\n159\n12\n2022\n22938.0\n871.0\n\n\n160\n12\n2023\n29330.0\n493.0\n\n\n161\n12\n2024\n11199.0\n1288.0\n\n\n\n\n162 rows × 4 columns\n\n\n\n\nimport matplotlib.font_manager as fm\nmonospace_font = fm.FontProperties(family='monospace')\n\n\nmonths_order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n                'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n\nfig, ax = plt.subplots(figsize=(10, 6))\njitter = 0.15\ndf_grp['month_measles'] = df_grp['month'] - jitter\ndf_grp['month_rubella'] = df_grp['month'] + jitter\n\ns1 = sns.scatterplot(data=df_grp, y=\"month_measles\", x=\"year\", size=\"measles_total\", sizes=(25,250),\n                hue=\"measles_total\", marker=\"o\", palette=\"Oranges\", legend='brief')\n\nhandles1, labels1 = s1.get_legend_handles_labels()\nlabels1 = [f\"{int(x):,}\" for x in labels1]\nlegend1 = ax.legend(handles1, labels1, title=\"Measles Cases\", loc=\"upper left\", \n                    bbox_to_anchor=(1, 0.6), frameon=False)\nax.add_artist(legend1)  \n\ns2 = sns.scatterplot(data=df_grp, y=\"month_rubella\", x=\"year\", size=\"rubella_total\", sizes=(5,50),\n                hue=\"rubella_total\", marker=\"s\", palette=\"Blues\", legend='brief')\n\nall_handles, all_labels = s2.get_legend_handles_labels()\n\nrubella_handles = all_handles[len(handles1):]\nrubella_labels = all_labels[len(labels1):] #[l for h, l in zip(all_handles, all_labels) if h not in handles1]\nrubella_labels = [f\"{int(x):,}\" for x in rubella_labels]\n\nlegend2 = ax.legend(rubella_handles, rubella_labels, title=\"Rubella Cases\", loc=\"upper left\", \n                    bbox_to_anchor=(1, 0.3), frameon=False)\n\ntitle = \"Timeline of global measles and rubella (M&R) cases.\"\nsub_title = \"Contagious diseases like M&R exhibit seasonality, with incidence peaking during the winter/spring transition.\\nRestrictions imposed due to Covid-19 led to a reduction in the spread of M&R viruses.\"\nplt.title(textwrap.fill(title, width=60), fontfamily=\"Serif\", loc='left',pad=35, fontweight=\"bold\")\n#plt.suptitle(textwrap.fill(sub_title, width=90), fontfamily=\"Serif\", y=0.82, x=.05, ha=\"left\", fontsize=10)\nplt.suptitle(sub_title, fontfamily=\"Serif\", y=0.86, x=.05, ha=\"left\", fontsize=10)\nplt.xlabel(\"\")\nplt.ylabel(\"\")\nplt.yticks(ticks=range(1, 13), labels=months_order,fontfamily='monospace')\nplt.xticks(ticks=range(2012,2026),fontfamily='monospace')\nax.invert_yaxis()\nplt.tight_layout()\n#plt.savefig('measles_cases.png', bbox_inches='tight', pad_inches=0.1, dpi=300)\nplt.show()"
  },
  {
    "objectID": "posts/Income_inequality/income_inequality.html",
    "href": "posts/Income_inequality/income_inequality.html",
    "title": "Income Inequality Before and After Taxes",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\nimport textwrap\n\n\nincome_inequality_processed = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-05/income_inequality_processed.csv')\nincome_inequality_raw = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-05/income_inequality_raw.csv')\n\n\nincome_inequality_processed\n\n\n\n\n\n\n\n\nEntity\nCode\nYear\ngini_mi_eq\ngini_dhi_eq\n\n\n\n\n0\nAustralia\nAUS\n1989\n0.431\n0.304\n\n\n1\nAustralia\nAUS\n1995\n0.470\n0.311\n\n\n2\nAustralia\nAUS\n2001\n0.481\n0.320\n\n\n3\nAustralia\nAUS\n2003\n0.469\n0.316\n\n\n4\nAustralia\nAUS\n2004\n0.467\n0.316\n\n\n...\n...\n...\n...\n...\n...\n\n\n942\nVietnam\nVNM\n2005\nNaN\n0.369\n\n\n943\nVietnam\nVNM\n2007\nNaN\n0.401\n\n\n944\nVietnam\nVNM\n2009\nNaN\n0.398\n\n\n945\nVietnam\nVNM\n2011\nNaN\n0.364\n\n\n946\nVietnam\nVNM\n2013\nNaN\n0.350\n\n\n\n\n947 rows × 5 columns\n\n\n\n\nincome_inequality_raw\n\n\n\n\n\n\n\n\nEntity\nCode\nYear\ngini_disposable__age_total\ngini_market__age_total\npopulation_historical\nowid_region\n\n\n\n\n0\nAfghanistan\nAFG\n-10000\nNaN\nNaN\n14737.0\nNaN\n\n\n1\nAfghanistan\nAFG\n-9000\nNaN\nNaN\n20405.0\nNaN\n\n\n2\nAfghanistan\nAFG\n-8000\nNaN\nNaN\n28253.0\nNaN\n\n\n3\nAfghanistan\nAFG\n-7000\nNaN\nNaN\n39120.0\nNaN\n\n\n4\nAfghanistan\nAFG\n-6000\nNaN\nNaN\n54166.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n58920\nZimbabwe\nZWE\n2019\nNaN\nNaN\n15271330.0\nNaN\n\n\n58921\nZimbabwe\nZWE\n2020\nNaN\nNaN\n15526837.0\nNaN\n\n\n58922\nZimbabwe\nZWE\n2021\nNaN\nNaN\n15797165.0\nNaN\n\n\n58923\nZimbabwe\nZWE\n2022\nNaN\nNaN\n16069010.0\nNaN\n\n\n58924\nZimbabwe\nZWE\n2023\nNaN\nNaN\n16340778.0\nAfrica\n\n\n\n\n58925 rows × 7 columns\n\n\n\n\nincome_inequality_processed_noNA = income_inequality_processed.dropna(subset=['gini_mi_eq'])\nincome_inequality_processed_noNA['diff'] = income_inequality_processed_noNA['gini_mi_eq'] - income_inequality_processed_noNA['gini_dhi_eq']\ndf1 = income_inequality_processed_noNA.sort_values('Year').groupby('Entity').last()\ndf1 = df1.sort_values('diff', ascending=False)\ndf1\n\n\n\n\n\n\n\n\nCode\nYear\ngini_mi_eq\ngini_dhi_eq\ndiff\n\n\nEntity\n\n\n\n\n\n\n\n\n\nBelgium\nBEL\n2021\n0.486\n0.255\n0.231\n\n\nItaly\nITA\n2020\n0.563\n0.335\n0.228\n\n\nIreland\nIRL\n2021\n0.514\n0.290\n0.224\n\n\nAustria\nAUT\n2022\n0.494\n0.287\n0.207\n\n\nGermany\nDEU\n2020\n0.506\n0.302\n0.204\n\n\nNorway\nNOR\n2004\n0.452\n0.261\n0.191\n\n\nCzechia\nCZE\n2016\n0.444\n0.254\n0.190\n\n\nDenmark\nDNK\n2022\n0.477\n0.288\n0.189\n\n\nGreece\nGRC\n2021\n0.488\n0.308\n0.180\n\n\nUnited Kingdom\nGBR\n2021\n0.479\n0.302\n0.177\n\n\nSweden\nSWE\n2021\n0.458\n0.285\n0.173\n\n\nRomania\nROU\n2021\n0.463\n0.295\n0.168\n\n\nSpain\nESP\n2022\n0.481\n0.314\n0.167\n\n\nLithuania\nLTU\n2021\n0.518\n0.366\n0.152\n\n\nSlovakia\nSVK\n2018\n0.387\n0.236\n0.151\n\n\nLuxembourg\nLUX\n2021\n0.434\n0.288\n0.146\n\n\nCanada\nCAN\n2021\n0.436\n0.291\n0.145\n\n\nBulgaria\nBGR\n2022\n0.517\n0.373\n0.144\n\n\nNetherlands\nNLD\n2021\n0.413\n0.270\n0.143\n\n\nEstonia\nEST\n2016\n0.456\n0.314\n0.142\n\n\nAustralia\nAUS\n2020\n0.459\n0.325\n0.134\n\n\nIsrael\nISR\n2021\n0.472\n0.345\n0.127\n\n\nFinland\nFIN\n2016\n0.382\n0.258\n0.124\n\n\nJapan\nJPN\n2020\n0.423\n0.305\n0.118\n\n\nUnited States\nUSA\n2023\n0.507\n0.392\n0.115\n\n\nBrazil\nBRA\n2015\n0.555\n0.446\n0.109\n\n\nSwitzerland\nCHE\n2019\n0.401\n0.310\n0.091\n\n\nSouth Africa\nZAF\n2017\n0.706\n0.616\n0.090\n\n\nIceland\nISL\n2017\n0.326\n0.251\n0.075\n\n\nDominican Republic\nDOM\n2007\n0.523\n0.515\n0.008\n\n\n\n\n\n\n\n\n\n\ncutoff = 0.460\nset1 = df1[df1['gini_mi_eq']&gt;=cutoff]\nset2 = df1[df1['gini_mi_eq']&lt;cutoff]\n\n\nx = [0,1]\n\ncategories1 = sorted(set(set1['gini_mi_eq']).union(set(set1['gini_dhi_eq'])))\ncategories2 = sorted(set(set2['gini_mi_eq']).union(set(set2['gini_dhi_eq'])))\ncategories = categories1 + categories2\n\nfig, (ax1,ax2) = plt.subplots(1,2,figsize=(6, 8),sharey=True)\nfor ind, (row_ind, row) in enumerate(set1.iterrows()):\n    ax1.plot(x, [row['gini_mi_eq'], row['gini_dhi_eq']], marker='o', label=row_ind, alpha=0.6)\n    if (row_ind in ['Germany', 'Denmark']):\n        ax1.annotate(row_ind, (0.5, row['gini_dhi_eq']))\n    elif (row_ind in ['Austria']):\n        ax1.annotate(row_ind, (1.01, row['gini_dhi_eq']-0.01))\n    else:\n        ax1.annotate(row_ind, (1.01, row['gini_dhi_eq']))\n\nfor ind, (row_ind, row) in enumerate(set2.iterrows()):\n    ax2.plot(x, [row['gini_mi_eq'], row['gini_dhi_eq']], marker='o', label=row_ind, alpha=0.6)\n    if (row_ind in ['Switzerland', 'Luxembourg', 'Finland', 'Iceland']):\n        ax2.annotate(row_ind, (0.4, row['gini_dhi_eq']))\n    else:\n        ax2.annotate(row_ind, (1.01, row['gini_dhi_eq']))\n\nax1.spines[['top', 'right', 'bottom', 'left']].set_visible(False)\nax2.spines[['top', 'right', 'bottom', 'left']].set_visible(False)\n\nax1.set_xticks([])\nax2.set_xticks([])\n\nax1.tick_params(axis='y', pad=15)\nax1.tick_params(axis='y', length=0)\nax2.tick_params(axis='y', length=0)\n\n\n#ax1.set_ylabel(r'Income inequality $\\longrightarrow\\!\\!\\!\\!\\!\\!&gt;$', fontsize=12)\n\ntitle_text = textwrap.fill(\"Gini coefficient for different countries pre- (left) and post- (right) tax deductions. Data for the most recent year are shown. The plot is split into two columns at the dotted line for better visualization.\",30)\nfig.text(x=.55,y=.75,s=title_text, fontsize=12, fontfamily='Serif')\n\nax1.annotate(\n    '', \n    xy=(-0.05, 1), xycoords='axes fraction',\n    xytext=(-0.05, 0), textcoords='axes fraction',\n    arrowprops=dict(arrowstyle='-&gt;', lw=10, mutation_scale=70, color='lightgrey'),\n    zorder=-1\n)\n\nax1.annotate(\n    'Income inequality', \n    xy=(-0.09, 1), xycoords='axes fraction',\n    xytext=(-0.09, 0.10), textcoords='axes fraction',\n    rotation=90\n)\n\nplt.tight_layout()\n\ny_display = ax1.transData.transform((0, cutoff))[1]\ny_fig = fig.transFigure.inverted().transform((0, y_display))[1]\n\n# Add horizontal line across the figure at the data-level cutoff\nline = Line2D([0.125, 0.9], [y_fig, y_fig],  # X in figure coords, Y is now in figure coords too\n              transform=fig.transFigure,\n              color='lightgrey', linewidth=1, linestyle='dotted')\n\nfig.add_artist(line)\n\n#plt.savefig(\"income_inequality.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "posts/Income_inequality/income_inequality.html#plotting",
    "href": "posts/Income_inequality/income_inequality.html#plotting",
    "title": "Income Inequality Before and After Taxes",
    "section": "",
    "text": "cutoff = 0.460\nset1 = df1[df1['gini_mi_eq']&gt;=cutoff]\nset2 = df1[df1['gini_mi_eq']&lt;cutoff]\n\n\nx = [0,1]\n\ncategories1 = sorted(set(set1['gini_mi_eq']).union(set(set1['gini_dhi_eq'])))\ncategories2 = sorted(set(set2['gini_mi_eq']).union(set(set2['gini_dhi_eq'])))\ncategories = categories1 + categories2\n\nfig, (ax1,ax2) = plt.subplots(1,2,figsize=(6, 8),sharey=True)\nfor ind, (row_ind, row) in enumerate(set1.iterrows()):\n    ax1.plot(x, [row['gini_mi_eq'], row['gini_dhi_eq']], marker='o', label=row_ind, alpha=0.6)\n    if (row_ind in ['Germany', 'Denmark']):\n        ax1.annotate(row_ind, (0.5, row['gini_dhi_eq']))\n    elif (row_ind in ['Austria']):\n        ax1.annotate(row_ind, (1.01, row['gini_dhi_eq']-0.01))\n    else:\n        ax1.annotate(row_ind, (1.01, row['gini_dhi_eq']))\n\nfor ind, (row_ind, row) in enumerate(set2.iterrows()):\n    ax2.plot(x, [row['gini_mi_eq'], row['gini_dhi_eq']], marker='o', label=row_ind, alpha=0.6)\n    if (row_ind in ['Switzerland', 'Luxembourg', 'Finland', 'Iceland']):\n        ax2.annotate(row_ind, (0.4, row['gini_dhi_eq']))\n    else:\n        ax2.annotate(row_ind, (1.01, row['gini_dhi_eq']))\n\nax1.spines[['top', 'right', 'bottom', 'left']].set_visible(False)\nax2.spines[['top', 'right', 'bottom', 'left']].set_visible(False)\n\nax1.set_xticks([])\nax2.set_xticks([])\n\nax1.tick_params(axis='y', pad=15)\nax1.tick_params(axis='y', length=0)\nax2.tick_params(axis='y', length=0)\n\n\n#ax1.set_ylabel(r'Income inequality $\\longrightarrow\\!\\!\\!\\!\\!\\!&gt;$', fontsize=12)\n\ntitle_text = textwrap.fill(\"Gini coefficient for different countries pre- (left) and post- (right) tax deductions. Data for the most recent year are shown. The plot is split into two columns at the dotted line for better visualization.\",30)\nfig.text(x=.55,y=.75,s=title_text, fontsize=12, fontfamily='Serif')\n\nax1.annotate(\n    '', \n    xy=(-0.05, 1), xycoords='axes fraction',\n    xytext=(-0.05, 0), textcoords='axes fraction',\n    arrowprops=dict(arrowstyle='-&gt;', lw=10, mutation_scale=70, color='lightgrey'),\n    zorder=-1\n)\n\nax1.annotate(\n    'Income inequality', \n    xy=(-0.09, 1), xycoords='axes fraction',\n    xytext=(-0.09, 0.10), textcoords='axes fraction',\n    rotation=90\n)\n\nplt.tight_layout()\n\ny_display = ax1.transData.transform((0, cutoff))[1]\ny_fig = fig.transFigure.inverted().transform((0, y_display))[1]\n\n# Add horizontal line across the figure at the data-level cutoff\nline = Line2D([0.125, 0.9], [y_fig, y_fig],  # X in figure coords, Y is now in figure coords too\n              transform=fig.transFigure,\n              color='lightgrey', linewidth=1, linestyle='dotted')\n\nfig.add_artist(line)\n\n#plt.savefig(\"income_inequality.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "posts/Gas_prices/gas_prices.html",
    "href": "posts/Gas_prices/gas_prices.html",
    "title": "Gas prices in the US",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\n\nweekly_gas_prices = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-01/weekly_gas_prices.csv')\n\n\nweekly_gas_prices['date'] = pd.to_datetime(weekly_gas_prices['date'])\n\n\nweekly_gas_prices\n\n\n\n\n\n\n\n\ndate\nfuel\ngrade\nformulation\nprice\n\n\n\n\n0\n1990-08-20\ngasoline\nregular\nall\n1.191\n\n\n1\n1990-08-20\ngasoline\nregular\nconventional\n1.191\n\n\n2\n1990-08-27\ngasoline\nregular\nall\n1.245\n\n\n3\n1990-08-27\ngasoline\nregular\nconventional\n1.245\n\n\n4\n1990-09-03\ngasoline\nregular\nall\n1.242\n\n\n...\n...\n...\n...\n...\n...\n\n\n22355\n2025-06-23\ngasoline\npremium\nall\n4.128\n\n\n22356\n2025-06-23\ngasoline\npremium\nconventional\n3.950\n\n\n22357\n2025-06-23\ngasoline\npremium\nreformulated\n4.333\n\n\n22358\n2025-06-23\ndiesel\nall\nNaN\n3.775\n\n\n22359\n2025-06-23\ndiesel\nultra_low_sulfur\nNaN\n3.775\n\n\n\n\n22360 rows × 5 columns\n\n\n\n\nweekly_gas_prices['year'] = weekly_gas_prices['date'].dt.year\n\n\n# create a new col fuel_grade by combining values from fuel and grade columns\nweekly_gas_prices['fuel_grade'] = weekly_gas_prices['fuel'] + '-' + weekly_gas_prices['grade']\nweekly_gas_prices\n\n\n\n\n\n\n\n\ndate\nfuel\ngrade\nformulation\nprice\nyear\nfuel_grade\n\n\n\n\n0\n1990-08-20\ngasoline\nregular\nall\n1.191\n1990\ngasoline-regular\n\n\n1\n1990-08-20\ngasoline\nregular\nconventional\n1.191\n1990\ngasoline-regular\n\n\n2\n1990-08-27\ngasoline\nregular\nall\n1.245\n1990\ngasoline-regular\n\n\n3\n1990-08-27\ngasoline\nregular\nconventional\n1.245\n1990\ngasoline-regular\n\n\n4\n1990-09-03\ngasoline\nregular\nall\n1.242\n1990\ngasoline-regular\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n22355\n2025-06-23\ngasoline\npremium\nall\n4.128\n2025\ngasoline-premium\n\n\n22356\n2025-06-23\ngasoline\npremium\nconventional\n3.950\n2025\ngasoline-premium\n\n\n22357\n2025-06-23\ngasoline\npremium\nreformulated\n4.333\n2025\ngasoline-premium\n\n\n22358\n2025-06-23\ndiesel\nall\nNaN\n3.775\n2025\ndiesel-all\n\n\n22359\n2025-06-23\ndiesel\nultra_low_sulfur\nNaN\n3.775\n2025\ndiesel-ultra_low_sulfur\n\n\n\n\n22360 rows × 7 columns"
  },
  {
    "objectID": "posts/Gas_prices/gas_prices.html#tidytuesday-data-for-2025-07-1",
    "href": "posts/Gas_prices/gas_prices.html#tidytuesday-data-for-2025-07-1",
    "title": "Gas prices in the US",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\n\nweekly_gas_prices = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-01/weekly_gas_prices.csv')\n\n\nweekly_gas_prices['date'] = pd.to_datetime(weekly_gas_prices['date'])\n\n\nweekly_gas_prices\n\n\n\n\n\n\n\n\ndate\nfuel\ngrade\nformulation\nprice\n\n\n\n\n0\n1990-08-20\ngasoline\nregular\nall\n1.191\n\n\n1\n1990-08-20\ngasoline\nregular\nconventional\n1.191\n\n\n2\n1990-08-27\ngasoline\nregular\nall\n1.245\n\n\n3\n1990-08-27\ngasoline\nregular\nconventional\n1.245\n\n\n4\n1990-09-03\ngasoline\nregular\nall\n1.242\n\n\n...\n...\n...\n...\n...\n...\n\n\n22355\n2025-06-23\ngasoline\npremium\nall\n4.128\n\n\n22356\n2025-06-23\ngasoline\npremium\nconventional\n3.950\n\n\n22357\n2025-06-23\ngasoline\npremium\nreformulated\n4.333\n\n\n22358\n2025-06-23\ndiesel\nall\nNaN\n3.775\n\n\n22359\n2025-06-23\ndiesel\nultra_low_sulfur\nNaN\n3.775\n\n\n\n\n22360 rows × 5 columns\n\n\n\n\nweekly_gas_prices['year'] = weekly_gas_prices['date'].dt.year\n\n\n# create a new col fuel_grade by combining values from fuel and grade columns\nweekly_gas_prices['fuel_grade'] = weekly_gas_prices['fuel'] + '-' + weekly_gas_prices['grade']\nweekly_gas_prices\n\n\n\n\n\n\n\n\ndate\nfuel\ngrade\nformulation\nprice\nyear\nfuel_grade\n\n\n\n\n0\n1990-08-20\ngasoline\nregular\nall\n1.191\n1990\ngasoline-regular\n\n\n1\n1990-08-20\ngasoline\nregular\nconventional\n1.191\n1990\ngasoline-regular\n\n\n2\n1990-08-27\ngasoline\nregular\nall\n1.245\n1990\ngasoline-regular\n\n\n3\n1990-08-27\ngasoline\nregular\nconventional\n1.245\n1990\ngasoline-regular\n\n\n4\n1990-09-03\ngasoline\nregular\nall\n1.242\n1990\ngasoline-regular\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n22355\n2025-06-23\ngasoline\npremium\nall\n4.128\n2025\ngasoline-premium\n\n\n22356\n2025-06-23\ngasoline\npremium\nconventional\n3.950\n2025\ngasoline-premium\n\n\n22357\n2025-06-23\ngasoline\npremium\nreformulated\n4.333\n2025\ngasoline-premium\n\n\n22358\n2025-06-23\ndiesel\nall\nNaN\n3.775\n2025\ndiesel-all\n\n\n22359\n2025-06-23\ndiesel\nultra_low_sulfur\nNaN\n3.775\n2025\ndiesel-ultra_low_sulfur\n\n\n\n\n22360 rows × 7 columns"
  },
  {
    "objectID": "posts/Gas_prices/gas_prices.html#plotting",
    "href": "posts/Gas_prices/gas_prices.html#plotting",
    "title": "Gas prices in the US",
    "section": "Plotting",
    "text": "Plotting\n\nfig, ax = plt.subplots(figsize=(8, 4))\n\nintervals = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]  \ncolors = ['#D6ECF3', '#87CEEB', '#1E90FF', '#0000CD', 'red']\n\ncmap = mcolors.ListedColormap(colors)\nnorm = mcolors.BoundaryNorm(intervals, cmap.N)\n\ndf_max_price = weekly_gas_prices.groupby(['year', 'fuel_grade'])['price'].max().reset_index()\n\nfuel_cat = ['gasoline-all', 'gasoline-premium', 'gasoline-midgrade', 'gasoline-regular', 'diesel-all', 'diesel-ultra_low_sulfur', 'diesel-low_sulfur']\ndf_max_price['fuel_grade'] = pd.Categorical(df_max_price['fuel_grade'], categories=fuel_cat, ordered=True)\n# sort by year and fuel_grade\ndf_max_price = df_max_price.sort_values(by=['fuel_grade'])\n\n# remove rows where fuel_grade contains \"all\"\ndf_max_price = df_max_price[~df_max_price['fuel_grade'].str.contains('all')]\n\ns1 = ax.scatter(\n    df_max_price['year'], \n    df_max_price['fuel_grade'], \n    c=df_max_price['price'], \n    cmap=cmap, \n    norm=norm,\n    marker='s'\n)\n\nax.invert_yaxis()\n\ncbar_ax = inset_axes(ax,\n                     width=\"50%\",  # Width as a percent of the parent axis\n                     height=\"5%\",  # Height as a percent of the parent axis\n                     loc='lower center',  # Position inside the main plot\n                     bbox_to_anchor=(-0.025, 0.15, 0.5, 0.5),\n                     bbox_transform=ax.transAxes)\n\ncbar = plt.colorbar(s1, cax=cbar_ax, shrink=0.5, orientation='horizontal')\ncbar.set_ticklabels([f'${interval:.0f}' for interval in intervals])\ncbar.outline.set_visible(False)\ncbar.set_label('Price per gallon', fontsize=8)\ncbar.ax.xaxis.set_label_position('top')   \ncbar.ax.tick_params(length=0, labelsize=8)  \n\nfor label in ax.get_yticklabels():\n    if \"diesel\" in label.get_text():\n        label.set_color('#666666')\n\nax.set_yticks(ax.get_yticks())\nax.set_yticklabels([k.get_text().split('-')[1] for k in ax.get_yticklabels()])\n\nax.text(0.01, 0.925, '\\n'.join(\"GASOLINE\"), transform=ax.transAxes, fontsize=10,\n        verticalalignment='top', fontfamily='Consolas')\nax.text(0.01, 0.30, '\\n'.join(\"DIESEL\"), transform=ax.transAxes, fontsize=10,\n        verticalalignment='top', color='#666666', fontfamily='Consolas')\nax.spines[['top', 'right']].set_visible(False)\n\n\n\n# Label max and min for each fuel_grade\nfor grade in df_max_price['fuel_grade'].unique():\n    subset = df_max_price[df_max_price['fuel_grade'] == grade]\n\n    # Skip if empty or NaNs\n    if subset.empty or subset['price'].isnull().all():\n        continue\n\n    max_row = subset.loc[subset['price'].idxmax()]\n    min_row = subset.loc[subset['price'].idxmin()]\n\n    arrow = dict(arrowstyle='&lt;-', color='black', linewidth=0.8)\n\n    # Annotate max\n    ax.annotate(\n        f\"${max_row['price']:.2f}\",\n        xy=(max_row['year'], max_row['fuel_grade']),\n        xytext=(0, 13),  \n        textcoords='offset points',\n        ha='center', va='bottom',\n        fontsize=8, color='red',\n        arrowprops=arrow\n    )\n    # Annotate min\n    ax.annotate(\n        f\"${min_row['price']:.2f}\",\n        xy=(min_row['year'], min_row['fuel_grade']),\n        xytext=(0, -20),  \n        textcoords='offset points',\n        ha='center', va='bottom',\n        fontsize=8, color='blue',\n        arrowprops=arrow\n    )\nfig.suptitle('Year-wise maximum fuel prices in the US across five different categories. \\n For each series, the highest and lowest prices are labeled.',x=0, ha='left', fontfamily='Serif')\nplt.savefig('gas_prices.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "posts/Flint_water/Flint_water.html",
    "href": "posts/Flint_water/Flint_water.html",
    "title": "Lead concentration in Flint water samples in 2015",
    "section": "",
    "text": "TidyTuesday dataset of November 4, 2025\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FuncFormatter\nimport seaborn as sns\nfrom scipy.stats import skew, kurtosis\nimport textwrap\n\n\nflint_mdeq = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-04/flint_mdeq.csv')\nflint_vt = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-04/flint_vt.csv')\n\n\nflint_mdeq\n\n\n\n\n\n\n\n\nsample\nlead\nlead2\nnotes\n\n\n\n\n0\n1\n0\n0.0\nNaN\n\n\n1\n2\n104\nNaN\nsample removed: house had a filter\n\n\n2\n3\n10\n10.0\nNaN\n\n\n3\n4\n6\n6.0\nNaN\n\n\n4\n5\n5\n5.0\nNaN\n\n\n...\n...\n...\n...\n...\n\n\n66\n67\n2\n2.0\nNaN\n\n\n67\n68\n0\n0.0\nNaN\n\n\n68\n69\n3\n3.0\nNaN\n\n\n69\n70\n5\n5.0\nNaN\n\n\n70\n71\n2\n2.0\nNaN\n\n\n\n\n71 rows × 4 columns\n\n\n\n\nsns.scatterplot(data=flint_mdeq, x='sample', y='lead')\n\n\n\n\n\n\n\n\n\nplt.scatter(flint_mdeq['sample'],flint_mdeq['lead2'])\n#plt.scatter(flint_vt['sample'],flint_vt['lead'])\n\n\n\n\n\n\n\n\n\nflint_mdeq.describe()\n\n\n\n\n\n\n\n\nsample\nlead\nlead2\n\n\n\n\ncount\n71.000000\n71.000000\n69.000000\n\n\nmean\n36.000000\n7.309859\n5.724638\n\n\nstd\n20.639767\n14.347316\n8.336461\n\n\nmin\n1.000000\n0.000000\n0.000000\n\n\n25%\n18.500000\n2.000000\n2.000000\n\n\n50%\n36.000000\n3.000000\n3.000000\n\n\n75%\n53.500000\n6.500000\n6.000000\n\n\nmax\n71.000000\n104.000000\n42.000000\n\n\n\n\n\n\n\n\ndesc = flint_mdeq[['lead','lead2']].describe().T[['mean','50%', 'min','max']]\ndesc\n\n\n\n\n\n\n\n\nmean\n50%\nmin\nmax\n\n\n\n\nlead\n7.309859\n3.0\n0.0\n104.0\n\n\nlead2\n5.724638\n3.0\n0.0\n42.0\n\n\n\n\n\n\n\n\ndesc.plot(kind='bar', figsize=(10,6))\n\n\n\n\n\n\n\n\n\nsns.swarmplot(flint_mdeq[['lead','lead2']])\n\n\n\n\n\n\n\n\n\nflint_mdeq\n\n\n\n\n\n\n\n\nsample\nlead\nlead2\nnotes\n\n\n\n\n0\n1\n0\n0.0\nNaN\n\n\n1\n2\n104\nNaN\nsample removed: house had a filter\n\n\n2\n3\n10\n10.0\nNaN\n\n\n3\n4\n6\n6.0\nNaN\n\n\n4\n5\n5\n5.0\nNaN\n\n\n...\n...\n...\n...\n...\n\n\n66\n67\n2\n2.0\nNaN\n\n\n67\n68\n0\n0.0\nNaN\n\n\n68\n69\n3\n3.0\nNaN\n\n\n69\n70\n5\n5.0\nNaN\n\n\n70\n71\n2\n2.0\nNaN\n\n\n\n\n71 rows × 4 columns\n\n\n\n\nflint_mdeq['diff'] = flint_mdeq.apply(\n    lambda row: row['lead'] if pd.isna(row['lead2']) else None,\n    axis=1\n)\n\n\nd = flint_mdeq['lead2'].dropna()\nprint(\"Skewness:\", skew(d))\nprint(\"Kurtosis:\", kurtosis(d))\n\nSkewness: 2.9376969574953016\nKurtosis: 9.133276189367894\n\n\n\nmelted = flint_mdeq[['lead', 'lead2','diff']].melt(var_name='Source', value_name='Lead Level')\nfig,ax=plt.subplots(figsize=(6, 6))\nax2 = ax.twinx()\nsns.swarmplot(x='Source', y='Lead Level', data=melted[melted['Source'].isin(['lead', 'lead2'])], size=6, hue='Source')\ntemp = melted[melted['Source']=='diff']\ntemp['Source'] = temp['Source'].str.replace('diff','lead')\nplt.scatter(temp['Source'], temp['Lead Level'],marker='*', color='red',s=100,zorder=10, alpha=0.5)\ndesc.plot(ax=ax, alpha=0, legend=False)\nax.set_yticks(desc.loc['lead'])\nax2.set_yticks(desc.loc['lead2'])\n#ax.grid(True, axis='y', color='#1f77b4')\nyticks = ax.get_yticks()\n\n# Get x-axis limits\nxlim = ax.get_xlim()\nx_half = (xlim[0] + xlim[1]) / 2  # midpoint of x-axis\n\n# Draw custom horizontal lines from left to midpoint\nfor ytick in yticks:\n    ax.hlines(y=ytick, xmin=xlim[0], xmax=x_half-0.1, color='#1f77b4', linestyle='--', linewidth=0.5)\n\n#ax2.grid(True, axis='y', color='#ff7f0e')\nyticks = ax2.get_yticks()\n\n# Get x-axis limits\nxlim = ax2.get_xlim()\nx_half = (xlim[0] + xlim[1]) / 2  # midpoint of x-axis\n\n# Draw custom horizontal lines from left to midpoint\nfor ytick in yticks:\n    ax2.hlines(y=ytick, xmin=xlim[1], xmax=x_half+0.1, color='#ff7f0e', linestyle='--', linewidth=0.5)\n\nax.tick_params(axis='both', which='both', length=0)\nax2.tick_params(axis='both', which='both', length=0)\nformatter = FuncFormatter(lambda val, pos: f'{val:.1f}')\nax2.yaxis.set_major_formatter(formatter)\nax.text(0.5, -1, 'Min',ha='center')\nax.text(0.5, 2.5, 'Median',ha='center')\nax.text(0.35, 7.5, 'Mean',ha='center', color='#1f77b4')\nax.text(0.6, 6, 'Mean',ha='center', color='#ff7f0e')\nax.text(0.35, 104, 'Max',ha='center', color='#1f77b4')\nax.text(0.6, 42, 'Max',ha='center', color='#ff7f0e')\n\nax.text(0,80,f'Skewness: {round(skew(flint_mdeq[\"lead\"]),2)}', ha='center', color='#1f77b4')\nax.text(0,75,f'Kurtosis: {round(kurtosis(flint_mdeq[\"lead\"]),2)}', ha='center', color='#1f77b4')\n\nax.text(1,80,f'Skewness: {round(skew(flint_mdeq[\"lead2\"].dropna()),2)}', ha='center', color='#ff7f0e')\nax.text(1,75,f'Kurtosis: {round(kurtosis(flint_mdeq[\"lead2\"].dropna()),2)}', ha='center', color='#ff7f0e')\n\nplt.ylabel('')\nsns.despine(left=True,bottom=True)\ntitle=\"Distribution of Lead concentration in Flint water samples. The two leptokurtic distributions differ by two data points (red asterisks).\"\nplt.title('\\n'.join(textwrap.wrap(title,50)), fontfamily='Serif', fontsize=14)\nax.set_facecolor('#EEEEEE')  \nfig.set_facecolor('#EEEEEE')\nplt.tight_layout()\nplt.savefig('Flint_water.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.histplot(flint_mdeq['lead2'], kde=True)\nplt.show()"
  },
  {
    "objectID": "posts/Euro_basketball/Euroleague_basketball.html",
    "href": "posts/Euro_basketball/Euroleague_basketball.html",
    "title": "EuroLeague Basketball",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\nimport flagpy as fp\n\n\neuroleague_basketball = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-07/euroleague_basketball.csv')\n\n\neuroleague_basketball\n\n\n\n\n\n\n\n\nTeam\nHome city\nArena\nCapacity\nLast season\nCountry\nFinalFour_Appearances\nTitles_Won\nYears_of_FinalFour_Appearances\nYears_of_Titles_Won\n\n\n\n\n0\nAnadolu Efes\nIstanbul\nBasketball Development Center\n10,000\n6th\nTurkey\n5\n2\n2000, 2001, 2019, 2021, 2022\n2021, 2022\n\n\n1\nBarcelona\nBarcelona\nPalau Blaugrana\n7,585\n5th\nSpain\n0\n0\nNaN\nNaN\n\n\n2\nBaskonia\nVitoria-Gasteiz\nBuesa Arena\n15,431\n14th\nSpain\n0\n0\nNaN\nNaN\n\n\n3\nBayern Munich\nMunich\nSAP Garden\n11,500\n9th\nGermany\n0\n0\nNaN\nNaN\n\n\n4\nCrvena zvezda Meridianbet\nBelgrade\nBelgrade Arena\n18,386\n10th\nSerbia\n0\n0\nNaN\nNaN\n\n\n5\nDubai Basketball\nDubai\nCoca-Cola Arena\n17,000\nNaN\nUnited Arab Emirates\n0\n0\nNaN\nNaN\n\n\n6\nFenerbahce\nIstanbul\nÜlker Sports and Event Hall\n13,000\n1st\nTurkey\n7\n2\n2015, 2016, 2017, 2018, 2019, 2024, 2025\n2017, 2025\n\n\n7\nHapoel IBI Tel Aviv\nTel Aviv\nArena 8888 Sofia \\ Arena Botevgrad \\ Menora ...\n12,373\n(EuroCup)\nIsrael\n0\n0\nNaN\nNaN\n\n\n8\nLDLC ASVEL\nVilleurbanne\nLDLC Arena, Astroballe\n12,523, 5,556\n15th\nFrance\n0\n0\nNaN\nNaN\n\n\n9\nMaccabi Rapyd Tel Aviv\nTel Aviv\nMenora Mivtachim Arena\n10,383\n16th\nIsrael\n0\n0\nNaN\nNaN\n\n\n10\nMonaco\nMonaco\nSalle Gaston Médecin\n5,000\n2nd\nMonaco\n2\n0\n2023, 2025\nNaN\n\n\n11\nOlimpia Milano\nMilan\nUnipol Forum\n12,700\n11th\nItaly\n4\n0\n1992, 2021\nNaN\n\n\n12\nOlympiacos\nPiraeus\nPeace and Friendship Stadium\n12,300\n3rd\nGreece\n14\n3\n1994, 1995, 1997, 1999, 2009, 2010, 2012, 2013...\n1997, 2012, 2013\n\n\n13\nPanathinaikos\nAthens\nTelekom Center Athens\n18,300\n4th\nGreece\n13\n7\n1994, 1995, 1996, 2000, 2001, 2002, 2005, 2007...\n1996, 2000, 2002, 2007, 2009, 2011, 2024\n\n\n14\nParis Basketball\nParis\nAdidas Arena, Accor Arena\n8,000, 15,705\n8th\nFrance\n0\n0\nNaN\nNaN\n\n\n15\nPartizan\nBelgrade\nBelgrade Arena\n18,386\n12th\nSerbia\n3\n1\n1988, 1992, 1998, 2010\n1992\n\n\n16\nReal Madrid\nMadrid\nMovistar Arena\n15,000\n7th\nSpain\n12\n6\n1995, 1996, 2011, 2013, 2014, 2015, 2017, 2018...\n1995, 2015, 2018, 2023\n\n\n17\nValencia Basket\nValencia\nRoig Arena\n15,600\n(EuroCup)\nSpain\n0\n0\nNaN\nNaN\n\n\n18\nVirtus Olidata Bologna\nBologna\nVirtus Arena, PalaDozza\n9,980, 5,570\n17th\nItaly\n0\n0\nNaN\nNaN\n\n\n19\nZalgiris\nKaunas\nŽalgirio Arena\n15,415\n13th\nLithuania\n2\n1\n1999, 2018\n1999\n\n\n\n\n\n\n\n\ncountries = euroleague_basketball['Country'].unique()\ncountries\n\narray(['Turkey', 'Spain', 'Germany', 'Serbia', 'United Arab Emirates',\n       'Israel', 'France', 'Monaco', 'Italy', 'Greece', 'Lithuania'],\n      dtype=object)\n\n\n\n# save flag for each country\nfor country in countries:\n    if country=='United Arab Emirates':\n        continue\n    img = fp.get_flag_img(country)\n    img.save(f'{country}_flag.png')\n\n\ndf_grp_team = (\n            euroleague_basketball.groupby(['Country','Team'])\n            .agg({'Titles_Won': 'sum',\n                  'Years_of_Titles_Won': lambda x: ', '.join(x.dropna()),\n                  })\n            .sort_values(by='Titles_Won', ascending=False)\n            .reset_index()\n)\ndf_grp_team = df_grp_team[df_grp_team['Titles_Won'] &gt; 0]\ndf_grp_team\n\n\n\n\n\n\n\n\nCountry\nTeam\nTitles_Won\nYears_of_Titles_Won\n\n\n\n\n0\nGreece\nPanathinaikos\n7\n1996, 2000, 2002, 2007, 2009, 2011, 2024\n\n\n1\nSpain\nReal Madrid\n6\n1995, 2015, 2018, 2023\n\n\n2\nGreece\nOlympiacos\n3\n1997, 2012, 2013\n\n\n3\nTurkey\nFenerbahce\n2\n2017, 2025\n\n\n4\nTurkey\nAnadolu Efes\n2\n2021, 2022\n\n\n5\nSerbia\nPartizan\n1\n1992\n\n\n6\nLithuania\nZalgiris\n1\n1999\n\n\n\n\n\n\n\n\n\n\nplt.rcParams['font.family'] = 'Segoe UI Emoji'  \n\n\nitems = df_grp_team['Years_of_Titles_Won'].to_list()\ndef comma_newline(s):\n    parts = [p.strip() for p in s.split(',')]\n    separators = [',' if i % 2 == 0 else '\\n' for i in range(len(parts) - 1)]\n    return ''.join(p + sep for p, sep in zip(parts, separators)) + parts[-1]\n#print(items)\nyear_won = [comma_newline(s) for s in items]\n#print(year_won)\n\nfig, ax = plt.subplots(figsize=(14, 3))#, subplot_kw=dict(polar=True))\nplt.scatter(df_grp_team['Team'],[1]*len(df_grp_team['Titles_Won']), s=df_grp_team['Titles_Won']*1500, \\\n            color='orange', alpha=0)\nfor ind, (x, y, z) in enumerate(zip(df_grp_team['Team'],[1]*len(df_grp_team['Titles_Won']), year_won)):\n    plt.text(x,y,z, fontsize=9, ha='center', va='top', color='black', family='monospace')\n    plt.text(x, y,'\\U0001F3C0', fontsize=(df_grp_team['Titles_Won'][ind]*10)+30, ha='center', va='center',\\\n            color='orange',zorder=1, alpha=0.8)\n    plt.text(x, y-0.18,x, fontsize=12, ha='center', va='center', color='#333333')\n    img = mpimg.imread(f'{df_grp_team[\"Country\"][ind]}_flag.png')\n    imagebox = OffsetImage(img, zoom=0.3)  \n#    ab = AnnotationBbox(imagebox, (x, y-0.26), frameon=True, bboxprops=dict(edgecolor='lightgray'), zorder=2)  # Higher zorder\n    ab = AnnotationBbox(imagebox, (x, y-0.26), frameon=False, zorder=2)  # Higher zorder\n    ax.add_artist(ab)\nplt.axis('off')\nplt.xlim(-0.75,6.5)\nplt.ylim(0.70,1.2)\nplt.title(\"EuroLeague Basketball Champions\", fontsize=20, family='Serif', color='#333333')\nfig.patch.set_facecolor('#FFFDD0')\nplt.savefig(\"euro_bb.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nIn the data, Real Madrid has six wins but only four years are given."
  },
  {
    "objectID": "posts/Euro_basketball/Euroleague_basketball.html#plotting",
    "href": "posts/Euro_basketball/Euroleague_basketball.html#plotting",
    "title": "EuroLeague Basketball",
    "section": "",
    "text": "plt.rcParams['font.family'] = 'Segoe UI Emoji'  \n\n\nitems = df_grp_team['Years_of_Titles_Won'].to_list()\ndef comma_newline(s):\n    parts = [p.strip() for p in s.split(',')]\n    separators = [',' if i % 2 == 0 else '\\n' for i in range(len(parts) - 1)]\n    return ''.join(p + sep for p, sep in zip(parts, separators)) + parts[-1]\n#print(items)\nyear_won = [comma_newline(s) for s in items]\n#print(year_won)\n\nfig, ax = plt.subplots(figsize=(14, 3))#, subplot_kw=dict(polar=True))\nplt.scatter(df_grp_team['Team'],[1]*len(df_grp_team['Titles_Won']), s=df_grp_team['Titles_Won']*1500, \\\n            color='orange', alpha=0)\nfor ind, (x, y, z) in enumerate(zip(df_grp_team['Team'],[1]*len(df_grp_team['Titles_Won']), year_won)):\n    plt.text(x,y,z, fontsize=9, ha='center', va='top', color='black', family='monospace')\n    plt.text(x, y,'\\U0001F3C0', fontsize=(df_grp_team['Titles_Won'][ind]*10)+30, ha='center', va='center',\\\n            color='orange',zorder=1, alpha=0.8)\n    plt.text(x, y-0.18,x, fontsize=12, ha='center', va='center', color='#333333')\n    img = mpimg.imread(f'{df_grp_team[\"Country\"][ind]}_flag.png')\n    imagebox = OffsetImage(img, zoom=0.3)  \n#    ab = AnnotationBbox(imagebox, (x, y-0.26), frameon=True, bboxprops=dict(edgecolor='lightgray'), zorder=2)  # Higher zorder\n    ab = AnnotationBbox(imagebox, (x, y-0.26), frameon=False, zorder=2)  # Higher zorder\n    ax.add_artist(ab)\nplt.axis('off')\nplt.xlim(-0.75,6.5)\nplt.ylim(0.70,1.2)\nplt.title(\"EuroLeague Basketball Champions\", fontsize=20, family='Serif', color='#333333')\nfig.patch.set_facecolor('#FFFDD0')\nplt.savefig(\"euro_bb.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nIn the data, Real Madrid has six wins but only four years are given."
  },
  {
    "objectID": "posts/Cranes_Sweden/cranes_sweden.html",
    "href": "posts/Cranes_Sweden/cranes_sweden.html",
    "title": "Crane Observations at Lake Hornborgasjön",
    "section": "",
    "text": "TidyTuesday dataset of September 30, 2025\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\nimport seaborn as sns\n\n\ncranes = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-30/cranes.csv')\n\n\ncranes\n\n\n\n\n\n\n\n\ndate\nobservations\ncomment\nweather_disruption\n\n\n\n\n0\n2024-10-03\n408.0\nLast count of season\nFalse\n\n\n1\n2024-09-30\n14940.0\nNaN\nFalse\n\n\n2\n2024-09-26\nNaN\nCanceled/No count\nTrue\n\n\n3\n2024-09-23\n18450.0\nNaN\nFalse\n\n\n4\n2024-09-19\n14900.0\nNaN\nFalse\n\n\n...\n...\n...\n...\n...\n\n\n1543\n1994-03-28\nNaN\nCanceled/No count\nFalse\n\n\n1544\n1994-03-27\nNaN\nCanceled/No count\nFalse\n\n\n1545\n1994-03-26\nNaN\nCanceled/No count\nFalse\n\n\n1546\n1994-03-25\n200.0\nNaN\nFalse\n\n\n1547\n1994-03-24\n200.0\nNaN\nFalse\n\n\n\n\n1548 rows × 4 columns\n\n\n\n\ncranes['date'] = pd.to_datetime(cranes['date'])\n# add month and year columns. Month should be three letter\n\ncranes['year'] = cranes['date'].dt.year\ncranes['month'] = cranes['date'].dt.month\n\ncranes['month'] = cranes['date'].dt.strftime('%b')\ncranes['year_half'] = cranes['month'].apply(lambda x: 'first_half' if x in ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun'] else 'second_half')\n\n\ncranes\n\n\n\n\n\n\n\n\ndate\nobservations\ncomment\nweather_disruption\nyear\nmonth\nyear_half\n\n\n\n\n0\n2024-10-03\n408.0\nLast count of season\nFalse\n2024\nOct\nsecond_half\n\n\n1\n2024-09-30\n14940.0\nNaN\nFalse\n2024\nSep\nsecond_half\n\n\n2\n2024-09-26\nNaN\nCanceled/No count\nTrue\n2024\nSep\nsecond_half\n\n\n3\n2024-09-23\n18450.0\nNaN\nFalse\n2024\nSep\nsecond_half\n\n\n4\n2024-09-19\n14900.0\nNaN\nFalse\n2024\nSep\nsecond_half\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1543\n1994-03-28\nNaN\nCanceled/No count\nFalse\n1994\nMar\nfirst_half\n\n\n1544\n1994-03-27\nNaN\nCanceled/No count\nFalse\n1994\nMar\nfirst_half\n\n\n1545\n1994-03-26\nNaN\nCanceled/No count\nFalse\n1994\nMar\nfirst_half\n\n\n1546\n1994-03-25\n200.0\nNaN\nFalse\n1994\nMar\nfirst_half\n\n\n1547\n1994-03-24\n200.0\nNaN\nFalse\n1994\nMar\nfirst_half\n\n\n\n\n1548 rows × 7 columns\n\n\n\n\ndf_obs = cranes.groupby(['year','year_half', 'month'])['observations'].sum().reset_index()\n\n\nmonth_mapping = {'Mar':'Mars', 'Apr':'April', 'Aug':'Augusti', 'Sep':'September', 'Oct':'Oktober'}\nbg_color = '#135E4B'\nfg_color = '#CCDCDB'\ncustom_map = ['#FCC5F8','#B3F7FE','#E7F8BE']\nsns.set_theme(font_scale=1.75)\nsns.set_style('white')\ng = sns.catplot(data=df_obs, x='year', y='observations', col='year_half', kind='point', hue='month', \\\n            aspect=2, alpha=1, legend=True, linestyles=':', height=5, palette=custom_map)\ng.set_titles(template=\"\")  \n\nlegend_labels = list(g._legend_data.keys())\n\n# remove legend\ng._legend.remove()\n# set x-tick at interval of 10\nfor ax in g.axes.flat:\n    ax.set_xticks(range(0,32,5))\n    ax.set_xlabel(\"\")\n    ax.set_ylabel(\"\")\n    ax.yaxis.grid(True,linestyle=':', linewidth=0.7,)\n#    ax.xaxis.grid(True,linestyle=':', linewidth=0.3,)\n    ax.spines[['left','bottom']].set_visible(False)\n    yticks = [f'{int(x/1000)}K' if x &gt; 0 else 0 for x in ax.get_yticks()]\n    ax.set_yticklabels(yticks, fontfamily='monospace')\n    ax.set_xticklabels(ax.get_xticklabels(), fontfamily='monospace')\n    ax.set_facecolor(bg_color)\n    ax.tick_params(colors=fg_color)\n\n\n# Create separate legends for each subplot\nfor ind, ax in enumerate(g.axes.flat):\n    # Get all lines in the subplot\n    lines = ax.lines\n    \n    # Filter lines that have actual data (non-NaN y-values)\n    valid_lines = []\n    for line in lines:\n        # Skip error bars (unlabeled lines)\n        if line.get_label() == '':\n            continue\n            \n        # Check if line has non-NaN data\n        ydata = line.get_ydata()\n        if not np.all(np.isnan(ydata)):\n            valid_lines.append(line)\n    \n    # Extract labels from valid lines\n    labels = [line.get_label() for line in valid_lines]\n    \n    # Sort labels numerically (if possible) or alphabetically\n    try:\n        # Try to convert labels to integers for numerical sorting\n        sorted_indices = sorted(range(len(labels)), key=lambda i: int(labels[i]))\n    except ValueError:\n        # Fall back to alphabetical sorting if labels aren't numbers\n        sorted_indices = sorted(range(len(labels)), key=lambda i: labels[i])\n    \n    # Reorder lines and labels\n    sorted_lines = [valid_lines[i] for i in sorted_indices]\n    sorted_labels = [labels[i] for i in sorted_indices]\n#    print(sorted_labels)\n    dummy_handles = [Line2D([], [], marker='none', linestyle='none') \n                         for _ in sorted_lines]\n        \n    # Add legend to subplot\n    if ind==0:  \n        legend = ax.legend(\n            handles=dummy_handles, \n            labels=[month_mapping[label] for label in legend_labels[:2]], \n            title='month', \n            loc='upper left',\n            frameon=False,\n            ncol=3,\n            bbox_to_anchor=(0.05, 0.9)\n        )\n    if ind&gt;0:  \n        legend = ax.legend(\n            handles=dummy_handles, \n            labels=[month_mapping[label] for label in legend_labels[2:]], \n            title='month', \n            loc='upper left',\n            frameon=False,\n            ncol=3,\n            bbox_to_anchor=(0.05, 0.9)\n        )\n    for text, line in zip(legend.get_texts(), sorted_lines):\n        text.set_color(line.get_color())\n\n     \n        # Remove markers from legend handles\n    legend.set_title('')\n\n\n\ng.axes[0][0].tick_params(labelleft=False)\ng.axes[0][1].tick_params(labelleft=True)\ng.axes[0][1].tick_params(axis='y', labelleft=True, pad=20) \ng.axes[0][1].set_yticklabels(yticks, ha='left')\ng.fig.subplots_adjust(wspace=0.05)\ng.fig.suptitle('Crane Observations at Lake Hornborgasjön in Sweden', fontsize=24, family='Serif', \\\n                weight='bold', color=fg_color)\ng.fig.patch.set_facecolor(bg_color)\nplt.savefig(\"cranes_observations.png\", dpi=300, bbox_inches='tight', pad_inches=0.1)\nplt.show()"
  },
  {
    "objectID": "posts/Boogg_burn/Boogg_burn.html",
    "href": "posts/Boogg_burn/Boogg_burn.html",
    "title": "Zurich’s “Sechselaeuten” spring festival",
    "section": "",
    "text": "TidyTuesday dataset of December 2, 2025\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport textwrap\n\n\nsechselaeuten = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-02/sechselaeuten.csv')\nsechselaeuten\n\n\n\n\n\n\n\n\nyear\nduration\ntre200m0\ntre200mn\ntre200mx\nsre000m0\nsremaxmv\nrre150m0\nrecord\n\n\n\n\n0\n1923\n60.00\n16.67\n7.03\n32.47\n247.43\n56.33\n73.97\nFalse\n\n\n1\n1952\n6.00\n18.73\n9.10\n33.70\n269.70\n61.67\n93.67\nFalse\n\n\n2\n1953\n8.00\n16.67\n7.17\n29.97\n209.53\n48.67\n157.90\nFalse\n\n\n3\n1956\n4.00\n15.07\n6.80\n28.77\n172.83\n39.67\n174.90\nFalse\n\n\n4\n1958\n8.00\n17.00\n8.23\n30.67\n230.30\n53.00\n177.87\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n62\n2021\n12.95\n17.97\n9.83\n30.13\n190.13\n44.67\n169.37\nFalse\n\n\n63\n2022\n37.98\n20.40\n11.40\n34.40\n284.66\n66.67\n82.17\nTrue\n\n\n64\n2023\n57.00\n20.07\n10.57\n33.23\n253.37\n59.33\n110.70\nTrue\n\n\n65\n2024\nNaN\n19.50\n10.33\n31.40\n214.59\n50.67\n98.93\nTrue\n\n\n66\n2025\n26.50\n19.60\n10.50\n32.80\n250.81\n58.67\n126.80\nTrue\n\n\n\n\n67 rows × 9 columns\n\n\n\n\nsechselaeuten.dropna(inplace=True)\nsechselaeuten.columns\n\nIndex(['year', 'duration', 'tre200m0', 'tre200mn', 'tre200mx', 'sre000m0',\n       'sremaxmv', 'rre150m0', 'record'],\n      dtype='object')\n\n\n\nduration_max = sechselaeuten.loc[sechselaeuten['duration'].idxmax()]\nduration_min = sechselaeuten.loc[sechselaeuten['duration'].idxmin()]\nduration_max['year'], duration_min['year'], duration_max['duration'], duration_min['duration']\n\n(np.int64(1923), np.int64(1956), np.float64(60.0), np.float64(4.0))\n\n\n\nsechselaeuten[sechselaeuten['record']==True]\n\n\n\n\n\n\n\n\nyear\nduration\ntre200m0\ntre200mn\ntre200mx\nsre000m0\nsremaxmv\nrre150m0\nrecord\n\n\n\n\n25\n1983\n24.33\n19.07\n9.10\n31.37\n207.14\n48.67\n52.83\nTrue\n\n\n36\n1994\n21.92\n19.13\n9.57\n31.70\n217.18\n51.33\n85.03\nTrue\n\n\n45\n2003\n5.70\n21.67\n11.30\n34.57\n281.84\n67.00\n83.50\nTrue\n\n\n56\n2015\n20.65\n20.17\n9.63\n33.17\n259.30\n61.00\n80.60\nTrue\n\n\n58\n2017\n9.93\n19.53\n10.13\n32.37\n226.42\n53.33\n119.87\nTrue\n\n\n59\n2018\n20.52\n20.17\n9.40\n32.63\n267.33\n62.67\n80.33\nTrue\n\n\n60\n2019\n17.73\n19.73\n9.83\n33.50\n251.68\n59.00\n109.37\nTrue\n\n\n63\n2022\n37.98\n20.40\n11.40\n34.40\n284.66\n66.67\n82.17\nTrue\n\n\n64\n2023\n57.00\n20.07\n10.57\n33.23\n253.37\n59.33\n110.70\nTrue\n\n\n66\n2025\n26.50\n19.60\n10.50\n32.80\n250.81\n58.67\n126.80\nTrue\n\n\n\n\n\n\n\n\nbg_color = \"#550000\"\ntext_color = \"#ED8E8E\"\nplot_color1 = '#BA55D3'\nsns.set_theme(style=\"white\", rc={\n    \"font.family\": \"monospace\", \n    \"text.color\": text_color,   \n    \"axes.labelcolor\": text_color, \n    \"xtick.color\": text_color,     \n    \"ytick.color\": text_color      \n})\ncustom_palette = {\n    True: \"#FC0000\",\n    False: plot_color1\n}\nfig, ax = plt.subplots(figsize=(8,5))\nsns.scatterplot(data=sechselaeuten, x='year', y='tre200mx', hue='record', size='duration',\\\n                sizes=(20, 200), alpha=0.8, edgecolor='none', palette=custom_palette, legend=False)\nsns.despine(bottom=True, left=True)\nfig.patch.set_facecolor(bg_color)\nax.set_facecolor(bg_color)\nplt.xlabel(\"\")\nplt.ylabel(\"Maximum Temperature (°C)\")\nplt.grid(True, axis='y', which='major', color='#820000', linestyle='--', linewidth=0.7)\nplt.yticks(range(28, 35, 2))\n#title = \"Timeline of yearly maximum temperature. Years with average temp &gt; 19°C are red.\"\n#plt.title(textwrap.fill(title, width=50))\nplt.title('The burning of the Böögg', fontsize=16, family='Serif')\nax.annotate(\n    textwrap.fill(f\"Big circle for longer burning duration ({duration_max['duration']:.0f} mins)\", 20),   # the text\n    xy=(duration_max['year'], duration_max['tre200mx']),           # point to annotate\n    xytext=(duration_max['year']+5, duration_max['tre200mx'] - 1.5),   # position of text\n    ha=\"center\", va=\"center\", fontsize=10, color=plot_color1,\n    arrowprops=dict(\n        arrowstyle=\"-&gt;\", color=plot_color1, lw=1.5, shrinkB=7.5\n    )\n)\nax.annotate(\n    textwrap.fill(f\"Small circle for shorter burning duration ({duration_min['duration']:.0f} mins)\", 25),\n    xy=(duration_min['year'], duration_min['tre200mx']),           # point to annotate\n    xytext=(duration_min['year']-5, duration_min['tre200mx'] - 1.5),   # position of text\n    ha=\"center\", va=\"center\", fontsize=10, color=plot_color1,\n    arrowprops=dict(\n        arrowstyle=\"-&gt;\", color=plot_color1, lw=1.5, shrinkB=7.5\n    )\n)\nax.annotate(\n    textwrap.fill(\"Average summer temperature &gt;19°C\", 25),\n    xy=(1983, sechselaeuten[sechselaeuten['year']==1983]['tre200mx']),   \n    xytext=(1983, 33.5),  \n    ha=\"center\", va=\"center\", fontsize=10, color=\"#FC0000\",\n    arrowprops=dict(\n        arrowstyle=\"-&gt;\", color=\"#FC0000\", lw=1.5, shrinkB=7.5\n    )\n)\nax.annotate(\n    textwrap.fill(\"Average summer temperature &gt;19°C\", 25),\n    xy=(2015, sechselaeuten[sechselaeuten['year']==2015]['tre200mx']),   \n    xytext=(1983, 33.5),  \n    ha=\"center\", va=\"center\", fontsize=10, color=\"#FC0000\",\n    arrowprops=dict(\n        arrowstyle=\"-&gt;\", color=\"#FC0000\", lw=1.5, shrinkB=7.5\n    )\n)\nplt.savefig(\"sechselaeuten.png\", dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "posts/Billboard_Hits/Billboard_Hits.html",
    "href": "posts/Billboard_Hits/Billboard_Hits.html",
    "title": "Billboard Hot 100 Number Ones",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nbillboard = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-26/billboard.csv')\ntopics = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-26/topics.csv')\n\n\nbillboard\n\n\n\n\n\n\n\n\nsong\nartist\ndate\nweeks_at_number_one\nnon_consecutive\nrating_1\nrating_2\nrating_3\noverall_rating\ndivisiveness\n...\nfeatured_in_a_then_contemporary_play\nwritten_for_a_film\nfeatured_in_a_then_contemporary_film\nwritten_for_a_t_v_show\nfeatured_in_a_then_contemporary_t_v_show\nassociated_with_dance\ntopped_the_charts_by_multiple_artist\ndouble_a_side\neurovision_entry\nu_s_artwork\n\n\n\n\n0\nPoor Little Fool\nRicky Nelson\n1958-08-04T00:00:00Z\n2\n0\n4\n5\n3\n4.000000\n1.333333\n...\nNaN\n0\nNaN\n0\nNaN\n0\n0\nNaN\n0\nCannot Locate\n\n\n1\nNel Blu Dipinto Di Blu\nDomenico Modugno\n1958-08-18T00:00:00Z\n5\n1\n7\n7\n5\n6.333333\n1.333333\n...\nNaN\n0\nNaN\n0\nNaN\n0\n0\nNaN\n1\nCannot Locate\n\n\n2\nLittle Star\nThe Elegants\n1958-08-25T00:00:00Z\n1\n0\n5\n6\n6\n5.666667\n0.666667\n...\nNaN\n0\nNaN\n0\nNaN\n0\n0\nNaN\n0\nCannot Locate\n\n\n3\nIt's All in the Game\nTommy Edwards\n1958-09-29T00:00:00Z\n6\n0\n3\n3\n7\n4.333333\n2.666667\n...\nNaN\n0\nNaN\n0\nNaN\n0\n0\nNaN\n0\nCannot Locate\n\n\n4\nIt's Only Make Believe\nConway Twitty\n1958-11-10T00:00:00Z\n2\n1\n7\n8\n9\n8.000000\n1.333333\n...\nNaN\n0\nNaN\n0\nNaN\n0\n0\nNaN\n0\nCannot Locate\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1172\nPlease Please Please\nSabrina Carpenter\n2024-06-29T00:00:00Z\n1\n0\n8\n7\n8\n7.666667\n0.666667\n...\nNaN\n0\nNaN\n0\nNaN\n0\n0\nNaN\n0\nArtist Photograph\n\n\n1173\nA Bar Song (Tipsy)\nShaboozey\n2024-07-13T00:00:00Z\n19\n1\n6\n4\n9\n6.333333\n3.333333\n...\nNaN\n0\nNaN\n0\nNaN\n0\n0\nNaN\n0\nArtist Photograph;Photograph Related to Song T...\n\n\n1174\nLove Somebody\nMorgan Wallen\n2024-11-02T00:00:00Z\n1\n0\n3\n5\n5\n4.333333\n1.333333\n...\nNaN\n0\nNaN\n0\nNaN\n0\n0\nNaN\n0\nPhotograph Not Related to Song\n\n\n1175\nsquabble up\nKendrick Lamar\n2024-12-07T00:00:00Z\n1\n0\n9\n10\n9\n9.333333\n0.666667\n...\nNaN\n0\nNaN\n0\nNaN\n0\n0\nNaN\n0\nNaN\n\n\n1176\nDie with a Smile\nBruno Mars & Lady Gaga\n2025-01-11T00:00:00Z\n2\n0\n8\n7\n4\n6.333333\n2.666667\n...\nNaN\n0\nNaN\n0\nNaN\n0\n0\nNaN\n0\nArtist Photograph;Text\n\n\n\n\n1177 rows × 105 columns\n\n\n\n\nbillboard.columns.values\n\narray(['song', 'artist', 'date', 'weeks_at_number_one', 'non_consecutive',\n       'rating_1', 'rating_2', 'rating_3', 'overall_rating',\n       'divisiveness', 'label', 'parent_label', 'cdr_genre', 'cdr_style',\n       'discogs_genre', 'discogs_style', 'artist_structure',\n       'featured_artists', 'multiple_lead_vocalists',\n       'group_named_after_non_lead_singer', 'talent_contestant',\n       'posthumous', 'artist_place_of_origin', 'front_person_age',\n       'artist_male', 'artist_white', 'artist_black', 'songwriters',\n       'songwriters_w_o_interpolation_sample_credits', 'songwriter_male',\n       'songwriter_white', 'artist_is_a_songwriter',\n       'artist_is_only_songwriter', 'producers', 'producer_male',\n       'producer_white', 'artist_is_a_producer',\n       'artist_is_only_producer', 'songwriter_is_a_producer',\n       'time_signature', 'keys', 'simplified_key', 'bpm', 'energy',\n       'danceability', 'happiness', 'loudness_d_b', 'acousticness',\n       'vocally_based', 'bass_based', 'guitar_based',\n       'piano_keyboard_based', 'orchestral_strings', 'horns_winds',\n       'accordion', 'banjo', 'bongos', 'clarinet', 'cowbell',\n       'falsetto_vocal', 'flute_piccolo', 'handclaps_snaps', 'harmonica',\n       'human_whistling', 'kazoo', 'mandolin', 'pedal_lap_steel',\n       'ocarina', 'saxophone', 'sitar', 'trumpet', 'ukulele', 'violin',\n       'sound_effects', 'song_structure', 'rap_verse_in_a_non_rap_song',\n       'length_sec', 'instrumental', 'instrumental_length_sec',\n       'intro_length_sec', 'vocal_introduction',\n       'free_time_vocal_introduction', 'fade_out', 'live', 'cover',\n       'sample', 'interpolation', 'inspired_by_a_different_song',\n       'lyrics', 'lyrical_topic', 'lyrical_narrative', 'spoken_word',\n       'explicit', 'foreign_language', 'written_for_a_play',\n       'featured_in_a_then_contemporary_play', 'written_for_a_film',\n       'featured_in_a_then_contemporary_film', 'written_for_a_t_v_show',\n       'featured_in_a_then_contemporary_t_v_show',\n       'associated_with_dance', 'topped_the_charts_by_multiple_artist',\n       'double_a_side', 'eurovision_entry', 'u_s_artwork'], dtype=object)\n\n\n\nbillboard['date'] = pd.to_datetime(billboard['date'])\n# add a new  col date2 having three categories Before 75, 75-2000, After 2000\nbillboard['date2'] = pd.cut(billboard['date'].dt.year, bins=[0, 1975, 2000, 2025], labels=['Before 1975', '1975-2000', 'After 2000'])\nbillboard['date2'].value_counts()\n\ndate2\n1975-2000      484\nBefore 1975    383\nAfter 2000     310\nName: count, dtype: int64\n\n\n\nbillboard['simplified_key_mod'] = billboard['simplified_key'].replace('Multiple Keys', 'multi')\nsk_values = billboard['simplified_key_mod'].value_counts()\nsk_values\n\nsimplified_key_mod\nC        120\nmulti     96\nG         83\nF         76\nD         70\nE         69\nA         67\nDb        57\nAb        49\nBb        48\nEb        42\nAm        40\nFm        34\nCm        32\nGm        31\nDbm       29\nDm        29\nGb        29\nB         29\nBbm       28\nEm        27\nEbm       26\nBm        25\nGbm       21\nAbm       20\nName: count, dtype: int64\n\n\n\nfig,ax = plt.subplots(figsize=(10,5))\n\nsns.swarmplot(data=billboard, x='date2', y='simplified_key_mod', hue='weeks_at_number_one', \\\nax=ax, alpha=0.75)\nax.legend(loc='lower center', ncol=6, bbox_to_anchor=(0.5, -0.2), \\\nframeon=False, title=\"\")\nax.text(-0.55,29.4, \"Weeks at Number One →\")\nsns.despine(bottom=True)\nax.tick_params(axis='x', which='both', length=0)\n\nplt.xlabel('')\nplt.ylabel('Key (simplified)')\nplt.title(\"Changing key preference over the years in Billboard hits.\", fontfamily='serif', \\\n         fontsize=14)\nplt.savefig('billboard_hits.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.pairplot(billboard[['bpm', 'energy','danceability', 'happiness','weeks_at_number_one']], hue='weeks_at_number_one',\\\ncorner=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nA revised visualization for this data was submitted to Plotnine contest 2025 (Repo).\n\n\n\nPlot"
  },
  {
    "objectID": "posts/Billboard_Hits/Billboard_Hits.html#update",
    "href": "posts/Billboard_Hits/Billboard_Hits.html#update",
    "title": "Billboard Hot 100 Number Ones",
    "section": "",
    "text": "A revised visualization for this data was submitted to Plotnine contest 2025 (Repo).\n\n\n\nPlot"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Visualization of various datasets using Python and R",
    "section": "",
    "text": "Zurich’s “Sechselaeuten” spring festival\n\n\n\nScatterplot\n\nTidyTuesday\n\nPyDyTuesday\n\n\n\nExploding snowman and the summer season.\n\n\n\n\n\nDec 2, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nWHO Statistical Performance Indicators\n\n\n\nrelplot\n\nTidyTuesday\n\nPyDyTuesday\n\n\n\nTop 10 countries with the biggest improvement in SPI.\n\n\n\n\n\nNov 25, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nSherlock Holmes stories and novels\n\n\n\nSpacy\n\nTidyTuesday\n\nPyDyTuesday\n\n\n\nFrequency of people and places in all the books.\n\n\n\n\n\nNov 18, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nWHO TB Burden Data\n\n\n\nlineplot\n\nTidyTuesday\n\nPyDyTuesday\n\n\n\nTimeline of global incidence and mortality.\n\n\n\n\n\nNov 11, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nLead concentration in Flint water samples in 2015\n\n\n\nSwarmplot\n\nTidyTuesday\n\nPyDyTuesday\n\n\n\nDescriptive statistics for the Lead distributions.\n\n\n\n\n\nNov 4, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nBritish Literary Prizes\n\n\n\nHeatmap\n\nTidyTuesday\n\nPyDyTuesday\n\n\n\nDistribution of prizes across the highest degree and gender.\n\n\n\n\n\nOct 28, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nHistoric UK Meteorological & Climate Data\n\n\n\nCatplot\n\nTidyTuesday\n\nPyDyTuesday\n\n\n\nMonthly variation in maximum temperature.\n\n\n\n\n\nOct 21, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nFAO’s Suite of Food Security Indicators\n\n\n\nStripplot\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nFood insecurity in the world.\n\n\n\n\n\nOct 14, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nEuroLeague Basketball\n\n\n\nCustom marker\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nFrequency of wins by different teams.\n\n\n\n\n\nOct 7, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nCrane Observations at Lake Hornborgasjön\n\n\n\nSeaborn\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nVariation in Crane observations during the two halves of the year.\n\n\n\n\n\nSep 30, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nFIDE chess player ratings\n\n\n\nPlotnine\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nCountries with maximum number of titled players.\n\n\n\n\n\nSep 23, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nRecipes and Cuisines data from allrecipes.com\n\n\n\nGreat Table\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nFive star cuisines in 10 minutes.\n\n\n\n\n\nSep 16, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nHenley Passport Index Data\n\n\n\nShiny\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nA shiny app for visa information.\n\n\n\n\n\nSep 9, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nFrog sightings in Australia\n\n\n\nTimeline\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nSeasonality of sightings for different frog species.\n\n\n\n\n\nSep 2, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nBillboard Hot 100 Number Ones\n\n\n\nSwarmplot\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nKeys for Billboard hits over the years.\n\n\n\n\n\nAug 26, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nScottish Munro Classification\n\n\n\nHistogram\n\nFacet\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nVariations in Munro and Munro Top assignment over the years.\n\n\n\n\n\nAug 19, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nExtreme Weather Attribution Studies\n\n\n\nsunburst\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nClassification of effect of climate change on different extreme events.\n\n\n\n\n\nAug 12, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nIncome Inequality Before and After Taxes\n\n\n\nParallel coordinates\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nGini coefficient for different countries before and after taxes.\n\n\n\n\n\nAug 5, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nNetflix viewing data\n\n\n\nWordcloud\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nWord cloud for movie and show titles.\n\n\n\n\n\nJul 29, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nMTA Permanent Art Catalog\n\n\n\nTimeline\n\nstem plot\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nEmergence of the different art materials over time.\n\n\n\n\n\nJul 22, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nBritish Library Funding\n\n\n\nTimeline\n\nno-code\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nDecline in the UK govenment funding to the British Library.\n\n\n\n\n\nJul 15, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nThe xkcd color survey\n\n\n\nsklearn\n\ncolors\n\nTidyTuesday\n\nPydyTuesday\n\n\n\nIdentification of colors by the participants.\n\n\n\n\n\nJul 8, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nGas prices in the US\n\n\n\nTimeseries\n\nTidyTuesday\n\nPydyTuesday\n\n\n\nTimeline of gasoline and diesel prices.\n\n\n\n\n\nJul 1, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nMeasles cases across the world\n\n\n\nTimeline\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nTimeline for the global Measles and Rubella cases.\n\n\n\n\n\nJun 24, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nWeb API dataset\n\n\n\nggplot2\n\ncolors\n\nTidyTuesday\n\n\n\nBackground color frequency in the API logos.\n\n\n\n\n\nJun 17, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nThe US Judges data\n\n\n\nPillow\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nGender and racial diversity among the judges.\n\n\n\n\n\nJun 10, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nProject Gutenberg\n\n\n\nggplot2\n\nwordcloud\n\nTidyTuesday\n\nmagick\n\n\n\nFrequency of books in different languages.\n\n\n\n\n\nJun 3, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nDungeons and Dragons Monsters (2024)\n\n\n\nUpset\n\nseaborn\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nFrequency of five senses and their combinations across all the characters.\n\n\n\n\n\nMay 27, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nWater Quality at Sydney Beaches\n\n\n\nggplot2\n\nheatmap\n\nTidyTuesday\n\n\n\nRanking of Sydney beaches based on Enterococci concentration\n\n\n\n\n\nMay 20, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nSeismic events at Mount Vesuvius\n\n\n\nplotly\n\npolar\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nFrequency of earthquakes at different times of the day.\n\n\n\n\n\nMay 13, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nNSF funding cut in April 2025\n\n\n\nsunburst\n\nplotly\n\nNSF\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nDistribution of funding cuts across directorates and divisions.\n\n\n\n\n\nMay 7, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nuseR2025 conference data analysis\n\n\n\nggplot2\n\nword count\n\nTidyTuesday\n\n\n\nTop keywords in useR 2025 talks\n\n\n\n\n\nMay 1, 2025\n\n\nManish Datt\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/API_gurus/API_gurus.html",
    "href": "posts/API_gurus/API_gurus.html",
    "title": "Web API dataset",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggtext)\n\n\napi_categories &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_categories.csv')\napi_info &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_info.csv')\napi_logos &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_logos.csv')\napi_origins &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_origins.csv')\napisguru_apis &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/apisguru_apis.csv')\n\n\nglimpse(api_logos)\n\nRows: 2,529\nColumns: 4\n$ name             &lt;chr&gt; \"1forge.com\", \"1password.com:events\", \"1password.loca…\n$ background_color &lt;chr&gt; \"#24292e\", NA, NA, \"#F26641\", NA, NA, NA, NA, NA, NA,…\n$ url              &lt;chr&gt; \"https://api.apis.guru/v2/cache/logo/https_1forge.com…\n$ alt_text         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\n\nnormalize_to_hex &lt;- function(color_str) {\n  if (is.na(color_str)) return(NA_character_)\n  \n  color_str &lt;- trimws(tolower(color_str))  # Normalize spacing and case\n  \n  if (grepl(\"^rgb\\\\(\", color_str)) {\n    nums &lt;- as.numeric(unlist(regmatches(color_str, gregexpr(\"\\\\d+\", color_str))))\n    if (length(nums) &lt; 3) return(NA_character_)\n    return(rgb(nums[1], nums[2], nums[3], maxColorValue = 255))\n  }\n  \n  # If it's already hex return it as-is\n  if (grepl(\"^#[0-9a-f]{6}$\", color_str)) {\n    return(toupper(color_str))  \n  }\n  \n  # Handle named colors or invalid inputs\n  tryCatch({\n    return(rgb(col2rgb(color_str)[, 1], maxColorValue = 255))\n  }, error = function(e) {\n    return(NA_character_)\n  })\n}\n\napi_logos &lt;- api_logos %&gt;% \n  mutate(hex_color = map_chr(background_color, normalize_to_hex))\n\nFunction for arranging colors by brightness.\n\nhex_brightness &lt;- function(hex) {\n  rgb &lt;- col2rgb(hex)\n  # Use relative luminance formula\n  0.299 * rgb[1, ] + 0.587 * rgb[2, ] + 0.114 * rgb[3, ]\n}\n\n\napi_logos_mod &lt;- api_logos %&gt;%\n  filter(!is.na(hex_color)) %&gt;%\n  select(name, hex_color) %&gt;%\n  group_by(hex_color) %&gt;%\n  summarize(n = n()) %&gt;% \n  arrange(hex_color) %&gt;% \n  filter(!grepl(\"transparent\", hex_color, ignore.case=T)) %&gt;% \n  mutate(brightness = hex_brightness(hex_color),\n         text_color = ifelse(brightness &gt; 128, \"black\", \"white\")) %&gt;%\n  arrange(desc(brightness))"
  },
  {
    "objectID": "posts/API_gurus/API_gurus.html#tidytuesday-data-for-2025-06-17",
    "href": "posts/API_gurus/API_gurus.html#tidytuesday-data-for-2025-06-17",
    "title": "Web API dataset",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggtext)\n\n\napi_categories &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_categories.csv')\napi_info &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_info.csv')\napi_logos &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_logos.csv')\napi_origins &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_origins.csv')\napisguru_apis &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/apisguru_apis.csv')\n\n\nglimpse(api_logos)\n\nRows: 2,529\nColumns: 4\n$ name             &lt;chr&gt; \"1forge.com\", \"1password.com:events\", \"1password.loca…\n$ background_color &lt;chr&gt; \"#24292e\", NA, NA, \"#F26641\", NA, NA, NA, NA, NA, NA,…\n$ url              &lt;chr&gt; \"https://api.apis.guru/v2/cache/logo/https_1forge.com…\n$ alt_text         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\n\nnormalize_to_hex &lt;- function(color_str) {\n  if (is.na(color_str)) return(NA_character_)\n  \n  color_str &lt;- trimws(tolower(color_str))  # Normalize spacing and case\n  \n  if (grepl(\"^rgb\\\\(\", color_str)) {\n    nums &lt;- as.numeric(unlist(regmatches(color_str, gregexpr(\"\\\\d+\", color_str))))\n    if (length(nums) &lt; 3) return(NA_character_)\n    return(rgb(nums[1], nums[2], nums[3], maxColorValue = 255))\n  }\n  \n  # If it's already hex return it as-is\n  if (grepl(\"^#[0-9a-f]{6}$\", color_str)) {\n    return(toupper(color_str))  \n  }\n  \n  # Handle named colors or invalid inputs\n  tryCatch({\n    return(rgb(col2rgb(color_str)[, 1], maxColorValue = 255))\n  }, error = function(e) {\n    return(NA_character_)\n  })\n}\n\napi_logos &lt;- api_logos %&gt;% \n  mutate(hex_color = map_chr(background_color, normalize_to_hex))\n\nFunction for arranging colors by brightness.\n\nhex_brightness &lt;- function(hex) {\n  rgb &lt;- col2rgb(hex)\n  # Use relative luminance formula\n  0.299 * rgb[1, ] + 0.587 * rgb[2, ] + 0.114 * rgb[3, ]\n}\n\n\napi_logos_mod &lt;- api_logos %&gt;%\n  filter(!is.na(hex_color)) %&gt;%\n  select(name, hex_color) %&gt;%\n  group_by(hex_color) %&gt;%\n  summarize(n = n()) %&gt;% \n  arrange(hex_color) %&gt;% \n  filter(!grepl(\"transparent\", hex_color, ignore.case=T)) %&gt;% \n  mutate(brightness = hex_brightness(hex_color),\n         text_color = ifelse(brightness &gt; 128, \"black\", \"white\")) %&gt;%\n  arrange(desc(brightness))"
  },
  {
    "objectID": "posts/API_gurus/API_gurus.html#plotting",
    "href": "posts/API_gurus/API_gurus.html#plotting",
    "title": "Web API dataset",
    "section": "Plotting",
    "text": "Plotting\n\ntitle &lt;- \"&lt;span style='color:white;'&gt;White&lt;/span&gt; is the preferred background &lt;br&gt;color in the API logos. Only one &lt;br&gt;logo has a transparent background.\"\n\napi_logos_mod %&gt;% \nggplot(aes(y = factor(seq_len(nrow(api_logos_mod))),x=1, fill = hex_color)) +\n  geom_tile(aes(width=log(n+1))) +\n  geom_text(aes(label = hex_color, color = text_color), size = 3, family=\"Consolas\") +\n  geom_text(aes(label = n, color = text_color), size = 3, nudge_x = 0.5, family=\"Consolas\") +\n  scale_fill_identity() +\n  scale_color_identity() +\n  theme_void() +\n#  labs(title = \"Background color for the API logos\") +\n  geom_richtext(x = -2.15, y = 20, label = title, size = 4, family=\"Serif\", hjust=0, fill=NA, label.color=NA) +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank(),\n        panel.background = element_rect(fill = \"darkgrey\"))\n\n\n\n\n\n\n\n#ggsave(\"API_logos.png\", dpi=300)"
  },
  {
    "objectID": "posts/BL_funding/BL_funding.html",
    "href": "posts/BL_funding/BL_funding.html",
    "title": "British Library Funding",
    "section": "",
    "text": "import pandas as pd\nbl_funding = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-15/bl_funding.csv')\nbl_funding\n\n\n\n\n\n\n\n\nyear\nnominal_gbp_millions\ngia_gbp_millions\nvoluntary_gbp_millions\ninvestment_gbp_millions\nservices_gbp_millions\nother_gbp_millions\nyear_2000_gbp_millions\ninflation_adjustment\ntotal_y2000_gbp_millions\npercentage_of_y2000_income\ngia_y2000_gbp_millions\nvoluntary_y2000_gbp_millions\ninvestment_y2000_gbp_millions\nservices_y2000_gbp_millions\nother_y2000_gbp_millions\ngia_as_percent_of_peak_gia\n\n\n\n\n0\n2023\n151.800\n127.800\n8.200\n3.000\n12.800\n0.000\nNaN\n1818796.76\n83.461772\n0.757366\n70.266235\n4.508475\n1.649442\n7.037620\n0.000000\n0.743019\n\n\n1\n2022\n147.600\n116.500\n14.900\n0.800\n15.600\n0.000\nNaN\n1674338.62\n88.154211\n0.799947\n69.579713\n8.899036\n0.477801\n9.317112\n0.000000\n0.735759\n\n\n2\n2021\n141.400\n110.412\n20.086\n0.080\n9.753\n1.042\nNaN\n1534702.63\n92.135113\n0.836072\n71.943579\n13.087877\n0.052127\n6.354977\n0.678959\n0.760756\n\n\n3\n2020\n125.900\n108.226\n9.524\n0.196\n7.584\n0.383\nNaN\n1495983.26\n84.158696\n0.763691\n72.344392\n6.366381\n0.131018\n5.069575\n0.256019\n0.764994\n\n\n4\n2019\n121.100\n96.899\n8.626\n0.426\n13.249\n1.925\nNaN\n1483364.96\n81.638709\n0.740823\n65.323776\n5.815157\n0.287185\n8.931720\n1.297725\n0.690756\n\n\n5\n2018\n122.200\n93.443\n10.754\n0.332\n14.558\n3.116\nNaN\n1457265.04\n83.855714\n0.760941\n64.122172\n7.379577\n0.227824\n9.989947\n2.138252\n0.678049\n\n\n6\n2017\n120.800\n93.443\n9.634\n0.269\n16.152\n1.328\nNaN\n1422026.43\n84.949195\n0.770864\n65.711156\n6.774839\n0.189167\n11.358439\n0.933879\n0.694852\n\n\n7\n2016\n118.000\n93.911\n9.672\n0.649\n13.780\n0.000\n76.39\n1384868.58\n85.206641\n0.773200\n67.812211\n6.984056\n0.468637\n9.950403\n0.000000\n0.717069\n\n\n8\n2015\n117.800\n93.043\n9.919\n1.003\n13.799\n0.000\n77.59\n1375792.94\n85.623350\n0.776981\n67.628636\n7.209660\n0.729034\n10.029852\n0.000000\n0.715128\n\n\n9\n2014\n118.900\n93.893\n8.931\n0.707\n15.352\n0.000\n79.09\n1375241.48\n86.457543\n0.784551\n68.273828\n6.494132\n0.514092\n11.163130\n0.000000\n0.721950\n\n\n10\n2013\n124.700\n95.106\n14.993\n0.623\n13.980\n0.000\n84.90\n1355438.10\n91.999775\n0.834844\n70.166244\n11.061368\n0.459630\n10.314008\n0.000000\n0.741961\n\n\n11\n2012\n126.100\n103.868\n6.265\n0.620\n15.349\n0.000\n88.46\n1321543.18\n95.418751\n0.865869\n78.595994\n4.740670\n0.469148\n11.614452\n0.000000\n0.831100\n\n\n12\n2011\n140.100\n101.873\n6.738\n0.531\n17.619\n13.339\n101.44\n1285194.76\n109.010715\n0.989208\n79.266585\n5.242785\n0.413167\n13.709206\n10.378972\n0.838192\n\n\n13\n2010\n137.900\n105.847\n11.774\n0.415\n19.906\n0.000\n105.05\n1230278.88\n112.088407\n1.017136\n86.034965\n9.570188\n0.337322\n16.180071\n0.000000\n0.909763\n\n\n14\n2009\n142.200\n109.464\n9.616\n0.418\n22.674\n0.000\n113.32\n1190997.74\n119.395693\n1.083445\n91.909494\n8.073903\n0.350966\n19.037820\n0.000000\n0.971882\n\n\n15\n2008\n140.500\n106.947\n7.993\n1.492\n24.030\n0.000\n111.37\n1165755.40\n120.522710\n1.093673\n91.740514\n6.856498\n1.279857\n20.613244\n0.000000\n0.970095\n\n\n16\n2007\n141.200\n106.411\n9.789\n1.878\n23.076\n0.000\n116.39\n1125225.72\n125.485934\n1.138711\n94.568581\n8.699588\n1.668998\n20.507885\n0.000000\n1.000000\n\n\n17\n2006\n159.200\n102.639\n31.879\n1.904\n22.768\n0.000\n136.85\n1099681.85\n144.769144\n1.313695\n93.335177\n28.989294\n1.731410\n20.704170\n0.000000\n0.986958\n\n\n18\n2005\n136.900\n97.562\n10.663\n1.770\n24.309\n2.602\n121.44\n1074656.20\n127.389578\n1.155985\n90.784383\n9.922243\n1.647038\n22.620258\n2.421239\n0.959985\n\n\n19\n2004\n121.600\n88.501\n6.399\n1.586\n24.520\n0.000\n110.92\n1052996.65\n115.479950\n1.047912\n84.046801\n6.076942\n1.506178\n23.285924\n0.000000\n0.888739\n\n\n20\n2003\n119.500\n89.263\n4.004\n0.856\n25.423\n0.000\n112.25\n1039036.95\n115.010347\n1.043651\n85.909361\n3.853568\n0.823840\n24.467850\n0.000000\n0.908434\n\n\n21\n2002\n119.200\n85.187\n6.162\n0.795\n27.068\n0.000\n115.20\n1025079.54\n116.283659\n1.055206\n83.102820\n6.011241\n0.775550\n26.405756\n0.000000\n0.878757\n\n\n22\n2001\n120.900\n88.617\n3.283\n0.710\n28.290\n0.000\n118.80\n1012335.12\n119.426855\n1.083728\n87.537218\n3.242997\n0.701349\n27.945291\n0.000000\n0.925648\n\n\n23\n2000\n110.200\n82.266\n2.852\n0.654\n24.491\n0.000\n110.20\n1000000.00\n110.200000\n1.000000\n82.266000\n2.852000\n0.654000\n24.491000\n0.000000\n0.869908\n\n\n24\n1999\n112.300\n78.465\n4.944\n0.455\n28.468\n0.000\n115.62\n992098.37\n113.194420\n1.027173\n79.089939\n4.983377\n0.458624\n28.694735\n0.000000\n0.836324\n\n\n25\n1998\n120.459\n80.450\n8.487\n0.474\n31.048\n0.000\nNaN\n979089.11\n123.031702\n1.116440\n82.168210\n8.668261\n0.484123\n31.711107\n0.000000\n0.868874\n\n\n\n\n\n\n\n\n\n\n\n\nNo-code plot\n\n\nUse the code below to generate the above plot with Deepnote notebook.\n    _dntk.DeepnoteChart(bl_funding, spec_dict={\n  \"layer\": [\n    {\n      \"layer\": [\n        {\n          \"layer\": [\n            {\n              \"mark\": {\n                \"clip\": True,\n                \"type\": \"circle\",\n                \"color\": \"#CC8BFF\",\n                \"tooltip\": True\n              },\n              \"encoding\": {\n                \"x\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": None,\n                    \"format\": \".0f\"\n                  },\n                  \"sort\": None,\n                  \"type\": \"quantitative\",\n                  \"field\": \"year\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"number\",\n                    \"decimals\": 0\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"y\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": \"GBP (millions)\",\n                    \"format\": {\n                      \"type\": \"default\",\n                      \"decimals\": None\n                    },\n                    \"formatType\": \"numberFormatFromNumberType\"\n                  },\n                  \"type\": \"quantitative\",\n                  \"field\": \"gia_y2000_gbp_millions\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"default\",\n                    \"decimals\": None\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"color\": {\n                  \"type\": \"nominal\",\n                  \"datum\": \"UK government (adjusted)\",\n                  \"scale\": {\n                    \"range\": [\n                      \"#CC8BFF\"\n                    ],\n                    \"domain\": [\n                      \"UK government (adjusted)\"\n                    ]\n                  }\n                }\n              },\n              \"transform\": []\n            }\n          ]\n        },\n        {\n          \"layer\": [\n            {\n              \"mark\": {\n                \"clip\": True,\n                \"type\": \"circle\",\n                \"color\": \"#8018D3\",\n                \"tooltip\": True\n              },\n              \"encoding\": {\n                \"x\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": None,\n                    \"format\": \".0f\"\n                  },\n                  \"sort\": None,\n                  \"type\": \"quantitative\",\n                  \"field\": \"year\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"number\",\n                    \"decimals\": 0\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"y\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": \"GBP (millions)\",\n                    \"format\": {\n                      \"type\": \"default\",\n                      \"decimals\": None\n                    },\n                    \"formatType\": \"numberFormatFromNumberType\"\n                  },\n                  \"type\": \"quantitative\",\n                  \"field\": \"total_y2000_gbp_millions\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"default\",\n                    \"decimals\": None\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"color\": {\n                  \"type\": \"nominal\",\n                  \"datum\": \"Total funding (adjusted)\",\n                  \"scale\": {\n                    \"range\": [\n                      \"#8018D3\"\n                    ],\n                    \"domain\": [\n                      \"Total funding (adjusted)\"\n                    ]\n                  }\n                }\n              },\n              \"transform\": []\n            }\n          ]\n        },\n        {\n          \"layer\": [\n            {\n              \"mark\": {\n                \"clip\": True,\n                \"type\": \"line\",\n                \"color\": \"#2266D3\",\n                \"tooltip\": True\n              },\n              \"encoding\": {\n                \"x\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": None,\n                    \"format\": \".0f\"\n                  },\n                  \"sort\": None,\n                  \"type\": \"quantitative\",\n                  \"field\": \"year\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"number\",\n                    \"decimals\": 0\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"y\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": \"GBP (millions)\",\n                    \"format\": {\n                      \"type\": \"default\",\n                      \"decimals\": None\n                    },\n                    \"formatType\": \"numberFormatFromNumberType\"\n                  },\n                  \"type\": \"quantitative\",\n                  \"field\": \"nominal_gbp_millions\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"default\",\n                    \"decimals\": None\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"color\": {\n                  \"type\": \"nominal\",\n                  \"datum\": \"Total funding\",\n                  \"scale\": {\n                    \"range\": [\n                      \"#2266D3\"\n                    ],\n                    \"domain\": [\n                      \"Total funding\"\n                    ]\n                  }\n                }\n              },\n              \"transform\": []\n            }\n          ]\n        },\n        {\n          \"layer\": [\n            {\n              \"mark\": {\n                \"clip\": True,\n                \"type\": \"line\",\n                \"color\": \"#83AFF6\",\n                \"tooltip\": True\n              },\n              \"encoding\": {\n                \"x\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": None,\n                    \"format\": \".0f\"\n                  },\n                  \"sort\": None,\n                  \"type\": \"quantitative\",\n                  \"field\": \"year\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"number\",\n                    \"decimals\": 0\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"y\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": \"GBP (millions)\",\n                    \"format\": {\n                      \"type\": \"default\",\n                      \"decimals\": None\n                    },\n                    \"formatType\": \"numberFormatFromNumberType\"\n                  },\n                  \"type\": \"quantitative\",\n                  \"field\": \"gia_gbp_millions\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"default\",\n                    \"decimals\": None\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"color\": {\n                  \"type\": \"nominal\",\n                  \"datum\": \"UK government\",\n                  \"scale\": {\n                    \"range\": [\n                      \"#83AFF6\"\n                    ],\n                    \"domain\": [\n                      \"UK government\"\n                    ]\n                  }\n                }\n              },\n              \"transform\": []\n            }\n          ]\n        }\n      ],\n      \"resolve\": {\n        \"scale\": {\n          \"color\": \"independent\"\n        }\n      }\n    }\n  ],\n  \"title\": \"Total funding and year 2000 adjusted funding for the British Library.\",\n  \"config\": {\n    \"legend\": {\n      \"orient\": \"top\",\n      \"disable\": False\n    }\n  },\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"encoding\": {}\n})"
  },
  {
    "objectID": "posts/BL_funding/BL_funding.html#plot-using-deepnote-notebook",
    "href": "posts/BL_funding/BL_funding.html#plot-using-deepnote-notebook",
    "title": "British Library Funding",
    "section": "",
    "text": "No-code plot\n\n\nUse the code below to generate the above plot with Deepnote notebook.\n    _dntk.DeepnoteChart(bl_funding, spec_dict={\n  \"layer\": [\n    {\n      \"layer\": [\n        {\n          \"layer\": [\n            {\n              \"mark\": {\n                \"clip\": True,\n                \"type\": \"circle\",\n                \"color\": \"#CC8BFF\",\n                \"tooltip\": True\n              },\n              \"encoding\": {\n                \"x\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": None,\n                    \"format\": \".0f\"\n                  },\n                  \"sort\": None,\n                  \"type\": \"quantitative\",\n                  \"field\": \"year\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"number\",\n                    \"decimals\": 0\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"y\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": \"GBP (millions)\",\n                    \"format\": {\n                      \"type\": \"default\",\n                      \"decimals\": None\n                    },\n                    \"formatType\": \"numberFormatFromNumberType\"\n                  },\n                  \"type\": \"quantitative\",\n                  \"field\": \"gia_y2000_gbp_millions\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"default\",\n                    \"decimals\": None\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"color\": {\n                  \"type\": \"nominal\",\n                  \"datum\": \"UK government (adjusted)\",\n                  \"scale\": {\n                    \"range\": [\n                      \"#CC8BFF\"\n                    ],\n                    \"domain\": [\n                      \"UK government (adjusted)\"\n                    ]\n                  }\n                }\n              },\n              \"transform\": []\n            }\n          ]\n        },\n        {\n          \"layer\": [\n            {\n              \"mark\": {\n                \"clip\": True,\n                \"type\": \"circle\",\n                \"color\": \"#8018D3\",\n                \"tooltip\": True\n              },\n              \"encoding\": {\n                \"x\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": None,\n                    \"format\": \".0f\"\n                  },\n                  \"sort\": None,\n                  \"type\": \"quantitative\",\n                  \"field\": \"year\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"number\",\n                    \"decimals\": 0\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"y\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": \"GBP (millions)\",\n                    \"format\": {\n                      \"type\": \"default\",\n                      \"decimals\": None\n                    },\n                    \"formatType\": \"numberFormatFromNumberType\"\n                  },\n                  \"type\": \"quantitative\",\n                  \"field\": \"total_y2000_gbp_millions\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"default\",\n                    \"decimals\": None\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"color\": {\n                  \"type\": \"nominal\",\n                  \"datum\": \"Total funding (adjusted)\",\n                  \"scale\": {\n                    \"range\": [\n                      \"#8018D3\"\n                    ],\n                    \"domain\": [\n                      \"Total funding (adjusted)\"\n                    ]\n                  }\n                }\n              },\n              \"transform\": []\n            }\n          ]\n        },\n        {\n          \"layer\": [\n            {\n              \"mark\": {\n                \"clip\": True,\n                \"type\": \"line\",\n                \"color\": \"#2266D3\",\n                \"tooltip\": True\n              },\n              \"encoding\": {\n                \"x\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": None,\n                    \"format\": \".0f\"\n                  },\n                  \"sort\": None,\n                  \"type\": \"quantitative\",\n                  \"field\": \"year\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"number\",\n                    \"decimals\": 0\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"y\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": \"GBP (millions)\",\n                    \"format\": {\n                      \"type\": \"default\",\n                      \"decimals\": None\n                    },\n                    \"formatType\": \"numberFormatFromNumberType\"\n                  },\n                  \"type\": \"quantitative\",\n                  \"field\": \"nominal_gbp_millions\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"default\",\n                    \"decimals\": None\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"color\": {\n                  \"type\": \"nominal\",\n                  \"datum\": \"Total funding\",\n                  \"scale\": {\n                    \"range\": [\n                      \"#2266D3\"\n                    ],\n                    \"domain\": [\n                      \"Total funding\"\n                    ]\n                  }\n                }\n              },\n              \"transform\": []\n            }\n          ]\n        },\n        {\n          \"layer\": [\n            {\n              \"mark\": {\n                \"clip\": True,\n                \"type\": \"line\",\n                \"color\": \"#83AFF6\",\n                \"tooltip\": True\n              },\n              \"encoding\": {\n                \"x\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": None,\n                    \"format\": \".0f\"\n                  },\n                  \"sort\": None,\n                  \"type\": \"quantitative\",\n                  \"field\": \"year\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"number\",\n                    \"decimals\": 0\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"y\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": \"GBP (millions)\",\n                    \"format\": {\n                      \"type\": \"default\",\n                      \"decimals\": None\n                    },\n                    \"formatType\": \"numberFormatFromNumberType\"\n                  },\n                  \"type\": \"quantitative\",\n                  \"field\": \"gia_gbp_millions\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"default\",\n                    \"decimals\": None\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"color\": {\n                  \"type\": \"nominal\",\n                  \"datum\": \"UK government\",\n                  \"scale\": {\n                    \"range\": [\n                      \"#83AFF6\"\n                    ],\n                    \"domain\": [\n                      \"UK government\"\n                    ]\n                  }\n                }\n              },\n              \"transform\": []\n            }\n          ]\n        }\n      ],\n      \"resolve\": {\n        \"scale\": {\n          \"color\": \"independent\"\n        }\n      }\n    }\n  ],\n  \"title\": \"Total funding and year 2000 adjusted funding for the British Library.\",\n  \"config\": {\n    \"legend\": {\n      \"orient\": \"top\",\n      \"disable\": False\n    }\n  },\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"encoding\": {}\n})"
  },
  {
    "objectID": "posts/British_Lit_Prizes/British_lit_prizes.html",
    "href": "posts/British_Lit_Prizes/British_lit_prizes.html",
    "title": "British Literary Prizes",
    "section": "",
    "text": "TidyTuesday dataset of October 28, 2025\n\nimport marimo as mo\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nprizes = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-28/prizes.csv')\n\n\nprizes\n\n\n\n\n\n\n\n\nprize_id\nprize_alias\nprize_name\nprize_institution\nprize_year\nprize_genre\nperson_id\nperson_role\nlast_name\nfirst_name\n...\nuk_residence\nethnicity_macro\nethnicity\nhighest_degree\ndegree_institution\ndegree_field_category\ndegree_field\nviaf\nbook_id\nbook_title\n\n\n\n\n0\n8\nBooker Prize\nBooker Prize\nBooker Foundation\n1991\nfiction\n294\nshortlisted\nAmis\nMartin\n...\nTrue\nWhite British\nEnglish\nBachelors\nUniversity of Oxford\nLanguage and Literature\nEnglish Literature\n36913662\n5\nTime's Arrow\n\n\n1\n1\nJames Tait Black Prize for Fiction\nJames Tait Black Prize for Fiction\nThe University of Edinburgh\n1991\nfiction\n33\nwinner\nBoyd\nWilliam\n...\nTrue\nWhite British\nBritish\nunknown\nUniversity of Oxford\nLanguage and Literature\nEnglish Literature\n111500719\n36\nBrazzaville Beach\n\n\n2\n3\nCosta First Novel Award\nWhitbread First Novel\nWhitbread\n1991\nfiction\n167\nwinner\nBurn\nGordon\n...\nTrue\nWhite British\nEnglish\nnone\nnone\nnone\nnone\n51988764\n42\nAlma Cogan\n\n\n3\n8\nBooker Prize\nBooker Prize\nBooker Foundation\n1991\nfiction\n286\nshortlisted\nDoyle\nRoddy\n...\nFalse\nIrish\nIrish\nBachelors\nUniversity College Dublin\nMultiple\nEnglish and Geography\n17301306\n77\nThe Van\n\n\n4\n4\nCosta Novel Award\nWhitbread Novel\nWhitbread\n1991\nfiction\n168\nwinner\nGardam\nJane\n...\nTrue\nWhite British\nEnglish\nBachelors\nUniversity of London\nLanguage and Literature\nEnglish Literature\n70213168\n114\nThe Queen of the Tambourine\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n947\n10\nWomen's Prize for Fiction\nWomen's Prize for Fiction\nWomen's Prize Trust\n2022\nfiction\n192\nshortlisted\nShafak\nElif\n...\nTrue\nNon-UK White\nTurkish British\nDoctorate\nMiddle East Technical University\nPolitics and Economics\nPolitical Science\n64320935\n367\nThe Island of Missing Trees\n\n\n948\n11\nGold Dagger\nGold Dagger\nThe Crime Writers' Association\n2022\ncrime\n410\nshortlisted\nShaw\nWilliam\n...\nTrue\nWhite British\nEnglish\nunknown\nunknown\nunknown\nunknown\nNaN\n521\nThe Trawlerman\n\n\n949\n10\nWomen's Prize for Fiction\nWomen's Prize for Fiction\nWomen's Prize Trust\n2022\nfiction\n182\nshortlisted\nShipstead\nMaggie\n...\nFalse\nNon-UK White\nWhite American\nMasters\nUniversity of Iowa\nWriting\nCreative Writing\n231972795\n296\nGreat Circle\n\n\n950\n8\nBooker Prize\nBooker Prize\nBooker Foundation\n2022\nfiction\n176\nshortlisted\nStrout\nElizabeth\n...\nFalse\nNon-UK White\nWhite American\nMasters\nRoyal College of Art\nunknown\nunknown\n66631918\n316\nOh William!\n\n\n951\n12\nBSFA Award for Best Novel\nBSFA Award for Best Novel\nBritish Science Fiction Association\n2022\nsff\n491\nwinner\nTchaikovsky\nAdrian\n...\nTrue\nWhite British\nBritish\nBachelors\nUniversity of Reading\nMaths and Sciences\nZoology and Psychology\n102929550\n627\nCity of Last Chances\n\n\n\n\n952 rows × 23 columns\n\n\n\n\nprizes['prize_genre']=prizes['prize_genre'].str.capitalize()\n\n\nprizes.groupby(['prize_genre','highest_degree']).count()['prize_name'].sort_values(ascending=False)\n\nprize_genre         highest_degree          \nFiction             Bachelors                   147\n                    Masters                     144\n                    unknown                      75\nNon-fiction         Doctorate                    59\nFiction             Doctorate                    52\nCrime               unknown                      46\nPoetry              Bachelors                    43\nNon-fiction         unknown                      33\n                    Bachelors                    30\nCrime               Bachelors                    30\nPoetry              unknown                      25\nFiction             none                         24\nPoetry              Masters                      23\n                    Doctorate                    19\nNo/any/multi genre  Bachelors                    17\nNon-fiction         Masters                      15\nChildren's          unknown                      14\nCrime               Masters                      12\nSff                 unknown                      12\nBiography           unknown                      11\nCrime               none                         10\nChildren's          Bachelors                    10\nPoetry              none                          9\nBiography           Bachelors                     9\nCrime               Doctorate                     9\nSff                 Masters                       7\nNo/any/multi genre  unknown                       7\nBiography           Doctorate                     7\nSff                 Bachelors                     6\n                    Doctorate                     5\nDrama               unknown                       4\n                    Bachelors                     4\nNo/any/multi genre  Doctorate                     4\nNon-fiction         none                          3\nChildren's          Masters                       3\nSff                 none                          3\nFiction             Postgraduate                  3\nChildren's          Doctorate                     2\nBiography           Masters                       2\nNo/any/multi genre  Masters                       2\nFiction             Juris Doctor                  2\nBiography           Diploma                       1\nChildren's          Postgraduate                  1\nBiography           Postgraduate                  1\nPoetry              MD                            1\n                    Postgraduate                  1\nBiography           Juris Doctor                  1\nNon-fiction         Juris Doctor                  1\nChildren's          none                          1\nNo/any/multi genre  none                          1\nFiction             Certificate of Education      1\nName: prize_name, dtype: int64\n\n\n\nprizes.groupby(['degree_field_category']).count()['prize_name'].sort_values(ascending=False)\n\ndegree_field_category\nLanguage and Literature         272\nunknown                         265\nWriting                         114\nHistory and Cultural Studies     72\nnone                             51\nArts                             40\nMaths and Sciences               34\nMultiple                         30\nPolitics and Economics           27\nPhilosophy and Theology          24\nLaw                              17\nMedicine and Social Work          5\nEducation                         1\nName: prize_name, dtype: int64\n\n\n\ndf = prizes[(~prizes[\"degree_field_category\"].isin(['unknown','none'])) & (prizes[\"highest_degree\"].isin(['Bachelors','Masters', 'Doctorate']))]\ndf\n\n\n\n\n\n\n\n\nprize_id\nprize_alias\nprize_name\nprize_institution\nprize_year\nprize_genre\nperson_id\nperson_role\nlast_name\nfirst_name\n...\nuk_residence\nethnicity_macro\nethnicity\nhighest_degree\ndegree_institution\ndegree_field_category\ndegree_field\nviaf\nbook_id\nbook_title\n\n\n\n\n0\n8\nBooker Prize\nBooker Prize\nBooker Foundation\n1991\nFiction\n294\nshortlisted\nAmis\nMartin\n...\nTrue\nWhite British\nEnglish\nBachelors\nUniversity of Oxford\nLanguage and Literature\nEnglish Literature\n36913662\n5\nTime's Arrow\n\n\n3\n8\nBooker Prize\nBooker Prize\nBooker Foundation\n1991\nFiction\n286\nshortlisted\nDoyle\nRoddy\n...\nFalse\nIrish\nIrish\nBachelors\nUniversity College Dublin\nMultiple\nEnglish and Geography\n17301306\n77\nThe Van\n\n\n4\n4\nCosta Novel Award\nWhitbread Novel\nWhitbread\n1991\nFiction\n168\nwinner\nGardam\nJane\n...\nTrue\nWhite British\nEnglish\nBachelors\nUniversity of London\nLanguage and Literature\nEnglish Literature\n70213168\n114\nThe Queen of the Tambourine\n\n\n6\n6\nCosta Poetry Award\nWhitbread Poetry\nWhitbread\n1991\nPoetry\n169\nwinner\nLongley\nMichael\n...\nTrue\nWhite British\nNorthern Irish\nBachelors\nTrinity College, Dublin\nLanguage and Literature\nClassics\n39398205\n182\nGorse Fires\n\n\n7\n8\nBooker Prize\nBooker Prize\nBooker Foundation\n1991\nFiction\n260\nshortlisted\nMistry\nRohinton\n...\nFalse\nAsian\nIndian Canadian\nBachelors\nBombay University/University of Toronto\nMultiple\nMathematics and Economics/English and Philosophy\n29581388\n225\nSuch a Long Journey\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n944\n10\nWomen's Prize for Fiction\nWomen's Prize for Fiction\nWomen's Prize Trust\n2022\nFiction\n221\nwinner\nOzeki\nRuth\n...\nFalse\nAsian\nJapanese American\nBachelors\nSmith College\nLanguage and Literature\nEnglish Literature and Asian Studies\n79458623\n371\nThe Book of Form & Emptiness\n\n\n946\n16\nBaillie Gifford Prize for Non-Fiction\nBaillie Gifford Prize for Non-Fiction\nSamuel Johnson Prize for Non-Fiction Limited\n2022\nNon-fiction\n58\nwinner\nRundell\nKatherine\n...\nTrue\nWhite British\nEnglish\nDoctorate\nUniversity of Oxford\nLanguage and Literature\nEnglish Literature\n163169611\n759\nSuper-Infinite: The Transformations of John Donne\n\n\n947\n10\nWomen's Prize for Fiction\nWomen's Prize for Fiction\nWomen's Prize Trust\n2022\nFiction\n192\nshortlisted\nShafak\nElif\n...\nTrue\nNon-UK White\nTurkish British\nDoctorate\nMiddle East Technical University\nPolitics and Economics\nPolitical Science\n64320935\n367\nThe Island of Missing Trees\n\n\n949\n10\nWomen's Prize for Fiction\nWomen's Prize for Fiction\nWomen's Prize Trust\n2022\nFiction\n182\nshortlisted\nShipstead\nMaggie\n...\nFalse\nNon-UK White\nWhite American\nMasters\nUniversity of Iowa\nWriting\nCreative Writing\n231972795\n296\nGreat Circle\n\n\n951\n12\nBSFA Award for Best Novel\nBSFA Award for Best Novel\nBritish Science Fiction Association\n2022\nSff\n491\nwinner\nTchaikovsky\nAdrian\n...\nTrue\nWhite British\nBritish\nBachelors\nUniversity of Reading\nMaths and Sciences\nZoology and Psychology\n102929550\n627\nCity of Last Chances\n\n\n\n\n580 rows × 23 columns\n\n\n\n\n#fix,ax = plt.subplots(figsize=(10,5))\nsns.catplot(data=df, \\\n            x='prize_year', y='prize_genre', hue='gender', dodge=True, kind='strip', \\\n            col='highest_degree', col_wrap=2, aspect=2, legend=False, jitter=True, size=10)\n\n\n\n\n\n\n\n\n\npivot_table = df.pivot_table(\n    index='prize_genre',\n    columns='highest_degree',\n    values='prize_name',\n    aggfunc='count',\n    fill_value=0\n)\n\n\nplt.figure(figsize=(8, 5))\nsns.heatmap(pivot_table, annot=True, fmt='d', cmap='YlGnBu', linewidths=0.5)\n#plt.title('Tile Plot of Prizes by Degree Field and Degree Level')\n#plt.xlabel('Highest Degree')\n#plt.ylabel('Degree Field Category')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nprizes['degree'] = prizes['highest_degree'].apply(lambda x: x if x in ['Bachelors', 'Masters', 'Doctorate'] else 'Others')\nprizes['gender_new'] = prizes['gender'].apply(lambda x: x if x in ['man', 'woman'] else 'NB')\n\n\ndf1= (prizes.groupby(['prize_genre', 'degree', 'gender_new']).count()['prize_id'].reset_index())\ndf1['degree_gender'] = df1['degree']+'_'+df1['gender_new']\ndf1\n\n\n\n\n\n\n\n\nprize_genre\ndegree\ngender_new\nprize_id\ndegree_gender\n\n\n\n\n0\nBiography\nBachelors\nman\n3\nBachelors_man\n\n\n1\nBiography\nBachelors\nwoman\n6\nBachelors_woman\n\n\n2\nBiography\nDoctorate\nman\n5\nDoctorate_man\n\n\n3\nBiography\nDoctorate\nwoman\n2\nDoctorate_woman\n\n\n4\nBiography\nMasters\nman\n1\nMasters_man\n\n\n...\n...\n...\n...\n...\n...\n\n\n67\nSff\nDoctorate\nwoman\n1\nDoctorate_woman\n\n\n68\nSff\nMasters\nman\n5\nMasters_man\n\n\n69\nSff\nMasters\nwoman\n2\nMasters_woman\n\n\n70\nSff\nOthers\nman\n12\nOthers_man\n\n\n71\nSff\nOthers\nwoman\n3\nOthers_woman\n\n\n\n\n72 rows × 5 columns\n\n\n\n\nreshaped = df1.pivot_table(\n    index='prize_genre',\n    columns='degree_gender',\n    values='prize_id',\n    aggfunc='sum',  # or 'count' if you want number of entries\n    fill_value=0\n).reset_index()\nreshaped = reshaped.set_index('prize_genre')\n\n\nind = prizes.groupby(['prize_genre']).count()['prize_name'].sort_values(ascending=False).index\nreshaped.columns\n\nIndex(['Bachelors_NB', 'Bachelors_man', 'Bachelors_woman', 'Doctorate_NB',\n       'Doctorate_man', 'Doctorate_woman', 'Masters_NB', 'Masters_man',\n       'Masters_woman', 'Others_NB', 'Others_man', 'Others_woman'],\n      dtype='object', name='degree_gender')\n\n\n\nreshaped1 = reshaped[['Bachelors_man', 'Bachelors_woman', 'Masters_man', 'Masters_woman',\n       'Doctorate_man', 'Doctorate_woman', 'Others_man', 'Others_woman']]\nreshaped1 = reshaped1.reindex(ind)\nreshaped1.index = reshaped1.index.str.replace('Sff', 'Sci-Fi and Fantasy')\n\n\nbg_color = '#DBD4FF'\nfg_color = '#723480'\nfg_color2 = '#FFFFE3'\n\nlw=5\ncmap = sns.color_palette(\"Wistia\", as_cmap=True)\ncmap.set_under('#FFFFE3')  \n\nfig1,ax1 = plt.subplots()\nsns.heatmap(reshaped1, annot=True, fmt='d', cmap=cmap, cbar=False, \\\n            linewidths=lw, ax=ax1,vmin=0.1, annot_kws={\"fontfamily\": \"monospace\"}, \\\n            cbar_kws={'shrink': 0.6, 'aspect': 30, 'pad': 0.02})\nfig1.set_facecolor(bg_color)\nax1.set_facecolor(bg_color)\nax1.patch.set_facecolor(bg_color)\n\nax_top = ax1.twiny()\n\n# Match the position and scale of the bottom axis\nax_top.set_xlim(ax1.get_xlim())\n\nax_top.set_xticks(ax1.get_xticks())\nax_top.set_xticklabels(['M', 'W']*4, fontsize=9)\n\nfor i in range(0, len(reshaped1.columns)+1):  \n    if(i % 2 == 0):\n        ax1.vlines(i, *ax1.get_ylim(), colors=bg_color, linewidth=lw)\n    else:\n        ax1.vlines(i, *ax1.get_ylim(), colors=fg_color2, linewidth=lw)\nfor i in range(0, len(reshaped1.index)+1):  \n    ax1.hlines(i, *ax1.get_xlim(), colors=bg_color, linewidth=lw)\n\nxticklabels = ax1.get_xticklabels()\nnew_labels = [label._text.split('_')[0] if i % 2 == 0 else '' for i, label in enumerate(xticklabels)]\n\n# Apply the new labels\nax1.set_xticklabels(new_labels, rotation=0, ha='left', fontsize=10)\nax1.tick_params(axis='both', which='both', length=0)\nax_top.tick_params(axis='both', which='both', length=0, pad=0)\nax1.set_xlabel('')\nax1.set_ylabel('')\n#colorbar = ax1.collections[0].colorbar\n#colorbar.ax.tick_params(labelsize=10)\nfor label in ax1.get_xticklabels()+ax_top.get_xticklabels():\n    label.set_fontfamily('monospace')\n    label.set_color(fg_color)\nfor label in ax1.get_yticklabels():\n    label.set_fontfamily('monospace')\n    label.set_color(fg_color)\nsns.despine(left=True,bottom=True)    \n#plt.tight_layout()\nplt.title(\"Genre-wise distribution of the British literary prizes \\nbased on the highest degree and gender of the authors.\", fontfamily='Serif', fontsize=16, color=fg_color, loc='left', x=-0.3, y=1.07)\nplt.savefig('British_prize.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "posts/Dungeons_Dragons/DD_monsters.html",
    "href": "posts/Dungeons_Dragons/DD_monsters.html",
    "title": "Dungeons and Dragons Monsters (2024)",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nmonsters = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-27/monsters.csv')\n\n\nmonsters\n\n\n\n\n\n\n\n\nname\ncategory\ncr\nsize\ntype\ndescriptive_tags\nalignment\nac\ninitiative\nhp\n...\nwis_save\ncha_save\nskills\nresistances\nvulnerabilities\nimmunities\ngear\nsenses\nlanguages\nfull_text\n\n\n\n\n0\nAboleth\nAboleth\n10.000\nLarge\nAberration\nNaN\nLawful Evil\n17\n7\n150 (20d10 + 40)\n...\n6\n4\nHistory +12, Perception +10\nNaN\nNaN\nNaN\nNaN\nDarkvision 120 ft.; Passive Perception 20\nDeep Speech; telepathy 120 ft.\nAboleth\\nLarge Aberration, Lawful Evil\\nAC 17\\...\n\n\n1\nAir Elemental\nAir Elemental\n5.000\nLarge\nElemental\nNaN\nNeutral\n15\n5\n90 (12d10 + 24)\n...\n0\n-2\nNaN\nBludgeoning, Lightning, Piercing, Slashing\nNaN\nPoison, Thunder; Exhaustion, Grappled, Paralyz...\nNaN\nDarkvision 60 ft.; Passive Perception 10\nPrimordial (Auran)\nAir Elemental\\nLarge Elemental, Neutral\\nAC 15...\n\n\n2\nAnimated Armor\nAnimated Objects\n1.000\nMedium\nConstruct\nNaN\nUnaligned\n18\n2\n33 (6d8 + 6)\n...\n-4\n-5\nNaN\nNaN\nNaN\nPoison, Psychic; Charmed, Deafened, Exhaustion...\nNaN\nBlindsight 60 ft.; Passive Perception 6\nNaN\nAnimated Armor\\nMedium Construct, Unaligned\\nA...\n\n\n3\nAnimated Flying Sword\nAnimated Objects\n0.250\nSmall\nConstruct\nNaN\nUnaligned\n17\n4\n14 (4d6)\n...\n-3\n-5\nNaN\nNaN\nNaN\nPoison, Psychic; Charmed, Deafened, Exhaustion...\nNaN\nBlindsight 60 ft.; Passive Perception 7\nNaN\nAnimated Flying Sword\\nSmall Construct, Unalig...\n\n\n4\nAnimated Rug of Smothering\nAnimated Objects\n2.000\nLarge\nConstruct\nNaN\nUnaligned\n12\n4\n27 (5d10)\n...\n-4\n-5\nNaN\nNaN\nNaN\nPoison, Psychic; Charmed, Deafened, Exhaustion...\nNaN\nBlindsight 60 ft.; Passive Perception 6\nNaN\nAnimated Rug of Smothering\\nLarge Construct, U...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n325\nVenomous Snake\nAnimals\n0.125\nTiny\nBeast\nNaN\nUnaligned\n12\n2\n5 (2d4)\n...\n0\n-4\nNaN\nNaN\nNaN\nNaN\nNaN\nBlindsight 10 ft.; Passive Perception 10\nNaN\nVenomous Snake\\nTiny Beast, Unaligned\\nAC 12 \\...\n\n\n326\nVulture\nAnimals\n0.000\nMedium\nBeast\nNaN\nUnaligned\n10\n0\n5 (1d8 + 1)\n...\n1\n-3\nPerception +3\nNaN\nNaN\nNaN\nNaN\nPassive Perception 13\nNaN\nVulture\\nMedium Beast, Unaligned\\nAC 10 \\t\\t ...\n\n\n327\nWarhorse\nAnimals\n0.500\nLarge\nBeast\nNaN\nUnaligned\n11\n1\n19 (3d10 + 3)\n...\n3\n-2\nNaN\nNaN\nNaN\nNaN\nNaN\nPassive Perception 11\nNaN\nWarhorse\\nLarge Beast, Unaligned\\nAC 11 \\t\\t ...\n\n\n328\nWeasel\nAnimals\n0.000\nTiny\nBeast\nNaN\nUnaligned\n13\n3\n1 (1d4 − 1)\n...\n1\n-4\nAcrobatics +5, Perception +3, Stealth +5\nNaN\nNaN\nNaN\nNaN\nDarkvision 60 ft.; Passive Perception 13\nNaN\nWeasel\\nTiny Beast, Unaligned\\nAC 13 \\t\\t ...\n\n\n329\nWolf\nAnimals\n0.250\nMedium\nBeast\nNaN\nUnaligned\n12\n2\n11 (2d8 + 2)\n...\n1\n-2\nPerception +5, Stealth +4\nNaN\nNaN\nNaN\nNaN\nDarkvision 60 ft.; Passive Perception 15\nNaN\nWolf\\nMedium Beast, Unaligned\\nAC 12 \\t\\t ...\n\n\n\n\n330 rows × 33 columns\n\n\n\n\nmonsters.columns\n\nIndex(['name', 'category', 'cr', 'size', 'type', 'descriptive_tags',\n       'alignment', 'ac', 'initiative', 'hp', 'hp_number', 'speed',\n       'speed_base_number', 'str', 'dex', 'con', 'int', 'wis', 'cha',\n       'str_save', 'dex_save', 'con_save', 'int_save', 'wis_save', 'cha_save',\n       'skills', 'resistances', 'vulnerabilities', 'immunities', 'gear',\n       'senses', 'languages', 'full_text'],\n      dtype='object')\n\n\n\nmonsters.describe()\n\n\n\n\n\n\n\n\ncr\nac\ninitiative\nhp_number\nspeed_base_number\nstr\ndex\ncon\nint\nwis\ncha\nstr_save\ndex_save\ncon_save\nint_save\nwis_save\ncha_save\n\n\n\n\ncount\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n\n\nmean\n4.551136\n14.287879\n3.148485\n86.669697\n30.878788\n15.384848\n12.833333\n15.178788\n7.863636\n11.815152\n9.918182\n2.675758\n2.118182\n2.784848\n-1.093939\n1.872727\n0.003030\n\n\nstd\n5.797444\n3.149589\n3.944803\n102.140570\n12.339566\n6.520047\n3.261563\n4.404492\n5.675860\n2.966748\n5.969220\n3.532010\n2.452213\n2.869886\n3.224190\n2.967224\n3.524554\n\n\nmin\n0.000000\n5.000000\n-5.000000\n1.000000\n5.000000\n1.000000\n1.000000\n8.000000\n1.000000\n3.000000\n1.000000\n-5.000000\n-5.000000\n-1.000000\n-5.000000\n-4.000000\n-5.000000\n\n\n25%\n0.500000\n12.000000\n1.000000\n18.250000\n30.000000\n11.000000\n10.000000\n12.000000\n2.000000\n10.000000\n5.000000\n0.000000\n1.000000\n1.000000\n-4.000000\n0.000000\n-3.000000\n\n\n50%\n2.000000\n14.000000\n2.000000\n52.000000\n30.000000\n16.000000\n13.000000\n14.500000\n7.000000\n12.000000\n8.000000\n3.000000\n2.000000\n2.000000\n-2.000000\n1.000000\n-1.000000\n\n\n75%\n6.000000\n17.000000\n4.000000\n119.000000\n40.000000\n19.000000\n15.000000\n17.000000\n12.000000\n13.000000\n14.000000\n4.000000\n3.000000\n4.000000\n1.000000\n3.000000\n2.000000\n\n\nmax\n30.000000\n25.000000\n20.000000\n697.000000\n60.000000\n30.000000\n28.000000\n30.000000\n25.000000\n25.000000\n30.000000\n17.000000\n10.000000\n15.000000\n12.000000\n12.000000\n12.000000\n\n\n\n\n\n\n\n\nmonsters.groupby([\"senses\"]).size().sort_values(ascending=False)\n\nsenses\nDarkvision 60 ft.; Passive Perception 10                         24\nPassive Perception 10                                            23\nDarkvision 60 ft.; Passive Perception 14                         15\nDarkvision 60 ft.; Passive Perception 13                         15\nDarkvision 60 ft.; Passive Perception 15                         14\n                                                                 ..\nDarkvision 30 ft.; Passive Perception 11                          1\nDarkvision 30 ft.; Passive Perception 13                          1\nDarkvision 30 ft.; Passive Perception 9                           1\nDarkvision 60 ft., Tremorsense 120 ft.; Passive Perception 16     1\nTruesight 60 ft.; Passive Perception 19                           1\nLength: 100, dtype: int64\n\n\n\nmonsters.groupby([\"size\"]).describe()[\"cr\"] \n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nsize\n\n\n\n\n\n\n\n\n\n\n\n\nGargantuan\n15.0\n21.066667\n4.366539\n11.000\n20.0000\n22.00\n23.000\n30.0\n\n\nHuge\n34.0\n9.264706\n5.088959\n2.000\n5.0000\n8.50\n13.750\n19.0\n\n\nLarge\n107.0\n5.003505\n4.626958\n0.125\n1.5000\n4.00\n7.500\n21.0\n\n\nMedium\n90.0\n2.183333\n3.178238\n0.000\n0.2500\n1.00\n3.000\n21.0\n\n\nMedium or Small\n36.0\n3.524306\n3.635124\n0.000\n0.8750\n3.00\n5.000\n15.0\n\n\nSmall\n23.0\n0.271739\n0.254650\n0.000\n0.0625\n0.25\n0.500\n1.0\n\n\nTiny\n25.0\n0.235000\n0.491225\n0.000\n0.0000\n0.00\n0.125\n2.0\n\n\n\n\n\n\n\n\nsns.heatmap(monsters.select_dtypes(include='number').corr())\n\n\n\n\n\n\n\n\n\nmonsters['senses'].values[:10]\n\narray(['Darkvision 120 ft.; Passive Perception 20',\n       'Darkvision 60 ft.; Passive Perception 10',\n       'Blindsight 60 ft.; Passive Perception 6',\n       'Blindsight 60 ft.; Passive Perception 7',\n       'Blindsight 60 ft.; Passive Perception 6',\n       'Darkvision 60 ft., Tremorsense 60 ft.; Passive Perception 11',\n       'Passive Perception 16', 'Passive Perception 10',\n       'Passive Perception 10', 'Passive Perception 10'], dtype=object)\n\n\n\nmonsters[monsters['senses'].astype(str).str.contains('unimpeded', case=False, na=False)]\n\n\n\n\n\n\n\n\nname\ncategory\ncr\nsize\ntype\ndescriptive_tags\nalignment\nac\ninitiative\nhp\n...\nwis_save\ncha_save\nskills\nresistances\nvulnerabilities\nimmunities\ngear\nsenses\nlanguages\nfull_text\n\n\n\n\n14\nBarbed Devil\nBarbed Devil\n5.0\nMedium\nFiend\nDevil\nLawful Evil\n15\n3\n110 (13d8 + 52)\n...\n5\n5\nDeception +5, Insight +5, Perception +8\nCold\nNaN\nFire, Poison; Poisoned\nNaN\nDarkvision 120 ft. (unimpeded by magical Darkn...\nInfernal; telepathy 120 ft.\nBarbed Devil\\nMedium Fiend (Devil), Lawful Evi...\n\n\n16\nBearded Devil\nBearded Devil\n3.0\nMedium\nFiend\nDevil\nLawful Evil\n13\n2\n58 (9d8 + 18)\n...\n0\n4\nNaN\nCold\nNaN\nFire, Poison; Frightened, Poisoned\nNaN\nDarkvision 120 ft. (unimpeded by magical Darkn...\nInfernal; telepathy 120 ft.\nBearded Devil\\nMedium Fiend (Devil), Lawful Ev...\n\n\n29\nBone Devil\nBone Devil\n9.0\nLarge\nFiend\nDevil\nLawful Evil\n16\n7\n161 (17d10 + 68)\n...\n6\n7\nDeception +7, Insight +6\nCold\nNaN\nFire, Poison; Poisoned\nNaN\nDarkvision 120 ft. (unimpeded by magical Darkn...\nInfernal; telepathy 120 ft.\nBone Devil\\nLarge Fiend (Devil), Lawful Evil\\n...\n\n\n42\nChain Devil\nChain Devil\n8.0\nMedium\nFiend\nDevil\nLawful Evil\n15\n5\n85 (10d8 + 40)\n...\n4\n2\nNaN\nBludgeoning, Cold, Piercing, Slashing\nNaN\nFire, Poison; Poisoned\nNaN\nDarkvision 120 ft. (unimpeded by magical Darkn...\nInfernal; telepathy 120 ft.\nChain Devil\\nMedium Fiend (Devil), Lawful Evil...\n\n\n117\nHorned Devil\nHorned Devil\n11.0\nLarge\nFiend\nDevil\nLawful Evil\n18\n7\n199 (19d10 + 95)\n...\n7\n8\nNaN\nCold\nNaN\nFire, Poison; Poisoned\nNaN\nDarkvision 150 ft. (unimpeded by magical Darkn...\nInfernal; telepathy 120 ft.\nHorned Devil\\nLarge Fiend (Devil), Lawful Evil...\n\n\n120\nImp\nImp\n1.0\nTiny\nFiend\nDevil\nLawful Evil\n13\n3\n21 (6d4 + 6)\n...\n1\n2\nDeception +4, Insight +3, Stealth +5\nCold\nNaN\nFire, Poison; Poisoned\nNaN\nDarkvision 120 ft. (unimpeded by magical Darkn...\nCommon, Infernal\nImp\\nTiny Fiend (Devil), Lawful Evil\\nAC 13 \\t...\n\n\n128\nLemure\nLemure\n0.0\nMedium\nFiend\nDevil\nLawful Evil\n9\n-3\n9 (2d8)\n...\n0\n-4\nNaN\nCold\nNaN\nFire, Poison; Charmed, Frightened, Poisoned\nNaN\nDarkvision 120 ft. (unimpeded by magical Darkn...\nUnderstands Infernal but can’t speak\nLemure\\nMedium Fiend (Devil), Lawful Evil\\nAC ...\n\n\n\n\n7 rows × 33 columns\n\n\n\n\nimport re\n\n\ndef extract_senses(sense_str):\n    result = {}\n    if not isinstance(sense_str, str):\n        return result\n\n    # Senses to extract\n    sense_names = ['darkvision', 'blindsight', 'tremorsense', 'truesight']\n    \n    # General pattern for senses with ft.\n    for match in re.findall(r'([a-zA-Z]+)\\s+(\\d+)\\s*ft*\\.?', sense_str, flags=re.IGNORECASE):\n        name, value = match\n        name = name.strip().lower()\n        if name in sense_names:\n            result[name] = int(value)\n    \n    # Passive Perception (no ft.)\n    pp_match = re.search(r'Passive Perception\\s+(\\d+)', sense_str, flags=re.IGNORECASE)\n    if pp_match:\n        result['passive perception'] = int(pp_match.group(1))\n    \n    return result\n\n\n\nsenses_df = monsters['senses'].apply(extract_senses).apply(pd.Series).fillna(0).astype(int)\nsenses_df\n#(senses_df['passive perception'] == 0).any()\n#senses_df[(senses_df == 0).sum(axis=1) == 2]\n#senses_df[senses_df[\"tremorsense\"] &gt; 0]\n\n\n\n\n\n\n\n\ndarkvision\npassive perception\nblindsight\ntremorsense\ntruesight\n\n\n\n\n0\n120\n20\n0\n0\n0\n\n\n1\n60\n10\n0\n0\n0\n\n\n2\n0\n6\n60\n0\n0\n\n\n3\n0\n7\n60\n0\n0\n\n\n4\n0\n6\n60\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n325\n0\n10\n10\n0\n0\n\n\n326\n0\n13\n0\n0\n0\n\n\n327\n0\n11\n0\n0\n0\n\n\n328\n60\n13\n0\n0\n0\n\n\n329\n60\n15\n0\n0\n0\n\n\n\n\n330 rows × 5 columns\n\n\n\n\nsenses_df_filtered = senses_df[(senses_df == 0).sum(axis=1) != 2]\nsenses_df_filtered\n\n\n\n\n\n\n\n\ndarkvision\npassive perception\nblindsight\ntremorsense\ntruesight\n\n\n\n\n0\n120\n20\n0\n0\n0\n\n\n1\n60\n10\n0\n0\n0\n\n\n2\n0\n6\n60\n0\n0\n\n\n3\n0\n7\n60\n0\n0\n\n\n4\n0\n6\n60\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n325\n0\n10\n10\n0\n0\n\n\n326\n0\n13\n0\n0\n0\n\n\n327\n0\n11\n0\n0\n0\n\n\n328\n60\n13\n0\n0\n0\n\n\n329\n60\n15\n0\n0\n0\n\n\n\n\n279 rows × 5 columns\n\n\n\n\nmonsters_mod = pd.concat([monsters, senses_df], axis=1)\n\n\nmonsters_mod = monsters_mod[[\"size\"]+list(monsters_mod.columns[-5:])]\n\n\nimport seaborn as sns\n\n\nlong_df = pd.melt(\n    senses_df, #filtered\n    id_vars=['passive perception'], #,'size'],             \n    value_vars=['darkvision', 'blindsight', 'tremorsense', 'truesight'], \n    var_name='sense_type',\n    value_name='distance'\n)\nlong_df #[(long_df[\"sense_type\"] == \"tremorsense\") & (long_df[\"distance\"] != 0)]\n\n\n\n\n\n\n\n\npassive perception\nsense_type\ndistance\n\n\n\n\n0\n20\ndarkvision\n120\n\n\n1\n10\ndarkvision\n60\n\n\n2\n6\ndarkvision\n0\n\n\n3\n7\ndarkvision\n0\n\n\n4\n6\ndarkvision\n0\n\n\n...\n...\n...\n...\n\n\n1315\n10\ntruesight\n0\n\n\n1316\n13\ntruesight\n0\n\n\n1317\n11\ntruesight\n0\n\n\n1318\n13\ntruesight\n0\n\n\n1319\n15\ntruesight\n0\n\n\n\n\n1320 rows × 3 columns\n\n\n\n\n\n\nCases with only passive perception are not there. If only long_df is used then there are lot many extra points.\nTwo points for cases with more than two senses.\n\n\nsns.set_theme(style=\"dark\", font=\"Comic Sans MS\")\ncurrent_style = sns.axes_style()\ncolors = [\"grey\", \"orange\", \"dodgerblue\", \"salmon\"]\nplot1 = sns.catplot(data=long_df[long_df[\"distance\"] != 0], x=\"distance\", y=\"passive perception\", hue=\"sense_type\",\\\n            kind=\"strip\", dodge=True, height=4, aspect=2, size=4, native_scale=True,\\\n                   jitter=0.25, palette=colors)\nplot1._legend.remove()\nplot1.add_legend(title='', ncol=4, bbox_to_anchor=(0.5, 1.05))\nfor ind, text in enumerate(plot1._legend.texts):\n    text.set_color(colors[ind]) \nfor handle in plot1._legend.legend_handles:\n    handle.set_visible(False)\nplot1.fig.set_facecolor(current_style['axes.facecolor'])\nsns.despine(left=True, bottom=True, right=True, top=True)\nfor ax in plot1.axes.flat:\n    ax.grid(axis='y', which='major')\n\nplt.xticks(ticks=range(0,151,30))\nplt.xlabel(\"Distance (feet)\")\n#plt.savefig(\"senses.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n#plot1 = sns.catplot(data=long_df, x=\"distance\", y=\"passive perception\", hue=\"size\",\\\n#            kind=\"strip\", dodge=True, height=4, aspect=2, size=4, native_scale=True,\\\n#                   jitter=1, col=\"sense_type\", col_wrap=2)\n#plot1._legend.remove()\n#plot1.add_legend(title='', ncol=7, bbox_to_anchor=(0.5, 1.05))\n#plt.xticks(ticks=range(0,151,30))\n#plt.xlabel(\"Distance (feet)\")\n#plt.tight_layout()\n#plt.show()\n\n\nsns.dark_palette(\"xkcd:golden\", 4)\n\n\n\n\n\n\n\n\nfrom upsetplot import plot\nfrom upsetplot import UpSet\nimport textwrap\nsns.reset_defaults()\n\n\nbinary_df = senses_df &gt; 0\ncounts = binary_df.value_counts().sort_values(ascending=False)\n\n\nupset = UpSet(counts, sort_by= \"cardinality\", show_percentages=True, facecolor=\"dodgerblue\")\nupset.style_subsets(present=\"truesight\", edgecolor=\"lightgreen\", linewidth=1)\nupset.style_subsets(present=\"tremorsense\", edgecolor=\"salmon\", linewidth=1)\n\n#upset.add_catplot(value=\"progression\", kind=\"strip\", color=\"blue\")\nupset.plot()\nfor ind, ax in enumerate(plt.gcf().get_axes()):\n    if(ind == 3):\n        ax.set_yticks(range(0,151,50))\n        ax.set_facecolor(\"whitesmoke\")\n        ax.yaxis.grid(True, color=\"#D0D0D0\")\n    if(ind == 2):\n        ax.xaxis.grid(True, color=\"#D0D0D0\")\n    ax.spines['left'].set_visible(False)\n    ax.tick_params(axis='both', length=0)\n\n    for text in ax.texts:\n        if \"%\" in text.get_text():  \n            text.set_fontsize(9)\n#                text.set_fontfamily(\"Consolas\")\ntitle_text = \"Frequency of five senses and their combinations across all the characters in Dungeons and Dragons Monsters (2024)\"\nplt.suptitle(\"\\n\".join(textwrap.wrap(title_text, width=20)), x=0.075, y=0.8, ha=\"left\", fontfamily=\"Serif\")\nplt.savefig(\"senses_comb.png\", dpi=300, facecolor=\"whitesmoke\", bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "posts/Dungeons_Dragons/DD_monsters.html#upset-plot",
    "href": "posts/Dungeons_Dragons/DD_monsters.html#upset-plot",
    "title": "Dungeons and Dragons Monsters (2024)",
    "section": "",
    "text": "from upsetplot import plot\nfrom upsetplot import UpSet\nimport textwrap\nsns.reset_defaults()\n\n\nbinary_df = senses_df &gt; 0\ncounts = binary_df.value_counts().sort_values(ascending=False)\n\n\nupset = UpSet(counts, sort_by= \"cardinality\", show_percentages=True, facecolor=\"dodgerblue\")\nupset.style_subsets(present=\"truesight\", edgecolor=\"lightgreen\", linewidth=1)\nupset.style_subsets(present=\"tremorsense\", edgecolor=\"salmon\", linewidth=1)\n\n#upset.add_catplot(value=\"progression\", kind=\"strip\", color=\"blue\")\nupset.plot()\nfor ind, ax in enumerate(plt.gcf().get_axes()):\n    if(ind == 3):\n        ax.set_yticks(range(0,151,50))\n        ax.set_facecolor(\"whitesmoke\")\n        ax.yaxis.grid(True, color=\"#D0D0D0\")\n    if(ind == 2):\n        ax.xaxis.grid(True, color=\"#D0D0D0\")\n    ax.spines['left'].set_visible(False)\n    ax.tick_params(axis='both', length=0)\n\n    for text in ax.texts:\n        if \"%\" in text.get_text():  \n            text.set_fontsize(9)\n#                text.set_fontfamily(\"Consolas\")\ntitle_text = \"Frequency of five senses and their combinations across all the characters in Dungeons and Dragons Monsters (2024)\"\nplt.suptitle(\"\\n\".join(textwrap.wrap(title_text, width=20)), x=0.075, y=0.8, ha=\"left\", fontfamily=\"Serif\")\nplt.savefig(\"senses_comb.png\", dpi=300, facecolor=\"whitesmoke\", bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "posts/FIDE_ratings/FIDE_ratings.html",
    "href": "posts/FIDE_ratings/FIDE_ratings.html",
    "title": "FIDE chess player ratings",
    "section": "",
    "text": "TidyTuesday dataset of September 23, 2025\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport textwrap\nfrom plotnine import *\n\n\nfide_ratings_august = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-23/fide_ratings_august.csv')\nfide_ratings_september = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-23/fide_ratings_september.csv')\n\n\nfide_ratings_august\n\n\n\n\n\n\n\n\nid\nname\nfed\nsex\ntitle\nwtitle\notitle\nfoa\nrating\ngames\nk\nbday\n\n\n\n\n0\n53707043\nA Darshil\nIND\nM\nNaN\nNaN\nNaN\nNaN\n1412\n4\n40\n2013\n\n\n1\n53200465\nA F M Ehteshamul, Hoque (tuhin\nBAN\nM\nNaN\nNaN\nNaN\nNaN\n1797\n0\n40\n1977\n\n\n2\n5716365\nA Hamid, Harman\nMAS\nM\nNaN\nNaN\nNaN\nNaN\n1552\n0\n20\n1970\n\n\n3\n53200553\nA I Sabbir\nBAN\nM\nNaN\nNaN\nNaN\nNaN\n1607\n0\n40\n1995\n\n\n4\n5045886\nA K, Kalshyan\nIND\nM\nNaN\nNaN\nNaN\nNaN\n1747\n0\n20\n1964\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n201010\n4600410\nZysk, Robert\nGER\nM\nIM\nNaN\nNaN\nNaN\n2373\n0\n10\n1966\n\n\n201011\n1141589\nZysko, Jan\nPOL\nM\nNaN\nNaN\nNaN\nNaN\n2129\n6\n20\n1990\n\n\n201012\n80411533\nZyskowski, Jean-Luc\nFRA\nM\nNaN\nNaN\nNaN\nNaN\n1517\n6\n20\n1972\n\n\n201013\n21836060\nZyto, Karol\nPOL\nM\nNaN\nNaN\nNaN\nNaN\n1748\n0\n20\n2006\n\n\n201014\n55812104\nZyuryunina, Tatiana\nRUS\nF\nNaN\nNaN\nNaN\nNaN\n1416\n0\n40\n2013\n\n\n\n\n201015 rows × 12 columns\n\n\n\n\nfide_Aug_fed_titles = (\n    fide_ratings_august[fide_ratings_august['title'].notna()]\n    .groupby(['fed','sex'])\n    .count()\n    .loc[lambda x: x['id']&gt;100]\n    .sort_values(by='id', ascending=False)\n    .reset_index()\n)\nfide_Aug_fed_titles\n\n\n\n\n\n\n\n\nfed\nsex\nid\nname\ntitle\nwtitle\notitle\nfoa\nrating\ngames\nk\nbday\n\n\n\n\n0\nGER\nM\n902\n902\n902\n0\n29\n0\n902\n902\n902\n902\n\n\n1\nESP\nM\n646\n646\n646\n0\n45\n0\n646\n646\n646\n646\n\n\n2\nFRA\nM\n404\n404\n404\n0\n8\n0\n404\n404\n404\n404\n\n\n3\nRUS\nM\n375\n375\n375\n0\n3\n0\n375\n375\n375\n375\n\n\n4\nSRB\nM\n360\n360\n360\n0\n24\n0\n360\n360\n360\n360\n\n\n5\nIND\nM\n349\n349\n349\n0\n24\n0\n349\n349\n349\n349\n\n\n6\nUSA\nM\n344\n344\n344\n0\n14\n0\n344\n344\n344\n344\n\n\n7\nHUN\nM\n338\n338\n338\n0\n29\n0\n338\n338\n338\n338\n\n\n8\nPOL\nM\n304\n304\n304\n0\n5\n0\n304\n304\n304\n304\n\n\n9\nNED\nM\n289\n289\n289\n0\n6\n0\n289\n289\n289\n289\n\n\n10\nCZE\nM\n282\n282\n282\n0\n12\n0\n282\n282\n282\n282\n\n\n11\nCRO\nM\n196\n196\n196\n0\n17\n0\n196\n196\n196\n196\n\n\n12\nAUT\nM\n186\n186\n186\n0\n6\n0\n186\n186\n186\n186\n\n\n13\nISR\nM\n175\n175\n175\n0\n8\n0\n175\n175\n175\n175\n\n\n14\nITA\nM\n175\n175\n175\n0\n7\n0\n175\n175\n175\n175\n\n\n15\nUKR\nM\n169\n169\n169\n0\n3\n0\n169\n169\n169\n169\n\n\n16\nTUR\nM\n159\n159\n159\n0\n5\n0\n159\n159\n159\n159\n\n\n17\nSWE\nM\n155\n155\n155\n0\n7\n0\n155\n155\n155\n155\n\n\n18\nENG\nM\n152\n152\n152\n0\n6\n0\n152\n152\n152\n152\n\n\n19\nROU\nM\n150\n150\n150\n0\n8\n0\n150\n150\n150\n150\n\n\n20\nNOR\nM\n133\n133\n133\n0\n5\n0\n133\n133\n133\n133\n\n\n21\nARG\nM\n133\n133\n133\n0\n10\n0\n133\n133\n133\n133\n\n\n22\nAUS\nM\n123\n123\n123\n0\n7\n0\n123\n123\n123\n123\n\n\n23\nIND\nF\n123\n123\n123\n122\n7\n0\n123\n123\n123\n123\n\n\n24\nFID\nM\n122\n122\n122\n0\n6\n0\n122\n122\n122\n122\n\n\n25\nGRE\nM\n115\n115\n115\n0\n9\n0\n115\n115\n115\n115\n\n\n26\nDEN\nM\n115\n115\n115\n0\n3\n0\n115\n115\n115\n115\n\n\n27\nCOL\nM\n114\n114\n114\n0\n9\n0\n114\n114\n114\n114\n\n\n28\nMEX\nM\n112\n112\n112\n0\n3\n0\n112\n112\n112\n112\n\n\n29\nCUB\nM\n111\n111\n111\n0\n11\n0\n111\n111\n111\n111\n\n\n30\nSUI\nM\n108\n108\n108\n0\n7\n0\n108\n108\n108\n108\n\n\n31\nBRA\nM\n107\n107\n107\n0\n8\n0\n107\n107\n107\n107\n\n\n32\nSVK\nM\n104\n104\n104\n0\n7\n0\n104\n104\n104\n104\n\n\n33\nGER\nF\n103\n103\n103\n102\n6\n0\n103\n103\n103\n103\n\n\n\n\n\n\n\n\nfig,ax = plt.subplots(figsize=(6,8))\nsns.scatterplot(data=fide_Aug_fed_titles, x='id', y='fed', hue='sex')\n# add fed label to points\nfor i, row in fide_Aug_fed_titles.iterrows():\n    plt.text(row['id']+10, row['fed'], row['fed'], fontsize=12, ha='left', va='center')\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\nfide_Aug_fed_titles['symbol'] = fide_Aug_fed_titles['sex'].apply(\n    lambda s: '♔' if s == 'M' else '♕'\n)\nfide_Aug_fed_titles\n\n\n\n\n\n\n\n\nfed\nsex\nid\nname\ntitle\nwtitle\notitle\nfoa\nrating\ngames\nk\nbday\nsymbol\n\n\n\n\n0\nGER\nM\n902\n902\n902\n0\n29\n0\n902\n902\n902\n902\n♔\n\n\n1\nESP\nM\n646\n646\n646\n0\n45\n0\n646\n646\n646\n646\n♔\n\n\n2\nFRA\nM\n404\n404\n404\n0\n8\n0\n404\n404\n404\n404\n♔\n\n\n3\nRUS\nM\n375\n375\n375\n0\n3\n0\n375\n375\n375\n375\n♔\n\n\n4\nSRB\nM\n360\n360\n360\n0\n24\n0\n360\n360\n360\n360\n♔\n\n\n5\nIND\nM\n349\n349\n349\n0\n24\n0\n349\n349\n349\n349\n♔\n\n\n6\nUSA\nM\n344\n344\n344\n0\n14\n0\n344\n344\n344\n344\n♔\n\n\n7\nHUN\nM\n338\n338\n338\n0\n29\n0\n338\n338\n338\n338\n♔\n\n\n8\nPOL\nM\n304\n304\n304\n0\n5\n0\n304\n304\n304\n304\n♔\n\n\n9\nNED\nM\n289\n289\n289\n0\n6\n0\n289\n289\n289\n289\n♔\n\n\n10\nCZE\nM\n282\n282\n282\n0\n12\n0\n282\n282\n282\n282\n♔\n\n\n11\nCRO\nM\n196\n196\n196\n0\n17\n0\n196\n196\n196\n196\n♔\n\n\n12\nAUT\nM\n186\n186\n186\n0\n6\n0\n186\n186\n186\n186\n♔\n\n\n13\nISR\nM\n175\n175\n175\n0\n8\n0\n175\n175\n175\n175\n♔\n\n\n14\nITA\nM\n175\n175\n175\n0\n7\n0\n175\n175\n175\n175\n♔\n\n\n15\nUKR\nM\n169\n169\n169\n0\n3\n0\n169\n169\n169\n169\n♔\n\n\n16\nTUR\nM\n159\n159\n159\n0\n5\n0\n159\n159\n159\n159\n♔\n\n\n17\nSWE\nM\n155\n155\n155\n0\n7\n0\n155\n155\n155\n155\n♔\n\n\n18\nENG\nM\n152\n152\n152\n0\n6\n0\n152\n152\n152\n152\n♔\n\n\n19\nROU\nM\n150\n150\n150\n0\n8\n0\n150\n150\n150\n150\n♔\n\n\n20\nNOR\nM\n133\n133\n133\n0\n5\n0\n133\n133\n133\n133\n♔\n\n\n21\nARG\nM\n133\n133\n133\n0\n10\n0\n133\n133\n133\n133\n♔\n\n\n22\nAUS\nM\n123\n123\n123\n0\n7\n0\n123\n123\n123\n123\n♔\n\n\n23\nIND\nF\n123\n123\n123\n122\n7\n0\n123\n123\n123\n123\n♕\n\n\n24\nFID\nM\n122\n122\n122\n0\n6\n0\n122\n122\n122\n122\n♔\n\n\n25\nGRE\nM\n115\n115\n115\n0\n9\n0\n115\n115\n115\n115\n♔\n\n\n26\nDEN\nM\n115\n115\n115\n0\n3\n0\n115\n115\n115\n115\n♔\n\n\n27\nCOL\nM\n114\n114\n114\n0\n9\n0\n114\n114\n114\n114\n♔\n\n\n28\nMEX\nM\n112\n112\n112\n0\n3\n0\n112\n112\n112\n112\n♔\n\n\n29\nCUB\nM\n111\n111\n111\n0\n11\n0\n111\n111\n111\n111\n♔\n\n\n30\nSUI\nM\n108\n108\n108\n0\n7\n0\n108\n108\n108\n108\n♔\n\n\n31\nBRA\nM\n107\n107\n107\n0\n8\n0\n107\n107\n107\n107\n♔\n\n\n32\nSVK\nM\n104\n104\n104\n0\n7\n0\n104\n104\n104\n104\n♔\n\n\n33\nGER\nF\n103\n103\n103\n102\n6\n0\n103\n103\n103\n103\n♕\n\n\n\n\n\n\n\n\nunique_fed_order = fide_Aug_fed_titles['fed'].drop_duplicates()[::-1].tolist()\nfide_Aug_fed_titles['fed'] = pd.Categorical(\n    fide_Aug_fed_titles['fed'],\n    categories=unique_fed_order,\n    ordered=True\n)\nfide_Aug_fed_titles['fed_label'] = fide_Aug_fed_titles.apply(\n    lambda row: f\"{row['fed']} ({row['id']})\", axis=1\n)\nfed_list = ['IND', 'GER', 'ESP', 'FRA', 'CRO', 'SVK']\n#label_df = fide_Aug_fed_titles[fide_Aug_fed_titles['fed'].isin(fed_list)].copy()\nlabel_df = fide_Aug_fed_titles.copy()\nlabel_df['label'] = label_df.apply(lambda row: f'({row.id})', axis=1)\nwrapped_title = textwrap.fill(\"Number of male (♔) and female (♕) chess players with titles in August 2025. Countries with more than 100 titled players are shown.\", \\\n                    width=35)\n\n(\n    ggplot(fide_Aug_fed_titles, aes(x='id', y='fed')) +\n#    geom_point(size=2) +\n    geom_text(aes(label='symbol'), size=15, va='center', color='#FFFFFF', show_legend=False) + \n    geom_text(aes(label='fed', color='sex'), nudge_x=20, size=10, ha='left', va='center',\\\n     show_legend=False, family='monospace') +\n    geom_text(data=label_df, mapping=aes(label='label', color='sex'), nudge_x=65, \\\n    size=9, va='center', ha='left', family='monospace', show_legend=False) +\n    annotate('text', x=325, y=10, label=wrapped_title, color='#EEEEEE', \\\n    size=14, ha='left', va='center') +\n    theme_minimal() +\n    scale_color_manual(values={'M':'skyblue', 'F':'pink'}) +\n#    labs(x='Number of players with titles') +\n    xlim(0, 985) +\n    theme(figure_size=(6, 6),\n        axis_title_y=element_blank(),\n        axis_text_y=element_blank(),\n        axis_ticks_major_y=element_blank(),\n        axis_ticks_minor_y=element_blank(),\n        panel_grid_major_y=element_blank(),\n        axis_title_x=element_blank(),\n        axis_text_x=element_blank(),\n        axis_ticks_major_x=element_blank(),\n        axis_ticks_minor_x=element_blank(),\n        panel_grid_major_x=element_blank(),\n        panel_grid_minor_x=element_blank(),\n        plot_background=element_rect(fill='#2E2E2E'),\n        panel_background=element_rect(fill='#2E2E2E')\n    )\n)#.save('chess_ratings.png', width=6, height=6, dpi=300)\n\n\ndf_sept_mod = (\n    fide_ratings_september[(fide_ratings_september['rating']&gt;2000) & \n    (fide_ratings_september['games']&gt;0)]\n    .groupby(\"fed\")\n    .obj\n#    .loc[lambda x: x['id']&gt;100]\n#    .loc[lambda x: x['fed']==\"IND\"]\n#    .sort_values(by='id', ascending=False)\n    # change bday to datetime\n    .assign(bday=lambda x: pd.to_datetime(x['bday'], format='%Y'))\n    .assign(age=lambda x: 2025 - x['bday'].dt.year)\n)\n\ndf_sept_mod\n\n\n\n\n\n\n\n\nid\nname\nfed\nsex\ntitle\nwtitle\notitle\nfoa\nrating\ngames\nk\nbday\nage\n\n\n\n\n57\n33362041\nAadik Theophane Lenin\nIND\nM\nNaN\nNaN\nNaN\nAIM\n2108\n9\n40\n2012-01-01\n13\n\n\n77\n25678191\nAaditya Dhingra\nIND\nM\nIM\nNaN\nNaN\nNaN\n2391\n17\n10\n2006-01-01\n19\n\n\n127\n25644394\nAakash G\nIND\nM\nFM\nNaN\nNaN\nNaN\n2349\n17\n20\n2010-01-01\n15\n\n\n131\n25033220\nAakash Sharadchandra, Dalvi\nIND\nM\nIM\nNaN\nNaN\nNaN\n2420\n18\n10\n2002-01-01\n23\n\n\n137\n510726\nAalto, Patrik\nFIN\nM\nFM\nNaN\nNaN\nNaN\n2296\n9\n20\n2004-01-01\n21\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n203110\n393770\nZvolensky, David\nCZE\nM\nNaN\nNaN\nNaN\nNaN\n2157\n18\n20\n2002-01-01\n23\n\n\n203124\n325511\nZwardon, Vojtech\nCZE\nM\nIM\nNaN\nNaN\nNaN\n2431\n10\n10\n1990-01-01\n35\n\n\n203127\n1332252\nZweifel, Richard\nSUI\nM\nCM\nNaN\nNaN\nNaN\n2196\n1\n20\n1973-01-01\n52\n\n\n203145\n1036432\nZwirs, Nico\nNED\nM\nIM\nNaN\nNaN\nNaN\n2432\n9\n10\n1994-01-01\n31\n\n\n203163\n4200292\nZygouris, Hristos\nGRE\nM\nNaN\nNaN\nNaN\nNaN\n2194\n9\n20\n1974-01-01\n51\n\n\n\n\n7581 rows × 13 columns\n\n\n\n\nsns.histplot(data=df_sept_mod, x='age', bins=20, hue='sex')"
  },
  {
    "objectID": "posts/Frogs_AUS/Frogs_AUS.html",
    "href": "posts/Frogs_AUS/Frogs_AUS.html",
    "title": "Frog sightings in Australia",
    "section": "",
    "text": "TidyTuesday dataset of September 2, 2025\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrogID_data = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-02/frogID_data.csv')\nfrog_names = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-02/frog_names.csv')\n\n\nfrogID_data\n\n\n\n\n\n\n\n\noccurrenceID\neventID\ndecimalLatitude\ndecimalLongitude\nscientificName\neventDate\neventTime\ntimezone\ncoordinateUncertaintyInMeters\nrecordedBy\nstateProvince\n\n\n\n\n0\n12832\n525618\n-28.500000\n153.100000\nPhiloria loveridgei\n2023-01-01\n11:18:32\nGMT+1100\n10000.000000\n41480\nNew South Wales\n\n\n1\n12833\n526341\n-33.700000\n151.200000\nHeleioporus australiacus\n2023-01-02\n20:39:30\nGMT+1100\n10000.000000\n834983\nNew South Wales\n\n\n2\n12834\n526673\n-28.700000\n152.700000\nMixophyes iteratus\n2023-01-02\n21:30:07\nGMT+1100\n10000.000000\n804177\nNew South Wales\n\n\n3\n12835\n526673\n-28.700000\n152.700000\nMixophyes fasciolatus\n2023-01-02\n21:30:07\nGMT+1100\n10000.000000\n804177\nNew South Wales\n\n\n4\n12836\n526673\n-28.700000\n152.700000\nLitoria latopalmata\n2023-01-02\n21:30:07\nGMT+1100\n10000.000000\n804177\nNew South Wales\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n136616\n974110\n619213\n-35.126800\n150.753000\nCrinia signifera\n2023-11-09\n17:29:47\nGMT+1100\n8.000000\n825385\nOther Territories\n\n\n136617\n974111\n658692\n-35.152206\n150.759737\nCrinia signifera\n2023-03-21\n11:59:24\nGMT+1100\n4.741598\n12486\nOther Territories\n\n\n136618\n974112\n691758\n-35.125944\n150.755366\nLimnodynastes peronii\n2023-08-25\n11:36:58\nGMT+1000\n11.824237\n826303\nOther Territories\n\n\n136619\n974118\n802152\n-35.126700\n150.755000\nParacrinia haswelli\n2023-06-06\n11:16:39\nGMT+1000\n4.357000\n35183\nOther Territories\n\n\n136620\n974119\n802152\n-35.126700\n150.755000\nCrinia signifera\n2023-06-06\n11:16:39\nGMT+1000\n4.357000\n35183\nOther Territories\n\n\n\n\n136621 rows × 11 columns\n\n\n\n\nfrog_names\n\n\n\n\n\n\n\n\nsubfamily\ntribe\nscientificName\ncommonName\nsecondary_commonNames\n\n\n\n\n0\nHylid\nPelodryadidae\nCyclorana\n—\n—\n\n\n1\nHylid\nPelodryadidae\nCyclorana alboguttata\nStriped Burrowing Frog\nGreen-striped Frog\n\n\n2\nHylid\nPelodryadidae\nCyclorana australis\nNorthern Snapping Frog\nGiant Frog\n\n\n3\nHylid\nPelodryadidae\nCyclorana brevipes\nSuperb Collared Frog\nShort-footed Frog\n\n\n4\nHylid\nPelodryadidae\nCyclorana cryptotis\nHidden-ear Frog\nEarless Frog\n\n\n...\n...\n...\n...\n...\n...\n\n\n289\nMyobatrachid\nMyobatrachidae\nUperoleia tyleri\nTyler’s Toadlet\n—\n\n\n290\nRanid\nRanidae\nPapurana\nNo widely used common name for the genus\n—\n\n\n291\nRanid\nRanidae\nPapurana daemeli\nWood Frog\nAustralian Wood Frog\n\n\n292\nToad\nBufonidae\nRhinella\nSouth American Toads\n—\n\n\n293\nToad\nBufonidae\nRhinella marina\nCane Toad\n—\n\n\n\n\n294 rows × 5 columns\n\n\n\n\nfrogID_data['genus'] = frogID_data['scientificName'].str.split(' ').str[0]\nfrogID_data['species'] = frogID_data['scientificName'].str.split(' ').str[1]\nfrogID_data\n\n\n\n\n\n\n\n\noccurrenceID\neventID\ndecimalLatitude\ndecimalLongitude\nscientificName\neventDate\neventTime\ntimezone\ncoordinateUncertaintyInMeters\nrecordedBy\nstateProvince\ngenus\nspecies\n\n\n\n\n0\n12832\n525618\n-28.500000\n153.100000\nPhiloria loveridgei\n2023-01-01\n11:18:32\nGMT+1100\n10000.000000\n41480\nNew South Wales\nPhiloria\nloveridgei\n\n\n1\n12833\n526341\n-33.700000\n151.200000\nHeleioporus australiacus\n2023-01-02\n20:39:30\nGMT+1100\n10000.000000\n834983\nNew South Wales\nHeleioporus\naustraliacus\n\n\n2\n12834\n526673\n-28.700000\n152.700000\nMixophyes iteratus\n2023-01-02\n21:30:07\nGMT+1100\n10000.000000\n804177\nNew South Wales\nMixophyes\niteratus\n\n\n3\n12835\n526673\n-28.700000\n152.700000\nMixophyes fasciolatus\n2023-01-02\n21:30:07\nGMT+1100\n10000.000000\n804177\nNew South Wales\nMixophyes\nfasciolatus\n\n\n4\n12836\n526673\n-28.700000\n152.700000\nLitoria latopalmata\n2023-01-02\n21:30:07\nGMT+1100\n10000.000000\n804177\nNew South Wales\nLitoria\nlatopalmata\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n136616\n974110\n619213\n-35.126800\n150.753000\nCrinia signifera\n2023-11-09\n17:29:47\nGMT+1100\n8.000000\n825385\nOther Territories\nCrinia\nsignifera\n\n\n136617\n974111\n658692\n-35.152206\n150.759737\nCrinia signifera\n2023-03-21\n11:59:24\nGMT+1100\n4.741598\n12486\nOther Territories\nCrinia\nsignifera\n\n\n136618\n974112\n691758\n-35.125944\n150.755366\nLimnodynastes peronii\n2023-08-25\n11:36:58\nGMT+1000\n11.824237\n826303\nOther Territories\nLimnodynastes\nperonii\n\n\n136619\n974118\n802152\n-35.126700\n150.755000\nParacrinia haswelli\n2023-06-06\n11:16:39\nGMT+1000\n4.357000\n35183\nOther Territories\nParacrinia\nhaswelli\n\n\n136620\n974119\n802152\n-35.126700\n150.755000\nCrinia signifera\n2023-06-06\n11:16:39\nGMT+1000\n4.357000\n35183\nOther Territories\nCrinia\nsignifera\n\n\n\n\n136621 rows × 13 columns\n\n\n\n\nfrogID_data.groupby('genus').nunique()['species'].sort_values(ascending=False)\n\ngenus\nLitoria            64\nUperoleia          21\nCrinia             14\nCyclorana          11\nPseudophryne       10\nLimnodynastes      10\nMixophyes           8\nCophixalus          7\nNeobatrachus        6\nHeleioporus         6\nAustrochaperina     5\nGeocrinia           4\nPhiloria            4\nPlatyplectrum       2\nNotaden             2\nAnstisia            2\nMyobatrachus        1\nPapurana            1\nParacrinia          1\nMetacrinia          1\nLechriodus          1\nAssa                1\nRhinella            1\nSpicospina          1\nTaudactylus         1\nAdelotus            1\nName: species, dtype: int64\n\n\n\nfrogID_data['eventDate'] = pd.to_datetime(frogID_data['eventDate'])\ndf_grp = frogID_data.groupby('eventDate').nunique().reset_index()\ndf_grp\n\n\n\n\n\n\n\n\neventDate\noccurrenceID\neventID\ndecimalLatitude\ndecimalLongitude\nscientificName\neventTime\ntimezone\ncoordinateUncertaintyInMeters\nrecordedBy\nstateProvince\ngenus\nspecies\n\n\n\n\n0\n2023-01-01\n1042\n574\n518\n476\n66\n571\n5\n413\n349\n8\n17\n62\n\n\n1\n2023-01-02\n722\n398\n372\n347\n62\n398\n5\n292\n257\n8\n15\n58\n\n\n2\n2023-01-03\n438\n268\n251\n234\n51\n267\n5\n219\n194\n8\n12\n49\n\n\n3\n2023-01-04\n604\n397\n367\n351\n59\n392\n5\n310\n281\n9\n13\n56\n\n\n4\n2023-01-05\n941\n485\n419\n394\n66\n483\n5\n369\n300\n8\n16\n62\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n308\n2023-11-05\n3405\n1981\n1761\n1588\n65\n1946\n5\n1403\n1089\n9\n15\n61\n\n\n309\n2023-11-06\n4253\n2155\n1960\n1760\n66\n2103\n5\n1517\n1193\n9\n15\n60\n\n\n310\n2023-11-07\n3601\n1864\n1721\n1561\n64\n1828\n5\n1334\n1077\n8\n14\n61\n\n\n311\n2023-11-08\n3540\n1755\n1622\n1476\n72\n1718\n5\n1292\n1003\n8\n14\n68\n\n\n312\n2023-11-09\n3200\n1512\n1398\n1294\n64\n1480\n5\n1116\n844\n9\n15\n59\n\n\n\n\n313 rows × 13 columns\n\n\n\n\naus_seasons = {'Summer': ('2023-01-01', '2023-02-28'),\n               'Autumn': ('2023-03-01', '2023-05-31'),\n               'Winter': ('2023-06-01', '2023-08-31'),\n               'Spring': ('2023-09-01', '2023-11-30')}\nfig, ax = plt.subplots(figsize=(8, 4))\nplt.rcParams['figure.facecolor'] = '#FFCA99'  # Dark grey figure background\nplt.rcParams['axes.facecolor'] = '#FFCA99'  \nplt.rcParams['axes.edgecolor'] = '#D69456'\nplt.rcParams['xtick.color'] = '#713600'\nplt.rcParams['ytick.color'] = '#713600'\nplt.rcParams['text.color'] = '#713600'\nplt.rcParams['axes.labelcolor'] = '#713600'\nplt.rcParams['axes.titlecolor'] = '#713600'\n\n\nfor season, (start, end) in aus_seasons.items():\n    if (season == 'Autumn' or season == 'Spring'):\n        ax.axvspan(pd.to_datetime(start), pd.to_datetime(end), color='#FFE5B4', alpha=0.3, lw=0)\n    ax.text((pd.to_datetime(end) - pd.to_datetime(start)) / 2 + pd.to_datetime(start), 10, season, fontsize=10, ha='center', va='bottom', color='#713600')\n\nsns.scatterplot(data=df_grp, x='eventDate', y='scientificName',alpha=0.75, hue='eventID', \\\npalette='Greens', edgecolor='none', size='eventID', sizes=(10,100), ax=ax)\nsns.despine()\n#handles, labels = ax.get_legend_handles_labels()\n#ax.legend(handles[::-1], labels[::-1], frameon=False, loc='upper center', title='Sighting Events', title_fontsize='medium')\nax.legend(frameon=False, loc='upper center', title='Sighting Events', title_fontsize='medium')\nplt.xlabel('')\nplt.ylabel('Number of Species')\nplt.title('Timeline of frog sightings in Australia in 2023. \\nMost species breed during the rainy season, leading to a \\npeak in sightings.', fontsize=14, fontfamily=\"Serif\", \\\nha='left', x=0, y=1)\n\ndates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='MS')\nlabels = ['Jan', '', 'Mar', '', '', 'Jun', '', '', 'Sep', '', '', 'Dec']\n# Filter out blank labels\nfiltered_dates = [date for date, label in zip(dates, labels) if label != '']\nfiltered_labels = [label for label in labels if label != '']\nplt.xticks(filtered_dates, filtered_labels)\n\n#plt.xticks(pd.date_range(start='2023-01-01', end='2023-12-31', freq='MS'), ['Jan', '', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\nplt.savefig(\"Frogs_AUS.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\n# count commonName values having Tree in it.\nprint(frog_names['commonName'].str.contains('Tree').sum())\nprint(frog_names['commonName'].str.contains('Green').sum())\n\n46\n6\n\n\n\nfrom wordcloud import WordCloud\nall_cnames = ' '.join(frog_names['commonName'])\n# remove frog from all_cnames\nall_cnames = all_cnames.replace('Frog', '')\nwordcloud = WordCloud(width = 800, height = 800, background_color ='white').generate(all_cnames)\n\nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)"
  },
  {
    "objectID": "posts/Gutenberg_book/Gutenberg_books.html",
    "href": "posts/Gutenberg_book/Gutenberg_books.html",
    "title": "Project Gutenberg",
    "section": "",
    "text": "library(tidyverse)\nlibrary(wordcloud)\nlibrary(RColorBrewer)\nlibrary(ISOcodes)\nlibrary(scales)\nlibrary(magick)\n\n\ngutenberg_authors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_authors.csv')\ngutenberg_languages &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_languages.csv')\ngutenberg_metadata &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_metadata.csv')\ngutenberg_subjects &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_subjects.csv')\n\nglimpse(gutenberg_authors)\n\nRows: 26,077\nColumns: 7\n$ gutenberg_author_id &lt;dbl&gt; 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ author              &lt;chr&gt; \"United States\", \"Lincoln, Abraham\", \"Henry, Patri…\n$ alias               &lt;chr&gt; \"U.S.A.\", NA, NA, NA, \"Dodgson, Charles Lutwidge\",…\n$ birthdate           &lt;dbl&gt; NA, 1809, 1736, 1849, 1832, NA, 1819, 1860, NA, 18…\n$ deathdate           &lt;dbl&gt; NA, 1865, 1799, 1931, 1898, NA, 1891, 1937, NA, 18…\n$ wikipedia           &lt;chr&gt; \"https://en.wikipedia.org/wiki/United_States\", \"ht…\n$ aliases             &lt;chr&gt; \"U.S.A.\", \"United States President (1861-1865)/Lin…\n\nglimpse(gutenberg_languages)\n\nRows: 76,205\nColumns: 3\n$ gutenberg_id    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ language        &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ total_languages &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n\nglimpse(gutenberg_metadata)\n\nRows: 79,491\nColumns: 8\n$ gutenberg_id        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, 12, 13, 14,…\n$ title               &lt;chr&gt; \"The Declaration of Independence of the United Sta…\n$ author              &lt;chr&gt; \"Jefferson, Thomas\", \"United States\", \"Kennedy, Jo…\n$ gutenberg_author_id &lt;dbl&gt; 1638, 1, 1666, 3, 1, 4, NA, 3, 3, NA, 7, 7, 7, 7, …\n$ language            &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"e…\n$ gutenberg_bookshelf &lt;chr&gt; \"Politics/American Revolutionary War/United States…\n$ rights              &lt;chr&gt; \"Public domain in the USA.\", \"Public domain in the…\n$ has_text            &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n\nglimpse(gutenberg_subjects)\n\nRows: 255,312\nColumns: 3\n$ gutenberg_id &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, …\n$ subject_type &lt;chr&gt; \"lcsh\", \"lcsh\", \"lcc\", \"lcc\", \"lcsh\", \"lcsh\", \"lcc\", \"lcc…\n$ subject      &lt;chr&gt; \"United States -- History -- Revolution, 1775-1783 -- Sou…\n\n\n\nlang_counts &lt;- gutenberg_languages %&gt;% \n  group_by(language) %&gt;%\n  summarise(total_languages = sum(total_languages), .groups = 'drop') %&gt;%\n  arrange(desc(total_languages)) %&gt;% \n  left_join(ISO_639_2, by = c(\"language\" = \"Alpha_2\")) %&gt;% \n  select(language, Name, total_languages)\nglimpse(lang_counts)\n\nRows: 70\nColumns: 3\n$ language        &lt;chr&gt; \"en\", \"fr\", \"fi\", \"de\", \"it\", \"nl\", \"es\", \"pt\", \"hu\", …\n$ Name            &lt;chr&gt; \"English\", \"French\", \"Finnish\", \"German\", \"Italian\", \"…\n$ total_languages &lt;dbl&gt; 60875, 4019, 3314, 2363, 1061, 1053, 917, 651, 610, 45…\n\n\n\npng(\"word_cloud.png\", width = 4, height = 4, units = \"in\", res = 300, bg = \"black\")\np1 &lt;- wordcloud(words = lang_counts$Name, freq = lang_counts$total_languages,\n          min.freq = 0,\n          random.order = FALSE,\n          colors = brewer.pal(9, \"Pastel1\"))\ndev.off()\n\npng \n  2 \n\n\n\nlang_counts %&gt;% \n  filter(language != \"en\") %&gt;% \n  arrange(desc(total_languages)) %&gt;% \n  slice(1:10) %&gt;% \n  ggplot(aes(x = total_languages, y =reorder(Name, total_languages))) +\n  geom_col(alpha=0) +\n  geom_text(aes(label = comma(total_languages), x=70), color = \"grey\", hjust=1) +\n  theme_minimal() +\n  scale_y_discrete(labels = function(x) paste0(x, \":\"))+\n  coord_cartesian(xlim = c(0, 100)) +\n  theme(axis.title = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.text.y = element_text(color = \"grey\"),\n        axis.line = element_blank(),\n        panel.grid = element_blank(),\n        plot.margin = margin(75, 0, 75, 0))\n\n\n\n\n\n\n\nggsave(\"plot1.png\", width = 1.75, height = 4, units = \"in\", bg=\"black\")\n\n\nimg1 &lt;- image_read(\"word_cloud.png\")\nimg2 &lt;- image_read(\"plot1.png\")\ncombined &lt;- image_append(c(img1, img2), stack = FALSE)\n#image_write(combined, \"combined.png\")\n\ntitle_text &lt;- paste(\"In Project Gutenberg, \", comma(lang_counts$total_languages[1]), \" out of \", comma(dim(gutenberg_languages)[1]), \"books are in English. The word cloud is based on the book counts in different languages, excluding English. Counts for top ten languages are shown on the right.\")\ntitle_text &lt;- paste(strwrap(title_text, width = 75), collapse = \"\\n\")\ntitle_image &lt;- image_blank(width = image_info(combined)$width,\n                           height = 160, # adjust height as needed\n                           color = \"black\") %&gt;%\n  image_annotate(text = title_text,\n                 size = 40, # adjust font size\n                 color = \"grey\",\n                 gravity = \"center\")\n\nfinal_image &lt;- image_composite(combined, title_image, offset = \"+0+0\")\nimage_write(final_image, \"combined_with_title.png\")"
  },
  {
    "objectID": "posts/Gutenberg_book/Gutenberg_books.html#tidytuesday-data-for-2025-06-03",
    "href": "posts/Gutenberg_book/Gutenberg_books.html#tidytuesday-data-for-2025-06-03",
    "title": "Project Gutenberg",
    "section": "",
    "text": "library(tidyverse)\nlibrary(wordcloud)\nlibrary(RColorBrewer)\nlibrary(ISOcodes)\nlibrary(scales)\nlibrary(magick)\n\n\ngutenberg_authors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_authors.csv')\ngutenberg_languages &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_languages.csv')\ngutenberg_metadata &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_metadata.csv')\ngutenberg_subjects &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_subjects.csv')\n\nglimpse(gutenberg_authors)\n\nRows: 26,077\nColumns: 7\n$ gutenberg_author_id &lt;dbl&gt; 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ author              &lt;chr&gt; \"United States\", \"Lincoln, Abraham\", \"Henry, Patri…\n$ alias               &lt;chr&gt; \"U.S.A.\", NA, NA, NA, \"Dodgson, Charles Lutwidge\",…\n$ birthdate           &lt;dbl&gt; NA, 1809, 1736, 1849, 1832, NA, 1819, 1860, NA, 18…\n$ deathdate           &lt;dbl&gt; NA, 1865, 1799, 1931, 1898, NA, 1891, 1937, NA, 18…\n$ wikipedia           &lt;chr&gt; \"https://en.wikipedia.org/wiki/United_States\", \"ht…\n$ aliases             &lt;chr&gt; \"U.S.A.\", \"United States President (1861-1865)/Lin…\n\nglimpse(gutenberg_languages)\n\nRows: 76,205\nColumns: 3\n$ gutenberg_id    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ language        &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ total_languages &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n\nglimpse(gutenberg_metadata)\n\nRows: 79,491\nColumns: 8\n$ gutenberg_id        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, 12, 13, 14,…\n$ title               &lt;chr&gt; \"The Declaration of Independence of the United Sta…\n$ author              &lt;chr&gt; \"Jefferson, Thomas\", \"United States\", \"Kennedy, Jo…\n$ gutenberg_author_id &lt;dbl&gt; 1638, 1, 1666, 3, 1, 4, NA, 3, 3, NA, 7, 7, 7, 7, …\n$ language            &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"e…\n$ gutenberg_bookshelf &lt;chr&gt; \"Politics/American Revolutionary War/United States…\n$ rights              &lt;chr&gt; \"Public domain in the USA.\", \"Public domain in the…\n$ has_text            &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n\nglimpse(gutenberg_subjects)\n\nRows: 255,312\nColumns: 3\n$ gutenberg_id &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, …\n$ subject_type &lt;chr&gt; \"lcsh\", \"lcsh\", \"lcc\", \"lcc\", \"lcsh\", \"lcsh\", \"lcc\", \"lcc…\n$ subject      &lt;chr&gt; \"United States -- History -- Revolution, 1775-1783 -- Sou…\n\n\n\nlang_counts &lt;- gutenberg_languages %&gt;% \n  group_by(language) %&gt;%\n  summarise(total_languages = sum(total_languages), .groups = 'drop') %&gt;%\n  arrange(desc(total_languages)) %&gt;% \n  left_join(ISO_639_2, by = c(\"language\" = \"Alpha_2\")) %&gt;% \n  select(language, Name, total_languages)\nglimpse(lang_counts)\n\nRows: 70\nColumns: 3\n$ language        &lt;chr&gt; \"en\", \"fr\", \"fi\", \"de\", \"it\", \"nl\", \"es\", \"pt\", \"hu\", …\n$ Name            &lt;chr&gt; \"English\", \"French\", \"Finnish\", \"German\", \"Italian\", \"…\n$ total_languages &lt;dbl&gt; 60875, 4019, 3314, 2363, 1061, 1053, 917, 651, 610, 45…\n\n\n\npng(\"word_cloud.png\", width = 4, height = 4, units = \"in\", res = 300, bg = \"black\")\np1 &lt;- wordcloud(words = lang_counts$Name, freq = lang_counts$total_languages,\n          min.freq = 0,\n          random.order = FALSE,\n          colors = brewer.pal(9, \"Pastel1\"))\ndev.off()\n\npng \n  2 \n\n\n\nlang_counts %&gt;% \n  filter(language != \"en\") %&gt;% \n  arrange(desc(total_languages)) %&gt;% \n  slice(1:10) %&gt;% \n  ggplot(aes(x = total_languages, y =reorder(Name, total_languages))) +\n  geom_col(alpha=0) +\n  geom_text(aes(label = comma(total_languages), x=70), color = \"grey\", hjust=1) +\n  theme_minimal() +\n  scale_y_discrete(labels = function(x) paste0(x, \":\"))+\n  coord_cartesian(xlim = c(0, 100)) +\n  theme(axis.title = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.text.y = element_text(color = \"grey\"),\n        axis.line = element_blank(),\n        panel.grid = element_blank(),\n        plot.margin = margin(75, 0, 75, 0))\n\n\n\n\n\n\n\nggsave(\"plot1.png\", width = 1.75, height = 4, units = \"in\", bg=\"black\")\n\n\nimg1 &lt;- image_read(\"word_cloud.png\")\nimg2 &lt;- image_read(\"plot1.png\")\ncombined &lt;- image_append(c(img1, img2), stack = FALSE)\n#image_write(combined, \"combined.png\")\n\ntitle_text &lt;- paste(\"In Project Gutenberg, \", comma(lang_counts$total_languages[1]), \" out of \", comma(dim(gutenberg_languages)[1]), \"books are in English. The word cloud is based on the book counts in different languages, excluding English. Counts for top ten languages are shown on the right.\")\ntitle_text &lt;- paste(strwrap(title_text, width = 75), collapse = \"\\n\")\ntitle_image &lt;- image_blank(width = image_info(combined)$width,\n                           height = 160, # adjust height as needed\n                           color = \"black\") %&gt;%\n  image_annotate(text = title_text,\n                 size = 40, # adjust font size\n                 color = \"grey\",\n                 gravity = \"center\")\n\nfinal_image &lt;- image_composite(combined, title_image, offset = \"+0+0\")\nimage_write(final_image, \"combined_with_title.png\")"
  },
  {
    "objectID": "posts/Judges_appoint/Judges_appoint.html",
    "href": "posts/Judges_appoint/Judges_appoint.html",
    "title": "The US Judges data",
    "section": "",
    "text": "import pandas as pd\n\n\njudges_appointments = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-10/judges_appointments.csv')\njudges_people = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-10/judges_people.csv')\n\n\njudges_appointments\n\n\n\n\n\n\n\n\njudge_id\ncourt_name\ncourt_type\npresident_name\npresident_party\nnomination_date\npredecessor_last_name\npredecessor_first_name\nsenate_confirmation_date\ncommission_date\nchief_judge_begin\nchief_judge_end\nretirement_from_active_service\ntermination_date\ntermination_reason\n\n\n\n\n0\n3419\nU. S. District Court, Southern District of New...\nUSDC\nBarack Obama\nDemocratic\n07/28/2011\nKaplan\nLewis A.\n03/22/2012\n03/23/2012\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n1\nU. S. District Court, Eastern District of New ...\nUSDC\nFranklin D. Roosevelt\nDemocratic\n02/03/1936\nnew\nNaN\n02/12/1936\n02/15/1936\nNaN\nNaN\n02/15/1966\n05/28/1971\nDeath\n\n\n2\n2\nU. S. District Court, Western District of Penn...\nUSDC\nRutherford B. Hayes\nRepublican\n01/06/1880\nKetcham\nWinthrop\n01/14/1880\n01/14/1880\nNaN\nNaN\nNaN\n02/09/1891\nAppointment to Another Judicial Position\n\n\n3\n3\nU. S. District Court, Northern District of Ala...\nUSDC\nRonald Reagan\nRepublican\n07/22/1982\nMcFadden\nFrank H.\n08/18/1982\n08/18/1982\nNaN\nNaN\n05/31/1996\nNaN\nNaN\n\n\n4\n4\nU. S. District Court, District of New Jersey\nUSDC\nJimmy Carter\nDemocratic\n09/28/1979\nBarlow\nGeorge H.\n10/31/1979\n11/02/1979\nNaN\nNaN\n02/15/1994\n12/02/2009\nDeath\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4197\n2201\nU. S. District Courts, Albemarle, Cape Fear & ...\nUSDC\nReassignment\nReassignment\nNaN\nnew\nNaN\nNaN\n02/13/1801\nNaN\nNaN\nNaN\n03/04/1802\nDeath\n\n\n4198\n2689\nU. S. District Court, Eastern District of Miss...\nUSDC\nHarry S Truman\nDemocratic\n01/13/1949\nHarper\nRoy Winfield\n01/31/1949\n02/02/1949\nNaN\nNaN\n01/05/1971\n02/13/1994\nDeath\n\n\n4199\n1126\nU. S. Court of Appeals for the Ninth Circuit\nUSCA\nWilliam H. Taft\nRepublican\n12/12/1910\nnew\nNaN\n01/31/1911\n02/08/1911\nNaN\nNaN\n01/31/1928\n11/30/1928\nRetirement\n\n\n4200\n1453\nU. S. Court of Appeals for the Second Circuit\nUSCA\nReassignment\nReassignment\nNaN\nnew\nNaN\nNaN\n07/01/1929\nNaN\nNaN\n09/06/1940\n09/05/1943\nDeath\n\n\n4201\n2689\nU. S. District Court, Western District of Miss...\nUSDC\nHarry S Truman\nDemocratic\n01/13/1949\nHarper\nRoy Winfield\n01/31/1949\n02/02/1949\nNaN\nNaN\n01/05/1971\n02/13/1994\nDeath\n\n\n\n\n4202 rows × 15 columns\n\n\n\n\njudges_appointments['commission_date'] = pd.to_datetime(judges_appointments['commission_date'])\njudges_appointments['commission_date'].dt.year.min()\n\n1789.0\n\n\n\njudges_people\n\n\n\n\n\n\n\n\njudge_id\nname_first\nname_middle\nname_last\nname_suffix\nbirth_date\nbirthplace_city\nbirthplace_state\ndeath_date\ndeath_city\ndeath_state\ngender\nrace\n\n\n\n\n0\n3419\nRonnie\nNaN\nAbrams\nNaN\n1968.0\nNew York\nNY\nNaN\nNaN\nNaN\nF\nWhite\n\n\n1\n1\nMatthew\nT.\nAbruzzo\nNaN\n1889.0\nBrooklyn\nNY\n1971.0\nPotomac\nMD\nM\nWhite\n\n\n2\n2\nMarcus\nWilson\nAcheson\nNaN\n1828.0\nWashington\nPA\n1906.0\nPittsburgh\nPA\nM\nWhite\n\n\n3\n3\nWilliam\nMarsh\nAcker\nJr.\n1927.0\nBirmingham\nAL\nNaN\nNaN\nNaN\nM\nWhite\n\n\n4\n4\nHarold\nArnold\nAckerman\nNaN\n1928.0\nNewark\nNJ\n2009.0\nWest Orange\nNJ\nM\nWhite\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3527\n3391\nJennifer\nGuerin\nZipps\nNaN\n1964.0\nAshland\nOH\nNaN\nNaN\nNaN\nF\nWhite\n\n\n3528\n2687\nAlfonso\nJoseph\nZirpoli\nNaN\n1905.0\nDenver\nCO\n1995.0\nSan Francisco\nCA\nM\nWhite\n\n\n3529\n2688\nWilliam\nJ.\nZloch\nNaN\n1944.0\nFort Lauderdale\nFL\nNaN\nNaN\nNaN\nM\nWhite\n\n\n3530\n2690\nRya\nWeickert\nZobel\nNaN\n1931.0\nZwickau\nGermany\nNaN\nNaN\nNaN\nF\nWhite\n\n\n3531\n3106\nJack\nNaN\nZouhary\nNaN\n1951.0\nToledo\nOH\nNaN\nNaN\nNaN\nM\nWhite\n\n\n\n\n3532 rows × 13 columns\n\n\n\n\njudges_people.groupby([\"gender\"]).size()\n\ngender\nF     389\nM    3143\ndtype: int64\n\n\n\njudges_people['race']=judges_people['race'].fillna(\"Others\")\n\n\ndf_grp = judges_people.groupby([\"gender\",\"race\"]).count().sort_values([\"gender\",\"judge_id\"], ascending=[True,False])\ndf_grp\n\n\n\n\n\n\n\n\n\njudge_id\nname_first\nname_middle\nname_last\nname_suffix\nbirth_date\nbirthplace_city\nbirthplace_state\ndeath_date\ndeath_city\ndeath_state\n\n\ngender\nrace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF\nWhite\n293\n293\n265\n293\n0\n293\n293\n293\n28\n24\n24\n\n\nAfrican American\n51\n51\n50\n51\n0\n51\n51\n51\n3\n3\n3\n\n\nHispanic\n29\n29\n21\n29\n0\n29\n29\n29\n1\n1\n1\n\n\nAsian American\n9\n9\n9\n9\n0\n9\n9\n9\n0\n0\n0\n\n\nOthers\n3\n3\n3\n3\n0\n3\n3\n3\n0\n0\n0\n\n\nAfrican Am./Hispanic\n1\n1\n1\n1\n0\n1\n1\n1\n0\n0\n0\n\n\nAmerican Indian\n1\n1\n1\n1\n0\n1\n1\n1\n0\n0\n0\n\n\nHispanic/Asian Am.\n1\n1\n0\n1\n0\n1\n1\n1\n0\n0\n0\n\n\nWhite/Asian Am.\n1\n1\n0\n1\n0\n1\n1\n1\n0\n0\n0\n\n\nM\nWhite\n2871\n2871\n2483\n2871\n299\n2870\n2848\n2869\n1906\n1350\n1352\n\n\nAfrican American\n150\n150\n135\n150\n41\n150\n150\n150\n42\n38\n38\n\n\nHispanic\n87\n87\n70\n87\n5\n87\n86\n86\n15\n9\n9\n\n\nAsian American\n22\n22\n18\n22\n0\n22\n22\n22\n6\n4\n4\n\n\nOthers\n6\n6\n6\n6\n1\n6\n6\n6\n1\n1\n1\n\n\nAmerican Indian\n2\n2\n2\n2\n0\n2\n2\n2\n0\n0\n0\n\n\nPac. Isl./Asian Am.\n2\n2\n2\n2\n0\n2\n2\n2\n0\n0\n0\n\n\nAfrican Am./Hispanic\n1\n1\n1\n1\n0\n1\n1\n1\n0\n0\n0\n\n\nHispanic/White\n1\n1\n1\n1\n0\n1\n1\n1\n0\n0\n0\n\n\nPac. Isl./White\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\n\ndf1 = df_grp[\"judge_id\"].reset_index()\ndf1\n\n\n\n\n\n\n\n\ngender\nrace\njudge_id\n\n\n\n\n0\nF\nWhite\n293\n\n\n1\nF\nAfrican American\n51\n\n\n2\nF\nHispanic\n29\n\n\n3\nF\nAsian American\n9\n\n\n4\nF\nOthers\n3\n\n\n5\nF\nAfrican Am./Hispanic\n1\n\n\n6\nF\nAmerican Indian\n1\n\n\n7\nF\nHispanic/Asian Am.\n1\n\n\n8\nF\nWhite/Asian Am.\n1\n\n\n9\nM\nWhite\n2871\n\n\n10\nM\nAfrican American\n150\n\n\n11\nM\nHispanic\n87\n\n\n12\nM\nAsian American\n22\n\n\n13\nM\nOthers\n6\n\n\n14\nM\nAmerican Indian\n2\n\n\n15\nM\nPac. Isl./Asian Am.\n2\n\n\n16\nM\nAfrican Am./Hispanic\n1\n\n\n17\nM\nHispanic/White\n1\n\n\n18\nM\nPac. Isl./White\n1\n\n\n\n\n\n\n\n\ndef process_group(group):\n    top_rows = group.nlargest(4, 'judge_id')  \n    remaining_rows = group.iloc[4:]  \n    if not remaining_rows.empty:\n        other_sum = remaining_rows['judge_id'].sum()\n        other_row = pd.DataFrame({'gender': [group.name], 'race': ['Others'], 'judge_id': [other_sum]})\n        return pd.concat([top_rows, other_row], ignore_index=True)\n    return top_rows\n\n# Apply function to each group\ndf_grouped = df1.groupby('gender', group_keys=False).apply(process_group)\n\ndf_grouped\n\n\n\n\n\n\n\n\ngender\nrace\njudge_id\n\n\n\n\n0\nF\nWhite\n293\n\n\n1\nF\nAfrican American\n51\n\n\n2\nF\nHispanic\n29\n\n\n3\nF\nAsian American\n9\n\n\n4\nF\nOthers\n7\n\n\n0\nM\nWhite\n2871\n\n\n1\nM\nAfrican American\n150\n\n\n2\nM\nHispanic\n87\n\n\n3\nM\nAsian American\n22\n\n\n4\nM\nOthers\n13\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\nfrom PIL import Image\nimport textwrap\n\n# Define race colors\nrace_colors = {\n    'White': (160, 200, 220), \n    'African American': (70, 130, 180),\n    'Hispanic': (0, 0, 128), \n    'Asian American': (30, 144, 255),  \n    'Others': (0, 0, 0)  \n}\ndf_grouped_dict = df_grouped.groupby(\"gender\").apply(lambda x: dict(zip(x[\"race\"], x[\"judge_id\"]))).to_dict()\n\ndf_percentages = {gender: {race: (count / sum(race_counts.values())) * 100 for race, count in race_counts.items()}\n                  for gender, race_counts in df_grouped_dict.items()}\n\n# Function to recolor an icon with stacked race colors\ndef recolor_icon_layers(icon_path, race_counts, race_colors):\n    img = Image.open(icon_path).convert(\"RGBA\")  # Convert to RGBA\n    data = np.array(img)  # Convert image to array\n\n    total_count = sum(race_counts.values())\n    height = data.shape[0]\n    y_start = 0\n\n    for race, count in race_counts.items():\n        layer_height = int((count / total_count) * height)\n        y_end = y_start + layer_height\n        mask = data[y_start:y_end, :, 3] &gt; 0  # Keep only non-transparent pixels\n        data[y_start:y_end, :, :-1][mask] = race_colors.get(race, (128, 128, 128))  # Apply race color\n        y_start = y_end\n\n    return Image.fromarray(data)\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Calculate total counts for scaling\ntotal_category1 = sum(df_grouped_dict.get(\"F\", {}).values())\ntotal_category2 = sum(df_grouped_dict.get(\"M\", {}).values())\n\nrace_colors_mpl = {race: (r/255, g/255, b/255) for race, (r, g, b) in race_colors.items()}\n\nfor i, (category, race_counts) in enumerate(df_grouped_dict.items()):\n    icon_path = \"person-dress.png\" if category == \"F\" else \"person.png\"\n    icon_array = recolor_icon_layers(icon_path, race_counts, race_colors)\n\n    scale_factor = sum(race_counts.values()) / max(total_category1, total_category2)\n    base_zoom = 0.8  # Adjust zoom level\n    imagebox = OffsetImage(np.array(icon_array), zoom=base_zoom * scale_factor)\n\n    ab = AnnotationBbox(imagebox, (i, 0), frameon=False, xycoords=\"data\", box_alignment=(0.5, 0))\n    ax.add_artist(ab)\n\n    percentages = df_percentages[category]\n    for j, (race, percent) in enumerate(reversed(percentages.items())):\n        if(category == \"F\"):\n            ax.text(i+0.3, j*150, f\"{percent:.1f}%\", ha='right', fontsize=10, color=race_colors_mpl[race])\n        else:            \n            ax.text(i+0.4, j*250, f\"{race}: {percent:.1f}%\", ha='left', fontsize=10, color=race_colors_mpl[race])\n\nax.text(0, 1000, f\"Female judges: {total_category1}\", ha='center', fontsize=10, color=\"black\")\nax.text(1, 4000, f\"Male judges: {total_category2:,}\", ha='center', fontsize=10, color=\"black\")\n\nax.set_xticks([0, 1])\nax.set_xlim(-0.5, 1.5)\nax.set_ylim(0, max(total_category1, total_category2) + 100)\ntitle = f\"Among the US judges appointed from {judges_appointments['commission_date'].dt.year.min():.0f} to {judges_appointments['commission_date'].dt.year.max():.0f}, there were about 8 male judges for every female judge. The proportion of the top four races is shown in different colors.\"\nwrapped_title = \"\\n\".join(textwrap.wrap(title, width=30))\nax.set_title(wrapped_title, loc='left', pad=75)\nax.axis(\"off\")\nfig.patch.set_facecolor(\"whitesmoke\")\nfig.savefig(\"judges_appoint.png\", bbox_inches=\"tight\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "posts/MTA_art/MTA_art.html",
    "href": "posts/MTA_art/MTA_art.html",
    "title": "MTA Permanent Art Catalog",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n\nmta_art = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-22/mta_art.csv')\nstation_lines = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-22/station_lines.csv')\n\n\nmta_art\n\n\n\n\n\n\n\n\nagency\nstation_name\nline\nartist\nart_title\nart_date\nart_material\nart_description\nart_image_link\n\n\n\n\n0\nNYCT\nClark St\n2,3\nRay Ring\nClark Street Passage\n1987\nTerrazzo floor tile\nThe first model that Brooklyn-born artist Ray ...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n1\nNYCT\n125 St\n4,5,6\nHouston Conwill\nThe Open Secret\n1986\nBronze - polychromed\nThe Open Secret, in the 125th Street and Lexin...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n2\nNYCT\nAstor Pl\n6\nMilton Glaser\nUntitled\n1986\nPorcelain enamel murals\nMilton Glaser, best known for his work in grap...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n3\nNYCT\nKings Hwy\nB,Q\nRhoda Andors\nKings Highway Hieroglyphs\n1987\nPorcelain Enamel Murals on Steel\nThe artist discusses her work: ÒIf public art...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n4\nNYCT\nNewkirk Av\nB,Q\nDavid Wilson\nTransit Skylight\n1988\nZinc-glazed Apolycarbonate skylight\nThe artist recalls, ÒAbout the same time that ...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n376\nNYCT\nAlabama Av\nJ,Z\nScott Redden\nUntitled\n2007\nFaceted glass\nImages of rural America evoke a nostalgia past...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n377\nNYCT\nWoodside-61 St\n7\nJohn Cavanagh\nCommuting/Community\n1986\nPorcelain enamel photomontage murals on steel\nCommuting/Community reflects John Cavanagh's i...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n378\nNYCT\nFulton St\nA, C, J, Z, 2, 3, 4, 5\nNancy Holt\nAstral Grating\n1987\nWrought iron, light elements\nNancy Holt's steel ceiling sculpture incorpora...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n379\nNYCT\n5 Av/53 St\nE,M\nRalph Fasanella\nSubway Riders\n1995\nPainting - Oil\nAll of us find ourselves from time to time mus...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n380\nNYCT\nCleveland St\nJ\nAmy Cheng\nLas Flores\n2007\nFaceted glass\nLocated within the platform windscreens, color...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n\n\n381 rows × 9 columns\n\n\n\n\nmta_art_date = mta_art.groupby('art_date').nunique()\nmta_art_date\n\n\n\n\n\n\n\n\nagency\nstation_name\nline\nartist\nart_title\nart_material\nart_description\nart_image_link\n\n\nart_date\n\n\n\n\n\n\n\n\n\n\n\n\n1980\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n1986\n1\n3\n3\n3\n3\n3\n3\n3\n\n\n1987\n1\n3\n3\n3\n3\n3\n3\n3\n\n\n1988\n1\n3\n3\n3\n3\n3\n3\n3\n\n\n1989\n1\n2\n2\n2\n2\n2\n2\n2\n\n\n1990\n2\n10\n5\n5\n6\n6\n5\n5\n\n\n1991\n2\n12\n7\n10\n13\n11\n11\n10\n\n\n1992\n3\n6\n5\n6\n6\n6\n6\n6\n\n\n1993\n2\n3\n2\n2\n3\n2\n2\n2\n\n\n1994\n2\n11\n7\n9\n9\n8\n9\n9\n\n\n1995\n2\n4\n3\n4\n4\n4\n4\n4\n\n\n1996\n2\n11\n10\n11\n11\n9\n10\n9\n\n\n1997\n2\n6\n6\n6\n6\n6\n6\n6\n\n\n1998\n3\n9\n9\n10\n10\n9\n10\n10\n\n\n1999\n3\n15\n10\n13\n13\n10\n13\n13\n\n\n2000\n2\n9\n9\n9\n9\n9\n9\n9\n\n\n2001\n2\n9\n9\n10\n10\n8\n10\n10\n\n\n2002\n3\n15\n13\n17\n17\n10\n17\n17\n\n\n2003\n1\n3\n3\n3\n3\n3\n3\n3\n\n\n2004\n1\n14\n13\n14\n14\n11\n14\n14\n\n\n2005\n3\n12\n11\n12\n12\n10\n12\n11\n\n\n2006\n2\n15\n6\n15\n15\n8\n15\n15\n\n\n2007\n2\n14\n7\n14\n14\n6\n14\n14\n\n\n2008\n2\n11\n8\n11\n11\n8\n11\n11\n\n\n2009\n3\n8\n7\n8\n8\n7\n8\n8\n\n\n2010\n2\n7\n5\n7\n7\n7\n7\n7\n\n\n2011\n2\n21\n10\n20\n21\n14\n20\n20\n\n\n2012\n2\n15\n5\n15\n15\n8\n15\n15\n\n\n2013\n3\n8\n3\n6\n6\n8\n6\n6\n\n\n2014\n2\n3\n2\n3\n3\n3\n3\n3\n\n\n2015\n3\n11\n6\n11\n11\n8\n11\n11\n\n\n2016\n2\n5\n4\n5\n5\n5\n5\n5\n\n\n2017\n3\n16\n5\n13\n14\n11\n14\n13\n\n\n2018\n4\n35\n20\n36\n36\n16\n36\n36\n\n\n2019\n3\n12\n7\n12\n12\n11\n12\n12\n\n\n2020\n3\n7\n7\n8\n8\n6\n8\n8\n\n\n2021\n3\n8\n4\n8\n8\n7\n8\n8\n\n\n2022\n2\n5\n5\n6\n6\n4\n6\n6\n\n\n2023\n3\n9\n6\n9\n9\n5\n8\n8\n\n\n\n\n\n\n\n\n# change art_matrial to sentence case\nmta_art['art_material'] = mta_art['art_material'].str.title()\nam_vc = mta_art['art_material'].value_counts()\n#am_vc\n# get am_vc greater than 1\nam_vc[am_vc &gt; 1]\n\nart_material\nFaceted Glass                                                                               50\nGlass Mosaic                                                                                49\nLaminated Glass                                                                             46\nStainless Steel                                                                             15\nCeramic Mosaic                                                                              11\nBronze                                                                                       6\nGlass And Ceramic Mosaic                                                                     6\nCopper Wire Mesh And Stainless Steel                                                         5\nMosaic                                                                                       4\nPainted Stainless Steel                                                                      4\nGlass Or Ceramic Mosaic                                                                      4\nAluminum                                                                                     4\nFused Glass                                                                                  3\nGlass Mosaics                                                                                3\nPowder-Coated Aluminum Fence Panels                                                          3\nSteel                                                                                        3\nCeramic - Porcelain Tiles                                                                    2\nSteel, Ceramic Tile, Granite, Fiberglass                                                     2\nSandblasted And Laminated Glass Windows, Steel Railings (Stair Railing & Exterior Fence)     2\nLaminated Glass And Mosaic                                                                   2\nMosaic/Glass                                                                                 2\nGlass Mosaic And Laminated Glass                                                             2\nPorcelain Enamel On Steel                                                                    2\nWrought Iron                                                                                 2\nCeramic Tile,  Glass Mosaic                                                                  2\nGlass Blocks                                                                                 2\nCeramic Tiles                                                                                2\nCeramic Tile                                                                                 2\nMosaic On Board                                                                              2\nCast Concrete                                                                                2\nCor-Ten Steel                                                                                2\nForged Steel                                                                                 2\nBronze - Patinated                                                                           2\nLaminated Glass; Stainless Steel                                                             2\nName: count, dtype: int64\n\n\n\n# Filter art_materials that appear more than once\nfiltered_materials = am_vc[am_vc &gt; 1].index\n\n# Filter the dataset for these materials\nfiltered_mta_art = mta_art[mta_art['art_material'].isin(filtered_materials)]\n\n# Find the first date for each art_material\nmaterial_timeline = filtered_mta_art.groupby('art_material')['art_date'].min().reset_index()\nmaterial_timeline = material_timeline.sort_values(by='art_date')\n# split art_material by \"(\" and keep only the first part\nmaterial_timeline['art_material'] = material_timeline['art_material'].str.split('(').str[0]\nmaterial_timeline\n\n\n\n\n\n\n\n\nart_material\nart_date\n\n\n\n\n1\nBronze\n1988\n\n\n6\nCeramic Tile\n1989\n\n\n2\nBronze - Patinated\n1990\n\n\n9\nCopper Wire Mesh And Stainless Steel\n1990\n\n\n3\nCast Concrete\n1991\n\n\n31\nSteel\n1991\n\n\n14\nGlass And Ceramic Mosaic\n1991\n\n\n10\nCor-Ten Steel\n1991\n\n\n8\nCeramic Tiles\n1991\n\n\n0\nAluminum\n1992\n\n\n16\nGlass Mosaic\n1992\n\n\n7\nCeramic Tile, Glass Mosaic\n1992\n\n\n11\nFaceted Glass\n1993\n\n\n4\nCeramic - Porcelain Tiles\n1993\n\n\n29\nSandblasted And Laminated Glass Windows, Steel...\n1994\n\n\n5\nCeramic Mosaic\n1994\n\n\n27\nPorcelain Enamel On Steel\n1997\n\n\n33\nWrought Iron\n1999\n\n\n32\nSteel, Ceramic Tile, Granite, Fiberglass\n2002\n\n\n18\nGlass Mosaics\n2003\n\n\n20\nLaminated Glass\n2006\n\n\n13\nFused Glass\n2007\n\n\n30\nStainless Steel\n2010\n\n\n12\nForged Steel\n2010\n\n\n19\nGlass Or Ceramic Mosaic\n2011\n\n\n21\nLaminated Glass And Mosaic\n2011\n\n\n24\nMosaic On Board\n2011\n\n\n15\nGlass Blocks\n2011\n\n\n17\nGlass Mosaic And Laminated Glass\n2013\n\n\n23\nMosaic\n2015\n\n\n26\nPainted Stainless Steel\n2017\n\n\n25\nMosaic/Glass\n2018\n\n\n28\nPowder-Coated Aluminum Fence Panels\n2018\n\n\n22\nLaminated Glass; Stainless Steel\n2022"
  },
  {
    "objectID": "posts/MTA_art/MTA_art.html#tidytuesday-data-for-2025-07-08",
    "href": "posts/MTA_art/MTA_art.html#tidytuesday-data-for-2025-07-08",
    "title": "MTA Permanent Art Catalog",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n\nmta_art = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-22/mta_art.csv')\nstation_lines = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-22/station_lines.csv')\n\n\nmta_art\n\n\n\n\n\n\n\n\nagency\nstation_name\nline\nartist\nart_title\nart_date\nart_material\nart_description\nart_image_link\n\n\n\n\n0\nNYCT\nClark St\n2,3\nRay Ring\nClark Street Passage\n1987\nTerrazzo floor tile\nThe first model that Brooklyn-born artist Ray ...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n1\nNYCT\n125 St\n4,5,6\nHouston Conwill\nThe Open Secret\n1986\nBronze - polychromed\nThe Open Secret, in the 125th Street and Lexin...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n2\nNYCT\nAstor Pl\n6\nMilton Glaser\nUntitled\n1986\nPorcelain enamel murals\nMilton Glaser, best known for his work in grap...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n3\nNYCT\nKings Hwy\nB,Q\nRhoda Andors\nKings Highway Hieroglyphs\n1987\nPorcelain Enamel Murals on Steel\nThe artist discusses her work: ÒIf public art...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n4\nNYCT\nNewkirk Av\nB,Q\nDavid Wilson\nTransit Skylight\n1988\nZinc-glazed Apolycarbonate skylight\nThe artist recalls, ÒAbout the same time that ...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n376\nNYCT\nAlabama Av\nJ,Z\nScott Redden\nUntitled\n2007\nFaceted glass\nImages of rural America evoke a nostalgia past...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n377\nNYCT\nWoodside-61 St\n7\nJohn Cavanagh\nCommuting/Community\n1986\nPorcelain enamel photomontage murals on steel\nCommuting/Community reflects John Cavanagh's i...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n378\nNYCT\nFulton St\nA, C, J, Z, 2, 3, 4, 5\nNancy Holt\nAstral Grating\n1987\nWrought iron, light elements\nNancy Holt's steel ceiling sculpture incorpora...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n379\nNYCT\n5 Av/53 St\nE,M\nRalph Fasanella\nSubway Riders\n1995\nPainting - Oil\nAll of us find ourselves from time to time mus...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n380\nNYCT\nCleveland St\nJ\nAmy Cheng\nLas Flores\n2007\nFaceted glass\nLocated within the platform windscreens, color...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n\n\n381 rows × 9 columns\n\n\n\n\nmta_art_date = mta_art.groupby('art_date').nunique()\nmta_art_date\n\n\n\n\n\n\n\n\nagency\nstation_name\nline\nartist\nart_title\nart_material\nart_description\nart_image_link\n\n\nart_date\n\n\n\n\n\n\n\n\n\n\n\n\n1980\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n1986\n1\n3\n3\n3\n3\n3\n3\n3\n\n\n1987\n1\n3\n3\n3\n3\n3\n3\n3\n\n\n1988\n1\n3\n3\n3\n3\n3\n3\n3\n\n\n1989\n1\n2\n2\n2\n2\n2\n2\n2\n\n\n1990\n2\n10\n5\n5\n6\n6\n5\n5\n\n\n1991\n2\n12\n7\n10\n13\n11\n11\n10\n\n\n1992\n3\n6\n5\n6\n6\n6\n6\n6\n\n\n1993\n2\n3\n2\n2\n3\n2\n2\n2\n\n\n1994\n2\n11\n7\n9\n9\n8\n9\n9\n\n\n1995\n2\n4\n3\n4\n4\n4\n4\n4\n\n\n1996\n2\n11\n10\n11\n11\n9\n10\n9\n\n\n1997\n2\n6\n6\n6\n6\n6\n6\n6\n\n\n1998\n3\n9\n9\n10\n10\n9\n10\n10\n\n\n1999\n3\n15\n10\n13\n13\n10\n13\n13\n\n\n2000\n2\n9\n9\n9\n9\n9\n9\n9\n\n\n2001\n2\n9\n9\n10\n10\n8\n10\n10\n\n\n2002\n3\n15\n13\n17\n17\n10\n17\n17\n\n\n2003\n1\n3\n3\n3\n3\n3\n3\n3\n\n\n2004\n1\n14\n13\n14\n14\n11\n14\n14\n\n\n2005\n3\n12\n11\n12\n12\n10\n12\n11\n\n\n2006\n2\n15\n6\n15\n15\n8\n15\n15\n\n\n2007\n2\n14\n7\n14\n14\n6\n14\n14\n\n\n2008\n2\n11\n8\n11\n11\n8\n11\n11\n\n\n2009\n3\n8\n7\n8\n8\n7\n8\n8\n\n\n2010\n2\n7\n5\n7\n7\n7\n7\n7\n\n\n2011\n2\n21\n10\n20\n21\n14\n20\n20\n\n\n2012\n2\n15\n5\n15\n15\n8\n15\n15\n\n\n2013\n3\n8\n3\n6\n6\n8\n6\n6\n\n\n2014\n2\n3\n2\n3\n3\n3\n3\n3\n\n\n2015\n3\n11\n6\n11\n11\n8\n11\n11\n\n\n2016\n2\n5\n4\n5\n5\n5\n5\n5\n\n\n2017\n3\n16\n5\n13\n14\n11\n14\n13\n\n\n2018\n4\n35\n20\n36\n36\n16\n36\n36\n\n\n2019\n3\n12\n7\n12\n12\n11\n12\n12\n\n\n2020\n3\n7\n7\n8\n8\n6\n8\n8\n\n\n2021\n3\n8\n4\n8\n8\n7\n8\n8\n\n\n2022\n2\n5\n5\n6\n6\n4\n6\n6\n\n\n2023\n3\n9\n6\n9\n9\n5\n8\n8\n\n\n\n\n\n\n\n\n# change art_matrial to sentence case\nmta_art['art_material'] = mta_art['art_material'].str.title()\nam_vc = mta_art['art_material'].value_counts()\n#am_vc\n# get am_vc greater than 1\nam_vc[am_vc &gt; 1]\n\nart_material\nFaceted Glass                                                                               50\nGlass Mosaic                                                                                49\nLaminated Glass                                                                             46\nStainless Steel                                                                             15\nCeramic Mosaic                                                                              11\nBronze                                                                                       6\nGlass And Ceramic Mosaic                                                                     6\nCopper Wire Mesh And Stainless Steel                                                         5\nMosaic                                                                                       4\nPainted Stainless Steel                                                                      4\nGlass Or Ceramic Mosaic                                                                      4\nAluminum                                                                                     4\nFused Glass                                                                                  3\nGlass Mosaics                                                                                3\nPowder-Coated Aluminum Fence Panels                                                          3\nSteel                                                                                        3\nCeramic - Porcelain Tiles                                                                    2\nSteel, Ceramic Tile, Granite, Fiberglass                                                     2\nSandblasted And Laminated Glass Windows, Steel Railings (Stair Railing & Exterior Fence)     2\nLaminated Glass And Mosaic                                                                   2\nMosaic/Glass                                                                                 2\nGlass Mosaic And Laminated Glass                                                             2\nPorcelain Enamel On Steel                                                                    2\nWrought Iron                                                                                 2\nCeramic Tile,  Glass Mosaic                                                                  2\nGlass Blocks                                                                                 2\nCeramic Tiles                                                                                2\nCeramic Tile                                                                                 2\nMosaic On Board                                                                              2\nCast Concrete                                                                                2\nCor-Ten Steel                                                                                2\nForged Steel                                                                                 2\nBronze - Patinated                                                                           2\nLaminated Glass; Stainless Steel                                                             2\nName: count, dtype: int64\n\n\n\n# Filter art_materials that appear more than once\nfiltered_materials = am_vc[am_vc &gt; 1].index\n\n# Filter the dataset for these materials\nfiltered_mta_art = mta_art[mta_art['art_material'].isin(filtered_materials)]\n\n# Find the first date for each art_material\nmaterial_timeline = filtered_mta_art.groupby('art_material')['art_date'].min().reset_index()\nmaterial_timeline = material_timeline.sort_values(by='art_date')\n# split art_material by \"(\" and keep only the first part\nmaterial_timeline['art_material'] = material_timeline['art_material'].str.split('(').str[0]\nmaterial_timeline\n\n\n\n\n\n\n\n\nart_material\nart_date\n\n\n\n\n1\nBronze\n1988\n\n\n6\nCeramic Tile\n1989\n\n\n2\nBronze - Patinated\n1990\n\n\n9\nCopper Wire Mesh And Stainless Steel\n1990\n\n\n3\nCast Concrete\n1991\n\n\n31\nSteel\n1991\n\n\n14\nGlass And Ceramic Mosaic\n1991\n\n\n10\nCor-Ten Steel\n1991\n\n\n8\nCeramic Tiles\n1991\n\n\n0\nAluminum\n1992\n\n\n16\nGlass Mosaic\n1992\n\n\n7\nCeramic Tile, Glass Mosaic\n1992\n\n\n11\nFaceted Glass\n1993\n\n\n4\nCeramic - Porcelain Tiles\n1993\n\n\n29\nSandblasted And Laminated Glass Windows, Steel...\n1994\n\n\n5\nCeramic Mosaic\n1994\n\n\n27\nPorcelain Enamel On Steel\n1997\n\n\n33\nWrought Iron\n1999\n\n\n32\nSteel, Ceramic Tile, Granite, Fiberglass\n2002\n\n\n18\nGlass Mosaics\n2003\n\n\n20\nLaminated Glass\n2006\n\n\n13\nFused Glass\n2007\n\n\n30\nStainless Steel\n2010\n\n\n12\nForged Steel\n2010\n\n\n19\nGlass Or Ceramic Mosaic\n2011\n\n\n21\nLaminated Glass And Mosaic\n2011\n\n\n24\nMosaic On Board\n2011\n\n\n15\nGlass Blocks\n2011\n\n\n17\nGlass Mosaic And Laminated Glass\n2013\n\n\n23\nMosaic\n2015\n\n\n26\nPainted Stainless Steel\n2017\n\n\n25\nMosaic/Glass\n2018\n\n\n28\nPowder-Coated Aluminum Fence Panels\n2018\n\n\n22\nLaminated Glass; Stainless Steel\n2022"
  },
  {
    "objectID": "posts/MTA_art/MTA_art.html#plotting",
    "href": "posts/MTA_art/MTA_art.html#plotting",
    "title": "MTA Permanent Art Catalog",
    "section": "Plotting",
    "text": "Plotting\n\nfig, ax = plt.subplots(figsize=(7,7))\nml,sl,bl = ax.stem(material_timeline['art_date'], material_timeline['art_material'],\\\n basefmt=' ', bottom=34)\nplt.setp(sl, linestyle=':', color='lightblue')\nplt.setp(ml,  marker=\"*\")\nfor i, txt in enumerate(material_timeline['art_material']):\n    ax.annotate(txt, (material_timeline['art_date'][i]+0.5, material_timeline['art_material'][i]), xytext=(0, -2), textcoords='offset points')\nax.get_yaxis().set_visible(False)\n#ax.invert_yaxis()\nax.set_xticks(material_timeline['art_date'].unique())\nax.set_xticklabels(material_timeline['art_date'].unique(), rotation=90)\nax.xaxis.set_ticks_position('top')\nax.xaxis.set_label_position('top')\nax.tick_params(axis='x', which='both', length=0)\nplt.xticks(fontfamily='monospace', color=\"gray\")\nplt.tick_params(axis='x', pad=-15)\nax.spines[['top', 'right', 'bottom', 'left']].set_visible(False)\nplt.title(\"First appearance of art materials in the MTA art data.\\nArt materials that appear more than once are shown.\", fontfamily='serif', fontsize=12, loc='left')\n#plt.savefig(\"MTA_art.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "posts/NSF_grants/NSF_grants_cut.html",
    "href": "posts/NSF_grants/NSF_grants_cut.html",
    "title": "NSF funding cut in April 2025",
    "section": "",
    "text": "import pandas as pd\nimport plotly.express as px\nnsf_terminations = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-06/nsf_terminations.csv')\nnsf_terminations\n\n\n\n\n\n\n\n\ngrant_number\nproject_title\ntermination_letter_date\norg_name\norg_city\norg_state\norg_district\nusaspending_obligated\naward_type\ndirectorate_abbrev\n...\ndivision\nnsf_program_name\nnsf_url\nusaspending_url\nnsf_startdate\nnsf_expected_end_date\norg_zip\norg_uei\nabstract\nin_cruz_list\n\n\n\n\n0\n2135329\nCollaborative Research: Research: Early-Career...\n2025-04-25\nUniversity of New Mexico\nALBUQUERQUE\nNM\nNM01\n190725.0\nStandard Grant\nENG\n...\nEngineering Education and Centers\nER2-Ethical & Responsible Res\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_213...\n2022-09-01\n2025-04-18\n871310001\nF6XLTRUQJEN4\nTransportation systems, computing algorithms, ...\nTrue\n\n\n1\n2342099\nMyTurn: An Afterschool Social Robotics Program...\n2025-04-25\nUniversity of Illinois at Chicago\nCHICAGO\nIL\nIL07\n499999.0\nStandard Grant\nEDU\n...\nResearch on Learning in Formal and Informal Se...\nITEST-Inov Tech Exp Stu & Teac\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_234...\n2024-08-15\n2027-07-31\n606124305\nW8XEAJDKMXH3\nComputational thinking and robotics are increa...\nFalse\n\n\n2\n2201103\nCollaborative Research: The Organizational Cli...\n2025-04-25\nAmerican Society For Engineering Education\nWASHINGTON\nDC\nDC00\n124241.0\nContinuing Grant\nEDU\n...\nEquity for Excellence in STEM\nECR-EDU Core Research\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_220...\n2022-08-01\n2026-07-31\n200362476\nF6G9C4HMNHW4\nThe ongoing lack of diversity in the engineeri...\nTrue\n\n\n3\n2215382\nEngaging Rural, Latinx Youth in an After Schoo...\n2025-04-25\nTERC Inc\nCAMBRIDGE\nMA\nMA05\n2601763.0\nContinuing Grant\nEDU\n...\nResearch on Learning in Formal and Informal Se...\nAISL\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_221...\n2022-08-01\n2026-07-31\n21401339\nGSLCJ3M62XX1\nThe project will develop and research an after...\nTrue\n\n\n4\n2405633\nDesign Effective and Equitable Professional Le...\n2025-04-25\nSan Francisco State University\nSAN FRANCISCO\nCA\nCA11\n565771.0\nContinuing Grant\nEDU\n...\nResearch on Learning in Formal and Informal Se...\nDiscovery Research K-12\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_240...\n2024-10-01\n2028-09-30\n941321740\nF4SLJ5WF59F6\nProviding computer science (CS) education to s...\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1036\n2411129\nCommunity-Situated Data Practices in Multiethn...\n2025-04-18\nMichigan State University\nEAST LANSING\nMI\nMI07\n1736866.0\nContinuing Grant\nEDU\n...\nResearch on Learning in Formal and Informal Se...\nDiscovery Research K-12\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_241...\n2024-09-15\n2029-08-31\n488242600\nR28EKN92ZTZ9\nBroadening STEM research and education to incl...\nFalse\n\n\n1037\n2224674\nCollaborative Research: Engaging Marginalized ...\n2025-04-18\nGeorge Mason University\nFAIRFAX\nVA\nVA11\n439380.0\nContinuing Grant\nEDU\n...\nResearch on Learning in Formal and Informal Se...\nECR-EDU Core Research\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_222...\n2023-06-15\n2028-05-31\n220304422\nEADLFP7Z72E5\nThis collaborative project investigates the la...\nTrue\n\n\n1038\n2315024\nCollaborative Research: Overcoming Isolation a...\n2025-04-18\nOhio State University\nCOLUMBUS\nOH\nOH03\n378441.0\nStandard Grant\nEDU\n...\nGraduate Education\nADVANCE\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_231...\n2023-09-15\n2025-04-18\n432101016\nDLWBSLWAJWR1\nThere is a growing need for scholars specializ...\nTrue\n\n\n1039\n2216826\nCommunity of Neighboring and National Entrepre...\n2025-04-18\nSt. Catherine University\nSAINT PAUL\nMN\nMN04\n75000.0\nStandard Grant\nBIO\n...\nBiological Infrastructure\nUBE - Undergraduate Biology Ed\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_221...\n2022-09-01\n2025-08-31\n551051750\nRQJ5KM1LQ935\nThis project aims to serve the national intere...\nTrue\n\n\n1040\n2210842\nCollaborative Research: HCC: Designing Technol...\n2025-04-18\nUniversity of California-Irvine\nIRVINE\nCA\nCA47\n90626.0\nStandard Grant\nCISE\n...\nInformation and Intelligent Systems\nHCC-Human-Centered Computing\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_221...\n2022-10-01\n2025-04-18\n926970001\nMJC5FCYQTPE6\nThis award supports research that examines the...\nTrue\n\n\n\n\n1041 rows × 21 columns\nnsf_terminations.columns\n\nIndex(['grant_number', 'project_title', 'termination_letter_date', 'org_name',\n       'org_city', 'org_state', 'org_district', 'usaspending_obligated',\n       'award_type', 'directorate_abbrev', 'directorate', 'division',\n       'nsf_program_name', 'nsf_url', 'usaspending_url', 'nsf_startdate',\n       'nsf_expected_end_date', 'org_zip', 'org_uei', 'abstract',\n       'in_cruz_list'],\n      dtype='object')"
  },
  {
    "objectID": "posts/NSF_grants/NSF_grants_cut.html#sunburst-plot-using-plotly",
    "href": "posts/NSF_grants/NSF_grants_cut.html#sunburst-plot-using-plotly",
    "title": "NSF funding cut in April 2025",
    "section": "Sunburst plot using plotly",
    "text": "Sunburst plot using plotly\n\nnsf_grp = nsf_terminations.groupby(['directorate', 'division'], as_index=False)['usaspending_obligated'].sum()\n\ncustom_colorscale = [\n    [0.0, 'black'],       # 0 → black\n    [0.000001, 'rgb(68,1,84)'],   # Start of Viridis after 0\n    [0.25, 'rgb(59,82,139)'],\n    [0.5, 'rgb(33,145,140)'],\n    [0.75, 'rgb(94,201,98)'],\n    [1.0, 'rgb(253,231,37)']\n]\n\nfig = px.sunburst(nsf_grp, path=['directorate', 'division'], values='usaspending_obligated', \\\n                  color='usaspending_obligated', color_continuous_scale=custom_colorscale, width=900, height=700, \\\n                  title=f\"Distribution of the NSF funding cut of &lt;b&gt;${nsf_grp[\"usaspending_obligated\"].sum():,.0f}&lt;/b&gt; in April 2025.\"\n                 )\nfig.update_traces(textinfo='label+percent entry')\nfor i in range(len(fig.data[0].marker.colors) - len(nsf_grp['directorate'].unique()), len(fig.data[0].marker.colors)):\n    fig.data[0].marker.colors[i] = 0\n#fig.write_image(\"sunburst_plot.png\")\nfig.show()"
  },
  {
    "objectID": "posts/Scottish_munro/Scottish_Munro.html",
    "href": "posts/Scottish_munro/Scottish_Munro.html",
    "title": "Scottish Munro Classification",
    "section": "",
    "text": "TidyTuesday dataset of 2025-08-19\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nscottish_munros = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-19/scottish_munros.csv', encoding='latin1')\n\n\nscottish_munros\n\n\n\n\n\n\n\n\nDoBIH_number\nName\nHeight_m\nHeight_ft\nxcoord\nycoord\n1891\n1921\n1933\n1953\n1969\n1974\n1981\n1984\n1990\n1997\n2021\nComments\n\n\n\n\n0\n1\nBen Chonzie\n931.0\n3054.0\n277324.0\n730857.0\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nNaN\n\n\n1\n17\nBen Vorlich\n985.3\n3233.0\n262912.0\n718916.0\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nNaN\n\n\n2\n18\nStuc a' Chroin\n973.0\n3192.0\n261746.0\n717465.0\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nNaN\n\n\n3\n32\nBen Lomond\n973.7\n3195.0\n236707.0\n702863.0\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nNaN\n\n\n4\n26\nBen More\n1174.0\n3852.0\n243276.0\n724417.0\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n599\n1258\nSgurr nan Eag\n926.3\n3039.0\n145705.0\n819536.0\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nNaN\n\n\n600\n1255\nBlabheinn [Bla Bheinn]\n929.0\n3048.0\n152990.0\n821743.0\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\n1891: Blath Bheinn (Blaven), N Top; 1921-1969:...\n\n\n601\n1257\nBlabheinn South Top\n926.5\n3040.0\n152855.0\n821529.0\nMunro Top\nNaN\nNaN\nNaN\nNaN\nNaN\nMunro Top\nMunro Top\nMunro Top\nMunro Top\nMunro Top\n1891: Blath Bheinn (Blaven), S Top; 1981-1990:...\n\n\n602\n1301\nBen More\n966.0\n3169.0\n152576.0\n733078.0\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nNaN\n\n\n603\nTen-figure grid references suitable for Garmin...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n604 rows × 18 columns\n\n\n\n\nscottish_munros.columns\n\nIndex(['DoBIH_number', 'Name', 'Height_m', 'Height_ft', 'xcoord', 'ycoord',\n       '1891', '1921', '1933', '1953', '1969', '1974', '1981', '1984', '1990',\n       '1997', '2021', 'Comments'],\n      dtype='object')\n\n\n\nscottish_munros.groupby('1891').count()\n\n\n\n\n\n\n\n\nDoBIH_number\nName\nHeight_m\nHeight_ft\nxcoord\nycoord\n1921\n1933\n1953\n1969\n1974\n1981\n1984\n1990\n1997\n2021\nComments\n\n\n1891\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMunro\n283\n283\n283\n283\n283\n283\n279\n279\n278\n278\n277\n279\n280\n280\n277\n277\n106\n\n\nMunro Top\n255\n255\n255\n255\n255\n255\n242\n242\n242\n242\n233\n196\n194\n194\n191\n190\n142\n\n\n\n\n\n\n\n\nsns.histplot(scottish_munros, x='Height_ft', hue='1891', multiple='stack', bins=20)\nplt.show()\n\n\n\n\n\n\n\n\n\nscottish_munros_long = scottish_munros.melt(id_vars=['DoBIH_number', 'Name', 'Height_m', 'Height_ft', 'xcoord', 'ycoord', 'Comments'], var_name='year', value_name='munro_type')\nmunro_counts = scottish_munros_long.groupby(['year', 'munro_type']).size()\n\n\nsns.set_style(\"dark\", {\n    'axes.facecolor': 'darkgrey',     # background of the plot\n    'figure.facecolor': 'darkgrey',   # background around the plot\n    'axes.edgecolor': 'gainsboro',\n    'font.family': 'monospace',\n})\nsns.set_context(\"notebook\", font_scale=2)\n\ng = sns.displot(data=scottish_munros_long, x='Height_ft', hue='munro_type', col='year', \\\ncol_wrap=6, kind='hist', multiple='dodge', palette=['lightgreen','white'], \\\nheight=3, aspect=1.5, bins=20, edgecolor='none')\n#sns.move_legend(g,\"lower right\", bbox_to_anchor=(0.875, 0.2), title=None, frameon=False)\ng.legend.set_visible(False)\ng.set_axis_labels(\"\", \"\")\ng.figure.suptitle(\"Year-wise distribution of heights for Scottish Munros and Munro Tops\",\\\nx=0.05, y=1.025, ha='left', fontfamily='serif', fontsize=36)\ng.figure.subplots_adjust(wspace=0, hspace=0)\ntext_obj = g.figure.supylabel('Count')\nx, y = text_obj.get_position()   # get current position\ntext_obj.set_position((x - 0.01, y))  \ng.figure.supxlabel('Height (feet)')\n\ncustom_ticks = [3000, 3500, 4000]\ncustom_labels = ['3K', '3.5', '4K']\n\n# Set the ticks and labels on each axis in the grid\nfor ax in g.axes.flat:\n    ax.set_xticks(custom_ticks)\n    ax.set_xticklabels(custom_labels)\n    title_text = ax.get_title()\n    year = title_text.split('=')[-1].strip()\n    year_counts = munro_counts.loc[year]\n    for i, (mt, m_count) in enumerate(year_counts.items()):\n        k=0\n        if(i==0):\n            if(ax == g.axes.flat[-1]):\n                m_count = f'{m_count} ← Total Munros'\n                k = 0.6\n            ax.text(0.9 + k, 0.5 - i * 0.1, f'{m_count}', transform=ax.transAxes, ha='right', va='center', color='lightgreen')\n        else:\n            if(ax == g.axes.flat[-1]):\n                m_count = f'{m_count} ← Total Munro Tops'\n                k = 0.8\n            ax.text(0.9 + k, 0.5 - i * 0.2, f'{m_count}', transform=ax.transAxes, ha='right', va='center', color='white')    \n\ng.set_titles(\"{col_name}\", y=0.7)\ng.figure.text(0.90, 0.01, '1K feet = 304.8 meters', ha='right', color='gainsboro')\n\nsns.despine()\n#g.figure.savefig('Scottish_munros.png', dpi=300, bbox_inches='tight',pad_inches=0.25)\nplt.show()"
  },
  {
    "objectID": "posts/Sherlock_Holmes/Sherlock_Holmes.html",
    "href": "posts/Sherlock_Holmes/Sherlock_Holmes.html",
    "title": "Sherlock Holmes stories and novels",
    "section": "",
    "text": "TidyTuesday dataset of November 18, 2025\n\nimport marimo as mo\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport spacy\nimport textwrap\n\n\nholmes = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-18/holmes.csv')\nholmes\n\n\n\n\n\n\n\n\nbook\ntext\nline_num\n\n\n\n\n0\nA Study In Scarlet\nA STUDY IN SCARLET\n1\n\n\n1\nA Study In Scarlet\nNaN\n2\n\n\n2\nA Study In Scarlet\nTable of contents\n3\n\n\n3\nA Study In Scarlet\nNaN\n4\n\n\n4\nA Study In Scarlet\nPart I\n5\n\n\n...\n...\n...\n...\n\n\n65953\nHis Last Bow\nSee http://sherlock-holm.es for an electronic ...\n755\n\n\n65954\nHis Last Bow\nadditional information about it.\n756\n\n\n65955\nHis Last Bow\nNaN\n757\n\n\n65956\nHis Last Bow\nThis text comes from the collection's version ...\n758\n\n\n65957\nHis Last Bow\nNaN\n759\n\n\n\n\n65958 rows × 3 columns\n\n\n\n\nconcatenated = holmes.groupby('book')['text'].apply(lambda x: ' '.join(x.dropna())).reset_index()\nconcatenated\n\n\n\n\n\n\n\n\nbook\ntext\n\n\n\n\n0\nA Case of Identity\nA CASE OF IDENTITY \"My dear fellow,\" said Sher...\n\n\n1\nA Scandal in Bohemia\nA SCANDAL IN BOHEMIA Table of contents Chapter...\n\n\n2\nA Study In Scarlet\nA STUDY IN SCARLET Table of contents Part I Mr...\n\n\n3\nHis Last Bow\nHIS LAST BOW An Epilogue of Sherlock Holmes It...\n\n\n4\nSilver Blaze\nSILVER BLAZE \"I am afraid, Watson, that I shal...\n\n\n5\nThe \"Gloria Scott\"\nTHE \"GLORIA SCOTT\" \"I have some papers here,\" ...\n\n\n6\nThe Adventure of Black Peter\nTHE ADVENTURE OF BLACK PETER I have never know...\n\n\n7\nThe Adventure of Charles Augustus Milverton\nTHE ADVENTURE OF CHARLES AUGUSTUS MILVERTON It...\n\n\n8\nThe Adventure of Wisteria Lodge\nTHE ADVENTURE OF WISTERIA LODGE Table of conte...\n\n\n9\nThe Adventure of the Abbey Grange\nTHE ADVENTURE OF THE ABBEY GRANGE It was on a ...\n\n\n10\nThe Adventure of the Beryl Coronet\nTHE ADVENTURE OF THE BERYL CORONET \"Holmes,\" s...\n\n\n11\nThe Adventure of the Blue Carbuncle\nTHE ADVENTURE OF THE BLUE CARBUNCLE I had call...\n\n\n12\nThe Adventure of the Bruce-Partington Plans\nTHE ADVENTURE OF THE BRUCE-PARTINGTON PLANS In...\n\n\n13\nThe Adventure of the Cardboard Box\nTHE ADVENTURE OF THE CARDBOARD BOX In choosing...\n\n\n14\nThe Adventure of the Copper Beeches\nTHE ADVENTURE OF THE COPPER BEECHES \"To the ma...\n\n\n15\nThe Adventure of the Dancing Men\nTHE ADVENTURE OF THE DANCING MEN Holmes had be...\n\n\n16\nThe Adventure of the Devil's Foot\nTHE ADVENTURE OF THE DEVIL'S FOOT In recording...\n\n\n17\nThe Adventure of the Dying Detective\nTHE ADVENTURE OF THE DYING DETECTIVE Mrs. Huds...\n\n\n18\nThe Adventure of the Empty House\nTHE ADVENTURE OF THE EMPTY HOUSE It was in the...\n\n\n19\nThe Adventure of the Engineer's Thumb\nTHE ADVENTURE OF THE ENGINEER'S THUMB Of all t...\n\n\n20\nThe Adventure of the Golden Pince-Nez\nTHE ADVENTURE OF THE GOLDEN PINCE-NEZ When I l...\n\n\n21\nThe Adventure of the Missing Three-Quarter\nTHE ADVENTURE OF THE MISSING THREE-QUARTER We ...\n\n\n22\nThe Adventure of the Noble Bachelor\nTHE ADVENTURE OF THE NOBLE BACHELOR The Lord S...\n\n\n23\nThe Adventure of the Norwood Builder\nTHE ADVENTURE OF THE NORWOOD BUILDER \"From the...\n\n\n24\nThe Adventure of the Priory School\nTHE ADVENTURE OF THE PRIORY SCHOOL We have had...\n\n\n25\nThe Adventure of the Red Circle\nTHE ADVENTURE OF THE RED CIRCLE Table of conte...\n\n\n26\nThe Adventure of the Second Stain\nTHE ADVENTURE OF THE SECOND STAIN I had intend...\n\n\n27\nThe Adventure of the Six Napoleons\nTHE ADVENTURE OF THE SIX NAPOLEONS It was no v...\n\n\n28\nThe Adventure of the Solitary Cyclist\nTHE ADVENTURE OF THE SOLITARY CYCLIST From the...\n\n\n29\nThe Adventure of the Speckled Band\nTHE ADVENTURE OF THE SPECKLED BAND On glancing...\n\n\n30\nThe Adventure of the Three Students\nTHE ADVENTURE OF THE THREE STUDENTS It was in ...\n\n\n31\nThe Boscombe Valley Mystery\nTHE BOSCOMBE VALLEY MYSTERY We were seated at ...\n\n\n32\nThe Crooked Man\nTHE CROOKED MAN One summer night, a few months...\n\n\n33\nThe Disappearance of Lady Frances Carfax\nTHE DISAPPEARANCE OF LADY FRANCES CARFAX \"But ...\n\n\n34\nThe Final Problem\nTHE FINAL PROBLEM It is with a heavy heart tha...\n\n\n35\nThe Five Orange Pips\nTHE FIVE ORANGE PIPS When I glance over my not...\n\n\n36\nThe Greek Interpreter\nTHE GREEK INTERPRETER During my long and intim...\n\n\n37\nThe Hound of the Baskervilles\nTHE HOUND OF THE BASKERVILLES Table of content...\n\n\n38\nThe Man with the Twisted Lip\nTHE MAN WITH THE TWISTED LIP Isa Whitney, brot...\n\n\n39\nThe Musgrave Ritual\nTHE MUSGRAVE RITUAL An anomaly which often str...\n\n\n40\nThe Naval Treaty\nTHE NAVAL TREATY The July which immediately su...\n\n\n41\nThe Red-Headed League\nTHE RED-HEADED LEAGUE I had called upon my fri...\n\n\n42\nThe Reigate Squires\nTHE REIGATE SQUIRES It was some time before th...\n\n\n43\nThe Resident Patient\nTHE RESIDENT PATIENT Glancing over the somewha...\n\n\n44\nThe Sign of the Four\nTHE SIGN OF THE FOUR Table of contents The Sci...\n\n\n45\nThe Stock-Broker's Clerk\nTHE STOCK-BROKER'S CLERK Shortly after my marr...\n\n\n46\nThe Valley Of Fear\nTHE VALLEY OF FEAR Table of contents Part I Th...\n\n\n47\nThe Yellow Face\nTHE YELLOW FACE [In publishing these short ske...\n\n\n\n\n\n\n\n\nconcatenated['text'].str.count('Holmes').sort_values(ascending=False)\n\n37    194\n46    153\n44    137\n2      98\n24     86\n26     80\n40     67\n23     64\n8      62\n12     62\n15     61\n16     59\n27     58\n20     58\n29     56\n7      56\n6      55\n33     54\n41     53\n42     53\n21     52\n4      51\n17     50\n30     49\n28     49\n1      48\n31     47\n0      46\n14     43\n9      41\n18     40\n43     39\n11     38\n22     34\n25     31\n34     30\n38     29\n36     29\n13     28\n10     28\n45     26\n35     25\n3      24\n47     19\n19     14\n32     11\n5       8\n39      5\nName: text, dtype: int64\n\n\n\nimport re\n\ndef regex_sent_tokenize(text):\n    return re.split(r'(?&lt;=[.!?])\\s+', text)\n\nconcatenated['sentences'] = concatenated['text'].apply(regex_sent_tokenize)\nsentences_df = concatenated['sentences'].apply(pd.Series)\nresult1 = pd.concat([concatenated[['book']], sentences_df], axis=1)\n\n\nfrom spacy.cli import download\n\ndownload(\"en_core_web_sm\")   # installs the model\nnlp1 = spacy.load(\"en_core_web_sm\")  # load it after install\n\n\n✔ Download and installation successful\n\nYou can now load the package via spacy.load('en_core_web_sm')\n\n⚠ Restart to reload dependencies\n\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\n\norder to load all the package's dependencies. You can do this by selecting the\n\n'Restart kernel' or 'Restart runtime' option.\n\n\n\n\n\nnlp = spacy.load(\"en_core_web_sm\") \ndef extract_entities(sentence):\n    doc = nlp(sentence)\n    return [(ent.text, ent.label_) for ent in doc.ents]\n\n# Apply to your DataFrame\nconcatenated['entities'] = concatenated['text'].apply(extract_entities)\nconcatenated.head()\n\n\n\n\n\n\n\n\nbook\ntext\nsentences\nentities\n\n\n\n\n0\nA Case of Identity\nA CASE OF IDENTITY \"My dear fellow,\" said Sher...\n[A CASE OF IDENTITY \"My dear fellow,\" said She...\n[(IDENTITY, ORG), (Holmes, PERSON), (Baker Str...\n\n\n1\nA Scandal in Bohemia\nA SCANDAL IN BOHEMIA Table of contents Chapter...\n[A SCANDAL IN BOHEMIA Table of contents Chapte...\n[(Chapter 1 Chapter 2 Chapter 3 CHAPTER, LAW),...\n\n\n2\nA Study In Scarlet\nA STUDY IN SCARLET Table of contents Part I Mr...\n[A STUDY IN SCARLET Table of contents Part I M...\n[(Holmes The Science Of Deduction, PERSON), (J...\n\n\n3\nHis Last Bow\nHIS LAST BOW An Epilogue of Sherlock Holmes It...\n[HIS LAST BOW An Epilogue of Sherlock Holmes I...\n[(Holmes, PERSON), (nine o'clock, TIME), (seco...\n\n\n4\nSilver Blaze\nSILVER BLAZE \"I am afraid, Watson, that I shal...\n[SILVER BLAZE \"I am afraid, Watson, that I sha...\n[(Watson, PERSON), (Holmes, PERSON), (one morn...\n\n\n\n\n\n\n\n\n# Keep only entities labeled as PERSON\nconcatenated['persons'] = concatenated['entities'].apply(\n    lambda ents: [text for text, label in ents if label == \"PERSON\"]\n)\n\n\nconcatenated.head()\n\n\n\n\n\n\n\n\nbook\ntext\nsentences\nentities\npersons\n\n\n\n\n0\nA Case of Identity\nA CASE OF IDENTITY \"My dear fellow,\" said Sher...\n[A CASE OF IDENTITY \"My dear fellow,\" said She...\n[(IDENTITY, ORG), (Holmes, PERSON), (Baker Str...\n[Holmes, Holmes, Holmes, Dundas, Irene Adler, ...\n\n\n1\nA Scandal in Bohemia\nA SCANDAL IN BOHEMIA Table of contents Chapter...\n[A SCANDAL IN BOHEMIA Table of contents Chapte...\n[(Chapter 1 Chapter 2 Chapter 3 CHAPTER, LAW),...\n[Holmes, Irene Adler, Irene Adler, Holmes, Hol...\n\n\n2\nA Study In Scarlet\nA STUDY IN SCARLET Table of contents Part I Mr...\n[A STUDY IN SCARLET Table of contents Part I M...\n[(Holmes The Science Of Deduction, PERSON), (J...\n[Holmes The Science Of Deduction, John Rance, ...\n\n\n3\nHis Last Bow\nHIS LAST BOW An Epilogue of Sherlock Holmes It...\n[HIS LAST BOW An Epilogue of Sherlock Holmes I...\n[(Holmes, PERSON), (nine o'clock, TIME), (seco...\n[Holmes, Von Bork, Von Bork, Von Herling, Von ...\n\n\n4\nSilver Blaze\nSILVER BLAZE \"I am afraid, Watson, that I shal...\n[SILVER BLAZE \"I am afraid, Watson, that I sha...\n[(Watson, PERSON), (Holmes, PERSON), (one morn...\n[Watson, Holmes, Dartmoor, Watson, Exeter, Hol...\n\n\n\n\n\n\n\n\nlen(set(concatenated['persons'][0]))\n\n21\n\n\n\nfrom collections import Counter\nconcatenated['unique_persons'] = concatenated['persons'].apply(lambda x: Counter(x))\n\n\nconcatenated\n\n\n\n\n\n\n\n\nbook\ntext\nsentences\nentities\npersons\nunique_persons\n\n\n\n\n0\nA Case of Identity\nA CASE OF IDENTITY \"My dear fellow,\" said Sher...\n[A CASE OF IDENTITY \"My dear fellow,\" said She...\n[(IDENTITY, ORG), (Holmes, PERSON), (Baker Str...\n[Holmes, Holmes, Holmes, Dundas, Irene Adler, ...\n{'Holmes': 45, 'Dundas': 1, 'Irene Adler': 2, ...\n\n\n1\nA Scandal in Bohemia\nA SCANDAL IN BOHEMIA Table of contents Chapter...\n[A SCANDAL IN BOHEMIA Table of contents Chapte...\n[(Chapter 1 Chapter 2 Chapter 3 CHAPTER, LAW),...\n[Holmes, Irene Adler, Irene Adler, Holmes, Hol...\n{'Holmes': 43, 'Irene Adler': 11, 'Atkinson': ...\n\n\n2\nA Study In Scarlet\nA STUDY IN SCARLET Table of contents Part I Mr...\n[A STUDY IN SCARLET Table of contents Part I M...\n[(Holmes The Science Of Deduction, PERSON), (J...\n[Holmes The Science Of Deduction, John Rance, ...\n{'Holmes The Science Of Deduction': 1, 'John R...\n\n\n3\nHis Last Bow\nHIS LAST BOW An Epilogue of Sherlock Holmes It...\n[HIS LAST BOW An Epilogue of Sherlock Holmes I...\n[(Holmes, PERSON), (nine o'clock, TIME), (seco...\n[Holmes, Von Bork, Von Bork, Von Herling, Von ...\n{'Holmes': 20, 'Von Bork': 31, 'Von Herling': ...\n\n\n4\nSilver Blaze\nSILVER BLAZE \"I am afraid, Watson, that I shal...\n[SILVER BLAZE \"I am afraid, Watson, that I sha...\n[(Watson, PERSON), (Holmes, PERSON), (one morn...\n[Watson, Holmes, Dartmoor, Watson, Exeter, Hol...\n{'Watson': 13, 'Holmes': 50, 'Dartmoor': 2, 'E...\n\n\n5\nThe \"Gloria Scott\"\nTHE \"GLORIA SCOTT\" \"I have some papers here,\" ...\n[THE \"GLORIA SCOTT\" \"I have some papers here,\"...\n[(THE \"GLORIA SCOTT, LAW), (Holmes, PERSON), (...\n[Holmes, Watson, Hudson, Holmes, Victor Trevor...\n{'Holmes': 8, 'Watson': 7, 'Hudson': 13, 'Vict...\n\n\n6\nThe Adventure of Black Peter\nTHE ADVENTURE OF BLACK PETER I have never know...\n[THE ADVENTURE OF BLACK PETER I have never kno...\n[(the year '95, DATE), (Baker Street, FAC), (H...\n[Holmes, Cardinal Tosca, His Holiness, Woodman...\n{'Holmes': 51, 'Cardinal Tosca': 1, 'His Holin...\n\n\n7\nThe Adventure of Charles Augustus Milverton\nTHE ADVENTURE OF CHARLES AUGUSTUS MILVERTON It...\n[THE ADVENTURE OF CHARLES AUGUSTUS MILVERTON I...\n[(CHARLES AUGUSTUS MILVERTON, PERSON), (years,...\n[CHARLES AUGUSTUS MILVERTON, Sherlock Holmes, ...\n{'CHARLES AUGUSTUS MILVERTON': 1, 'Sherlock Ho...\n\n\n8\nThe Adventure of Wisteria Lodge\nTHE ADVENTURE OF WISTERIA LODGE Table of conte...\n[THE ADVENTURE OF WISTERIA LODGE Table of cont...\n[(The Singular Experience, ORG), (John Scott E...\n[John Scott Eccles, John Scott Eccles, Holmes,...\n{'John Scott Eccles': 3, 'Holmes': 59, 'Watson...\n\n\n9\nThe Adventure of the Abbey Grange\nTHE ADVENTURE OF THE ABBEY GRANGE It was on a ...\n[THE ADVENTURE OF THE ABBEY GRANGE It was on a...\n[(morning, TIME), (the winter of '97, DATE), (...\n[Holmes, Watson, Holmes, Holmes, Abbey Grange,...\n{'Holmes': 40, 'Watson': 18, 'Abbey Grange': 1...\n\n\n10\nThe Adventure of the Beryl Coronet\nTHE ADVENTURE OF THE BERYL CORONET \"Holmes,\" s...\n[THE ADVENTURE OF THE BERYL CORONET \"Holmes,\" ...\n[(one morning, TIME), (February morning, TIME)...\n[Holmes, Holmes, Holmes, Holmes, Alexander Hol...\n{'Holmes': 27, 'Alexander Holder': 1, 'Holder'...\n\n\n11\nThe Adventure of the Blue Carbuncle\nTHE ADVENTURE OF THE BLUE CARBUNCLE I had call...\n[THE ADVENTURE OF THE BLUE CARBUNCLE I had cal...\n[(Holmes, PERSON), (the second morning, TIME),...\n[Holmes, one\"--he, Sherlock Holmes, Irene Adle...\n{'Holmes': 37, 'one\"--he': 1, 'Sherlock Holmes...\n\n\n12\nThe Adventure of the Bruce-Partington Plans\nTHE ADVENTURE OF THE BRUCE-PARTINGTON PLANS In...\n[THE ADVENTURE OF THE BRUCE-PARTINGTON PLANS I...\n[(the third week of November, DATE), (the year...\n[Holmes, Watson, Holmes, Holmes, Watson, Holme...\n{'Holmes': 50, 'Watson': 22, 'Jove': 2, 'Mycro...\n\n\n13\nThe Adventure of the Cardboard Box\nTHE ADVENTURE OF THE CARDBOARD BOX In choosing...\n[THE ADVENTURE OF THE CARDBOARD BOX In choosin...\n[(Holmes, PERSON), (August, DATE), (Baker Stre...\n[Holmes, Holmes, Holmes, Watson, Holmes, Poe, ...\n{'Holmes': 28, 'Watson': 9, 'Poe': 1, 'Gordon'...\n\n\n14\nThe Adventure of the Copper Beeches\nTHE ADVENTURE OF THE COPPER BEECHES \"To the ma...\n[THE ADVENTURE OF THE COPPER BEECHES \"To the m...\n[(Sherlock Holmes, PERSON), (the Daily Telegra...\n[Sherlock Holmes, Watson, Holmes, the King of ...\n{'Sherlock Holmes': 1, 'Watson': 7, 'Holmes': ...\n\n\n15\nThe Adventure of the Dancing Men\nTHE ADVENTURE OF THE DANCING MEN Holmes had be...\n[THE ADVENTURE OF THE DANCING MEN Holmes had b...\n[(Holmes, PERSON), (some hours, TIME), (Watson...\n[Holmes, Watson, Holmes, Watson, Thurston, Wat...\n{'Holmes': 60, 'Watson': 12, 'Thurston': 1, 'H...\n\n\n16\nThe Adventure of the Devil's Foot\nTHE ADVENTURE OF THE DEVIL'S FOOT In recording...\n[THE ADVENTURE OF THE DEVIL'S FOOT In recordin...\n[(Sherlock Holmes, PERSON), (late years, DATE)...\n[Sherlock Holmes, Holmes, Moore Agar, Holmes, ...\n{'Sherlock Holmes': 1, 'Holmes': 58, 'Moore Ag...\n\n\n17\nThe Adventure of the Dying Detective\nTHE ADVENTURE OF THE DYING DETECTIVE Mrs. Huds...\n[THE ADVENTURE OF THE DYING DETECTIVE Mrs., Hu...\n[(Hudson, PERSON), (Holmes, PERSON), (first, O...\n[Hudson, Holmes, Holmes, Watson, Holmes, Watso...\n{'Hudson': 4, 'Holmes': 49, 'Watson': 45, 'gau...\n\n\n18\nThe Adventure of the Empty House\nTHE ADVENTURE OF THE EMPTY HOUSE It was in the...\n[THE ADVENTURE OF THE EMPTY HOUSE It was in th...\n[(the spring of the year 1894, DATE), (London,...\n[Ronald Adair, Holmes, Ronald Adair, Holmes, R...\n{'Ronald Adair': 6, 'Holmes': 37, 'the Earl of...\n\n\n19\nThe Adventure of the Engineer's Thumb\nTHE ADVENTURE OF THE ENGINEER'S THUMB Of all t...\n[THE ADVENTURE OF THE ENGINEER'S THUMB Of all ...\n[(Sherlock Holmes, PERSON), (the years, DATE),...\n[Sherlock Holmes, Hatherley, Warburton, Holmes...\n{'Sherlock Holmes': 3, 'Hatherley': 7, 'Warbur...\n\n\n20\nThe Adventure of the Golden Pince-Nez\nTHE ADVENTURE OF THE GOLDEN PINCE-NEZ When I l...\n[THE ADVENTURE OF THE GOLDEN PINCE-NEZ When I ...\n[(three, CARDINAL), (the year 1894, DATE), (Cr...\n[Crosby, Holmes, Willoughby Smith, Holmes, Wat...\n{'Crosby': 1, 'Holmes': 55, 'Willoughby Smith'...\n\n\n21\nThe Adventure of the Missing Three-Quarter\nTHE ADVENTURE OF THE MISSING THREE-QUARTER We ...\n[THE ADVENTURE OF THE MISSING THREE-QUARTER We...\n[(Baker Street, FAC), (February morning, TIME)...\n[Sherlock Holmes, Overton, Holmes, Overton, Ho...\n{'Sherlock Holmes': 3, 'Overton': 8, 'Holmes':...\n\n\n22\nThe Adventure of the Noble Bachelor\nTHE ADVENTURE OF THE NOBLE BACHELOR The Lord S...\n[THE ADVENTURE OF THE NOBLE BACHELOR The Lord ...\n[(Simon, PERSON), (four-year-old, DATE), (Holm...\n[Simon, Holmes, Holmes, Watson, Sherlock Holme...\n{'Simon': 8, 'Holmes': 32, 'Watson': 6, 'Sherl...\n\n\n23\nThe Adventure of the Norwood Builder\nTHE ADVENTURE OF THE NORWOOD BUILDER \"From the...\n[THE ADVENTURE OF THE NORWOOD BUILDER \"From th...\n[(Sherlock Holmes, PERSON), (London, GPE), (Mo...\n[Sherlock Holmes, Moriarty, Watson, Holmes, Ve...\n{'Sherlock Holmes': 6, 'Moriarty': 1, 'Watson'...\n\n\n24\nThe Adventure of the Priory School\nTHE ADVENTURE OF THE PRIORY SCHOOL We have had...\n[THE ADVENTURE OF THE PRIORY SCHOOL We have ha...\n[(Baker Street, FAC), (first, ORDINAL), (Thorn...\n[Thorneycroft Huxtable, Holmes, Watson, Holmes...\n{'Thorneycroft Huxtable': 2, 'Holmes': 82, 'Wa...\n\n\n25\nThe Adventure of the Red Circle\nTHE ADVENTURE OF THE RED CIRCLE Table of conte...\n[THE ADVENTURE OF THE RED CIRCLE Table of cont...\n[(THE RED CIRCLE Table, ORG), (One, CARDINAL),...\n[Warren, Holmes, Fairdale Hobbs, Holmes, Warre...\n{'Warren': 22, 'Holmes': 30, 'Fairdale Hobbs':...\n\n\n26\nThe Adventure of the Second Stain\nTHE ADVENTURE OF THE SECOND STAIN I had intend...\n[THE ADVENTURE OF THE SECOND STAIN I had inten...\n[(SECOND, ORDINAL), (The Adventure of the Abbe...\n[Sherlock Holmes, Holmes, Holmes, Holmes, Trel...\n{'Sherlock Holmes': 1, 'Holmes': 77, 'Trelawne...\n\n\n27\nThe Adventure of the Six Napoleons\nTHE ADVENTURE OF THE SIX NAPOLEONS It was no v...\n[THE ADVENTURE OF THE SIX NAPOLEONS It was no ...\n[(SIX, CARDINAL), (Lestrade, PERSON), (Scotlan...\n[Lestrade, Sherlock Holmes, Lestrade, Holmes, ...\n{'Lestrade': 33, 'Sherlock Holmes': 7, 'Holmes...\n\n\n28\nThe Adventure of the Solitary Cyclist\nTHE ADVENTURE OF THE SOLITARY CYCLIST From the...\n[THE ADVENTURE OF THE SOLITARY CYCLIST From th...\n[(the years 1894 to 1901, DATE), (Sherlock Hol...\n[Sherlock Holmes, Holmes, John Vincent Harden,...\n{'Sherlock Holmes': 2, 'Holmes': 46, 'John Vin...\n\n\n29\nThe Adventure of the Speckled Band\nTHE ADVENTURE OF THE SPECKLED BAND On glancing...\n[THE ADVENTURE OF THE SPECKLED BAND On glancin...\n[(the last eight years, DATE), (Holmes, PERSON...\n[Holmes, Surrey, Holmes, Grimesby Roylott, Hol...\n{'Holmes': 54, 'Surrey': 3, 'Grimesby Roylott'...\n\n\n30\nThe Adventure of the Three Students\nTHE ADVENTURE OF THE THREE STUDENTS It was in ...\n[THE ADVENTURE OF THE THREE STUDENTS It was in...\n[(THE THREE STUDENTS, ORG), (the year '95, DAT...\n[Sherlock Holmes, Holmes, Hilton Soames, Soame...\n{'Sherlock Holmes': 1, 'Holmes': 46, 'Hilton S...\n\n\n31\nThe Boscombe Valley Mystery\nTHE BOSCOMBE VALLEY MYSTERY We were seated at ...\n[THE BOSCOMBE VALLEY MYSTERY We were seated at...\n[(one morning, TIME), (Holmes, PERSON), (a cou...\n[Holmes, Sherlock Holmes, Holmes, gaunt, Watso...\n{'Holmes': 44, 'Sherlock Holmes': 2, 'gaunt': ...\n\n\n32\nThe Crooked Man\nTHE CROOKED MAN One summer night, a few months...\n[THE CROOKED MAN One summer night, a few month...\n[(summer night, TIME), (a few months, DATE), (...\n[Holmes, Watson, Watson, Waterloo, Holmes, Wat...\n{'Holmes': 11, 'Watson': 14, 'Waterloo': 2, 'E...\n\n\n33\nThe Disappearance of Lady Frances Carfax\nTHE DISAPPEARANCE OF LADY FRANCES CARFAX \"But ...\n[THE DISAPPEARANCE OF LADY FRANCES CARFAX \"But...\n[(Turkish, NORP), (Sherlock Holmes, PERSON), (...\n[Sherlock Holmes, Holmes, Holmes, Watson, Holm...\n{'Sherlock Holmes': 2, 'Holmes': 50, 'Watson':...\n\n\n34\nThe Final Problem\nTHE FINAL PROBLEM It is with a heavy heart tha...\n[THE FINAL PROBLEM It is with a heavy heart th...\n[(Sherlock Holmes, PERSON), (first, ORDINAL), ...\n[Sherlock Holmes, James Moriarty, Moriarty, Sh...\n{'Sherlock Holmes': 2, 'James Moriarty': 1, 'M...\n\n\n35\nThe Five Orange Pips\nTHE FIVE ORANGE PIPS When I glance over my not...\n[THE FIVE ORANGE PIPS When I glance over my no...\n[(FIVE, CARDINAL), (Holmes, PERSON), (82, CARD...\n[Holmes, Anderson, Holmes, Holmes, Clark Russe...\n{'Holmes': 24, 'Anderson': 1, 'Clark Russell's...\n\n\n36\nThe Greek Interpreter\nTHE GREEK INTERPRETER During my long and intim...\n[THE GREEK INTERPRETER During my long and inti...\n[(THE GREEK INTERPRETER During, ORG), (Sherloc...\n[Sherlock Holmes, Art, Mycroft, Holmes, Watson...\n{'Sherlock Holmes': 2, 'Art': 1, 'Mycroft': 12...\n\n\n37\nThe Hound of the Baskervilles\nTHE HOUND OF THE BASKERVILLES Table of content...\n[THE HOUND OF THE BASKERVILLES Table of conten...\n[(Holmes The Curse, PERSON), (Henry Baskervill...\n[Holmes The Curse, Henry Baskerville, Watson S...\n{'Holmes The Curse': 1, 'Henry Baskerville': 2...\n\n\n38\nThe Man with the Twisted Lip\nTHE MAN WITH THE TWISTED LIP Isa Whitney, brot...\n[THE MAN WITH THE TWISTED LIP Isa Whitney, bro...\n[(Elias Whitney, PERSON), (D.D., GPE), (Princi...\n[Elias Whitney, Principal, De Quincey's, Kate ...\n{'Elias Whitney': 1, 'Principal': 1, 'De Quinc...\n\n\n39\nThe Musgrave Ritual\nTHE MUSGRAVE RITUAL An anomaly which often str...\n[THE MUSGRAVE RITUAL An anomaly which often st...\n[(Holmes, PERSON), (one, CARDINAL), (Afghanist...\n[Holmes, Bohemianism, Holmes, V. R., Watson, W...\n{'Holmes': 5, 'Bohemianism': 1, 'V. R.': 1, 'W...\n\n\n40\nThe Naval Treaty\nTHE NAVAL TREATY The July which immediately su...\n[THE NAVAL TREATY The July which immediately s...\n[(THE NAVAL TREATY, ORG), (July, DATE), (three...\n[Holmes, Holmes, Fritz von Waldbaum, Percy Phe...\n{'Holmes': 64, 'Fritz von Waldbaum': 1, 'Percy...\n\n\n41\nThe Red-Headed League\nTHE RED-HEADED LEAGUE I had called upon my fri...\n[THE RED-HEADED LEAGUE I had called upon my fr...\n[(THE RED-HEADED LEAGUE I, ORG), (Sherlock Hol...\n[Sherlock Holmes, Holmes, Watson, Wilson, Holm...\n{'Sherlock Holmes': 4, 'Holmes': 48, 'Watson':...\n\n\n42\nThe Reigate Squires\nTHE REIGATE SQUIRES It was some time before th...\n[THE REIGATE SQUIRES It was some time before t...\n[(Sherlock Holmes, PERSON), (the spring of '87...\n[Sherlock Holmes, Maupertuis, Holmes, Hayter, ...\n{'Sherlock Holmes': 2, 'Maupertuis': 1, 'Holme...\n\n\n43\nThe Resident Patient\nTHE RESIDENT PATIENT Glancing over the somewha...\n[THE RESIDENT PATIENT Glancing over the somewh...\n[(Memoirs, ORG), (Sherlock Holmes, PERSON), (H...\n[Sherlock Holmes, Holmes, Charybdis, Holmes, H...\n{'Sherlock Holmes': 2, 'Holmes': 37, 'Charybdi...\n\n\n44\nThe Sign of the Four\nTHE SIGN OF THE FOUR Table of contents The Sci...\n[THE SIGN OF THE FOUR Table of contents The Sc...\n[(THE FOUR Table, ORG), (The Science of Deduct...\n[Holmes Gives, Jonathan Small, Holmes, Watson,...\n{'Holmes Gives': 2, 'Jonathan Small': 17, 'Hol...\n\n\n45\nThe Stock-Broker's Clerk\nTHE STOCK-BROKER'S CLERK Shortly after my marr...\n[THE STOCK-BROKER'S CLERK Shortly after my mar...\n[(Paddington, ORG), (Farquhar, PERSON), (one, ...\n[Farquhar, Holmes, Watson, Watson, Holmes, Hol...\n{'Farquhar': 1, 'Holmes': 21, 'Watson': 12, 'H...\n\n\n46\nThe Valley Of Fear\nTHE VALLEY OF FEAR Table of contents Part I Th...\n[THE VALLEY OF FEAR Table of contents Part I T...\n[(Holmes Discourses, PERSON), (The Tragedy, PE...\n[Holmes Discourses, The Tragedy, Vermissa The ...\n{'Holmes Discourses': 2, 'The Tragedy': 1, 'Ve...\n\n\n47\nThe Yellow Face\nTHE YELLOW FACE [In publishing these short ske...\n[THE YELLOW FACE [In publishing these short sk...\n[(some half-dozen, CARDINAL), (The Adventure o...\n[Holmes, Holmes, I., Holmes, Watson, Holmes, I...\n{'Holmes': 19, 'I.': 2, 'Watson': 4, 'Grosveno...\n\n\n\n\n\n\n\n\nconcatenated['places'] = concatenated['entities'].apply(\n    lambda ents: [text for text, label in ents if label == \"GPE\"]\n)\n\n\nconcatenated['persons_sorted'] = concatenated['unique_persons'].apply(\n    lambda d: dict(sorted(d.items(), key=lambda item: item[1], reverse=True))\n)\n\n\nconcatenated['unique_person_count'] = concatenated['persons'].apply(lambda x: len(set(x)))\nconcatenated['unique_places_count'] = concatenated['places'].apply(lambda x: len(set(x)))\n\n\nconcatenated.sort_values(['unique_person_count', 'unique_places_count'], inplace=True, ascending=False)\n\n\nconcatenated.reset_index(drop=True, inplace=True)\nconcatenated\n\n\n\n\n\n\n\n\nbook\ntext\nsentences\nentities\npersons\nunique_persons\nplaces\npersons_sorted\nunique_person_count\nunique_places_count\n\n\n\n\n0\nThe Valley Of Fear\nTHE VALLEY OF FEAR Table of contents Part I Th...\n[THE VALLEY OF FEAR Table of contents Part I T...\n[(Holmes Discourses, PERSON), (The Tragedy, PE...\n[Holmes Discourses, The Tragedy, Vermissa The ...\n{'Holmes Discourses': 2, 'The Tragedy': 1, 'Ve...\n[Porlock, Asteroid, Porlock, Porlock, Machiave...\n{'McMurdo': 186, 'Holmes': 147, 'Douglas': 97,...\n188\n51\n\n\n1\nThe Sign of the Four\nTHE SIGN OF THE FOUR Table of contents The Sci...\n[THE SIGN OF THE FOUR Table of contents The Sc...\n[(THE FOUR Table, ORG), (The Science of Deduct...\n[Holmes Gives, Jonathan Small, Holmes, Watson,...\n{'Holmes Gives': 2, 'Jonathan Small': 17, 'Hol...\n[morocco, Jezail, St. Louis, Paris, England, E...\n{'Holmes': 118, 'Sholto': 42, 'Jones': 29, 'Mo...\n124\n45\n\n\n2\nA Study In Scarlet\nA STUDY IN SCARLET Table of contents Part I Mr...\n[A STUDY IN SCARLET Table of contents Part I M...\n[(Holmes The Science Of Deduction, PERSON), (J...\n[Holmes The Science Of Deduction, John Rance, ...\n{'Holmes The Science Of Deduction': 1, 'John R...\n[M.D., M.D., Netley, India, Bombay, Maiwand, J...\n{'Holmes': 79, 'Drebber': 51, 'Lestrade': 46, ...\n112\n53\n\n\n3\nThe Hound of the Baskervilles\nTHE HOUND OF THE BASKERVILLES Table of content...\n[THE HOUND OF THE BASKERVILLES Table of conten...\n[(Holmes The Curse, PERSON), (Henry Baskervill...\n[Holmes The Curse, Henry Baskerville, Watson S...\n{'Holmes The Curse': 1, 'Henry Baskerville': 2...\n[Penang, M.R.C.S., C.C., London, M.R.C.S., Lon...\n{'Holmes': 176, 'Henry': 130, 'Watson': 110, '...\n101\n51\n\n\n4\nThe Adventure of Wisteria Lodge\nTHE ADVENTURE OF WISTERIA LODGE Table of conte...\n[THE ADVENTURE OF WISTERIA LODGE Table of cont...\n[(The Singular Experience, ORG), (John Scott E...\n[John Scott Eccles, John Scott Eccles, Holmes,...\n{'John Scott Eccles': 3, 'Holmes': 59, 'Watson...\n[Esher, Kensington, London, London, Spain, Oxs...\n{'Holmes': 59, 'Garcia': 29, 'Watson': 19, 'Sc...\n49\n14\n\n\n5\nThe Adventure of the Priory School\nTHE ADVENTURE OF THE PRIORY SCHOOL We have had...\n[THE ADVENTURE OF THE PRIORY SCHOOL We have ha...\n[(Baker Street, FAC), (first, ORDINAL), (Thorn...\n[Thorneycroft Huxtable, Holmes, Watson, Holmes...\n{'Thorneycroft Huxtable': 2, 'Holmes': 82, 'Wa...\n[M.A., colour, Mackleton, Mackleton, London, K...\n{'Holmes': 82, 'Watson': 25, 'Grace': 19, 'Hux...\n39\n15\n\n\n6\nThe Adventure of the Empty House\nTHE ADVENTURE OF THE EMPTY HOUSE It was in the...\n[THE ADVENTURE OF THE EMPTY HOUSE It was in th...\n[(the spring of the year 1894, DATE), (London,...\n[Ronald Adair, Holmes, Ronald Adair, Holmes, R...\n{'Ronald Adair': 6, 'Holmes': 37, 'the Earl of...\n[London, Australia, Carstairs, Bagatelle, Kens...\n{'Holmes': 37, 'Watson': 21, 'Moran': 14, 'Les...\n36\n21\n\n\n7\nThe Adventure of the Red Circle\nTHE ADVENTURE OF THE RED CIRCLE Table of conte...\n[THE ADVENTURE OF THE RED CIRCLE Table of cont...\n[(THE RED CIRCLE Table, ORG), (One, CARDINAL),...\n[Warren, Holmes, Fairdale Hobbs, Holmes, Warre...\n{'Warren': 22, 'Holmes': 30, 'Fairdale Hobbs':...\n[London, groans, Brixton, London, London, Wats...\n{'Holmes': 30, 'Watson': 26, 'Warren': 22, 'Ge...\n36\n11\n\n\n8\nThe Adventure of the Blue Carbuncle\nTHE ADVENTURE OF THE BLUE CARBUNCLE I had call...\n[THE ADVENTURE OF THE BLUE CARBUNCLE I had cal...\n[(Holmes, PERSON), (the second morning, TIME),...\n[Holmes, one\"--he, Sherlock Holmes, Irene Adle...\n{'Holmes': 37, 'one\"--he': 1, 'Sherlock Holmes...\n[morocco, St. James's, Echo, China, crisply, H...\n{'Holmes': 37, 'Peterson': 11, 'Horner': 10, '...\n35\n8\n\n\n9\nThe Five Orange Pips\nTHE FIVE ORANGE PIPS When I glance over my not...\n[THE FIVE ORANGE PIPS When I glance over my no...\n[(FIVE, CARDINAL), (Holmes, PERSON), (82, CARD...\n[Holmes, Anderson, Holmes, Holmes, Clark Russe...\n{'Holmes': 24, 'Anderson': 1, 'Clark Russell's...\n[Uffa, London, America, Florida, Sussex, Horsh...\n{'Holmes': 24, 'Horsham': 8, 'Openshaw': 5, 'J...\n34\n21\n\n\n10\nThe Adventure of the Bruce-Partington Plans\nTHE ADVENTURE OF THE BRUCE-PARTINGTON PLANS In...\n[THE ADVENTURE OF THE BRUCE-PARTINGTON PLANS I...\n[(the third week of November, DATE), (the year...\n[Holmes, Watson, Holmes, Holmes, Watson, Holme...\n{'Holmes': 50, 'Watson': 22, 'Jove': 2, 'Mycro...\n[London, London, London, India, Canada, London...\n{'Holmes': 50, 'Watson': 22, 'Mycroft': 20, 'L...\n34\n13\n\n\n11\nThe Disappearance of Lady Frances Carfax\nTHE DISAPPEARANCE OF LADY FRANCES CARFAX \"But ...\n[THE DISAPPEARANCE OF LADY FRANCES CARFAX \"But...\n[(Turkish, NORP), (Sherlock Holmes, PERSON), (...\n[Sherlock Holmes, Holmes, Holmes, Watson, Holm...\n{'Sherlock Holmes': 2, 'Holmes': 50, 'Watson':...\n[Rufton, Lausanne, Lausanne, London, Lausanne,...\n{'Holmes': 50, 'Watson': 23, 'Peters': 10, 'Sh...\n34\n9\n\n\n12\nThe Boscombe Valley Mystery\nTHE BOSCOMBE VALLEY MYSTERY We were seated at ...\n[THE BOSCOMBE VALLEY MYSTERY We were seated at...\n[(one morning, TIME), (Holmes, PERSON), (a cou...\n[Holmes, Sherlock Holmes, Holmes, gaunt, Watso...\n{'Holmes': 44, 'Sherlock Holmes': 2, 'gaunt': ...\n[England, Boscombe Valley, Afghanistan, Herefo...\n{'Holmes': 44, 'McCarthy': 32, 'Lestrade': 24,...\n33\n13\n\n\n13\nSilver Blaze\nSILVER BLAZE \"I am afraid, Watson, that I shal...\n[SILVER BLAZE \"I am afraid, Watson, that I sha...\n[(Watson, PERSON), (Holmes, PERSON), (one morn...\n[Watson, Holmes, Dartmoor, Watson, Exeter, Hol...\n{'Watson': 13, 'Holmes': 50, 'Dartmoor': 2, 'E...\n[England, Paddington, England, Mapleton, Bayar...\n{'Holmes': 50, 'Ross': 17, 'Watson': 13, 'John...\n33\n12\n\n\n14\nThe Man with the Twisted Lip\nTHE MAN WITH THE TWISTED LIP Isa Whitney, brot...\n[THE MAN WITH THE TWISTED LIP Isa Whitney, bro...\n[(Elias Whitney, PERSON), (D.D., GPE), (Princi...\n[Elias Whitney, Principal, De Quincey's, Kate ...\n{'Elias Whitney': 1, 'Principal': 1, 'De Quinc...\n[D.D., Capital, London, Middlesex, Neville, Ch...\n{'Holmes': 28, 'Watson': 15, 'Neville St. Clai...\n32\n6\n\n\n15\nThe Adventure of the Noble Bachelor\nTHE ADVENTURE OF THE NOBLE BACHELOR The Lord S...\n[THE ADVENTURE OF THE NOBLE BACHELOR The Lord ...\n[(Simon, PERSON), (four-year-old, DATE), (Holm...\n[Simon, Holmes, Holmes, Watson, Sherlock Holme...\n{'Simon': 8, 'Holmes': 32, 'Watson': 6, 'Sherl...\n[Jezail, England, Tudor, Esq, San Francisco, C...\n{'Holmes': 32, 'Frank': 19, 'Lestrade': 14, 'S...\n31\n19\n\n\n16\nThe Adventure of the Solitary Cyclist\nTHE ADVENTURE OF THE SOLITARY CYCLIST From the...\n[THE ADVENTURE OF THE SOLITARY CYCLIST From th...\n[(the years 1894 to 1901, DATE), (Sherlock Hol...\n[Sherlock Holmes, Holmes, John Vincent Harden,...\n{'Sherlock Holmes': 2, 'Holmes': 46, 'John Vin...\n[South Africa, Johannesburg, London, Tudor, Lo...\n{'Holmes': 46, 'Woodley': 22, 'Carruthers': 16...\n31\n7\n\n\n17\nThe Adventure of the Cardboard Box\nTHE ADVENTURE OF THE CARDBOARD BOX In choosing...\n[THE ADVENTURE OF THE CARDBOARD BOX In choosin...\n[(Holmes, PERSON), (August, DATE), (Baker Stre...\n[Holmes, Holmes, Holmes, Watson, Holmes, Poe, ...\n{'Holmes': 28, 'Watson': 9, 'Poe': 1, 'Gordon'...\n[India, Croydon, Croydon, Belfast, Ireland, Be...\n{'Holmes': 28, 'Sarah': 23, 'Lestrade': 22, 'C...\n30\n10\n\n\n18\nThe Adventure of the Missing Three-Quarter\nTHE ADVENTURE OF THE MISSING THREE-QUARTER We ...\n[THE ADVENTURE OF THE MISSING THREE-QUARTER We...\n[(Baker Street, FAC), (February morning, TIME)...\n[Sherlock Holmes, Overton, Holmes, Overton, Ho...\n{'Sherlock Holmes': 3, 'Overton': 8, 'Holmes':...\n[Cambridge, England, Wales, England, Cambridge...\n{'Holmes': 46, 'Godfrey Staunton': 18, 'Godfre...\n29\n12\n\n\n19\nThe Adventure of the Second Stain\nTHE ADVENTURE OF THE SECOND STAIN I had intend...\n[THE ADVENTURE OF THE SECOND STAIN I had inten...\n[(SECOND, ORDINAL), (The Adventure of the Abbe...\n[Sherlock Holmes, Holmes, Holmes, Holmes, Trel...\n{'Sherlock Holmes': 1, 'Holmes': 77, 'Trelawne...\n[London, Britain, England, England, Great Brit...\n{'Holmes': 77, 'Watson': 15, 'Lestrade': 10, '...\n29\n12\n\n\n20\nThe \"Gloria Scott\"\nTHE \"GLORIA SCOTT\" \"I have some papers here,\" ...\n[THE \"GLORIA SCOTT\" \"I have some papers here,\"...\n[(THE \"GLORIA SCOTT, LAW), (Holmes, PERSON), (...\n[Holmes, Watson, Hudson, Holmes, Victor Trevor...\n{'Holmes': 8, 'Watson': 7, 'Hudson': 13, 'Vict...\n[London, Norfolk, Birmingham, New Zealand, Jap...\n{'Trevor': 16, 'Hudson': 13, 'Beddoes': 11, 'H...\n28\n12\n\n\n21\nThe Adventure of the Six Napoleons\nTHE ADVENTURE OF THE SIX NAPOLEONS It was no v...\n[THE ADVENTURE OF THE SIX NAPOLEONS It was no ...\n[(SIX, CARDINAL), (Lestrade, PERSON), (Scotlan...\n[Lestrade, Sherlock Holmes, Lestrade, Holmes, ...\n{'Lestrade': 33, 'Sherlock Holmes': 7, 'Holmes...\n[London, London, London, Pitt Street, Kensingt...\n{'Holmes': 51, 'Lestrade': 33, 'Watson': 13, '...\n28\n12\n\n\n22\nThe Crooked Man\nTHE CROOKED MAN One summer night, a few months...\n[THE CROOKED MAN One summer night, a few month...\n[(summer night, TIME), (a few months, DATE), (...\n[Holmes, Watson, Watson, Waterloo, Holmes, Wat...\n{'Holmes': 11, 'Watson': 14, 'Waterloo': 2, 'E...\n[India, India, Bhurtee, Bhurtee, Darjeeling, P...\n{'Barclay': 21, 'Watson': 14, 'Holmes': 11, 'M...\n28\n6\n\n\n23\nThe Adventure of Black Peter\nTHE ADVENTURE OF BLACK PETER I have never know...\n[THE ADVENTURE OF BLACK PETER I have never kno...\n[(the year '95, DATE), (Baker Street, FAC), (H...\n[Holmes, Cardinal Tosca, His Holiness, Woodman...\n{'Holmes': 51, 'Cardinal Tosca': 1, 'His Holin...\n[London, London, Dundee, Forest Row, Sussex, P...\n{'Holmes': 51, 'Peter Carey': 21, 'Hopkins': 2...\n27\n17\n\n\n24\nA Scandal in Bohemia\nA SCANDAL IN BOHEMIA Table of contents Chapter...\n[A SCANDAL IN BOHEMIA Table of contents Chapte...\n[(Chapter 1 Chapter 2 Chapter 3 CHAPTER, LAW),...\n[Holmes, Irene Adler, Irene Adler, Holmes, Hol...\n{'Holmes': 43, 'Irene Adler': 11, 'Atkinson': ...\n[Holland, Scarlet, London, Egria, Bohemia, Boh...\n{'Holmes': 43, 'Majesty': 17, 'Irene Adler': 1...\n27\n13\n\n\n25\nThe Red-Headed League\nTHE RED-HEADED LEAGUE I had called upon my fri...\n[THE RED-HEADED LEAGUE I had called upon my fr...\n[(THE RED-HEADED LEAGUE I, ORG), (Sherlock Hol...\n[Sherlock Holmes, Holmes, Watson, Wilson, Holm...\n{'Sherlock Holmes': 4, 'Holmes': 48, 'Watson':...\n[China, China, China, China, Lebanon, Pennsylv...\n{'Holmes': 48, 'Wilson': 13, 'Merryweather': 1...\n27\n11\n\n\n26\nThe Naval Treaty\nTHE NAVAL TREATY The July which immediately su...\n[THE NAVAL TREATY The July which immediately s...\n[(THE NAVAL TREATY, ORG), (July, DATE), (three...\n[Holmes, Holmes, Fritz von Waldbaum, Percy Phe...\n{'Holmes': 64, 'Fritz von Waldbaum': 1, 'Percy...\n[Paris, Cambridge, Bunsen, England, Italy, Gre...\n{'Holmes': 64, 'Phelps': 31, 'Watson': 18, 'Jo...\n26\n18\n\n\n27\nThe Stock-Broker's Clerk\nTHE STOCK-BROKER'S CLERK Shortly after my marr...\n[THE STOCK-BROKER'S CLERK Shortly after my mar...\n[(Paddington, ORG), (Farquhar, PERSON), (one, ...\n[Farquhar, Holmes, Watson, Watson, Holmes, Hol...\n{'Farquhar': 1, 'Holmes': 21, 'Watson': 12, 'H...\n[St. Vitus's, Birmingham, Birmingham, Birmingh...\n{'Holmes': 21, 'Mawson': 15, 'Hall Pycroft': 1...\n26\n11\n\n\n28\nThe Adventure of the Copper Beeches\nTHE ADVENTURE OF THE COPPER BEECHES \"To the ma...\n[THE ADVENTURE OF THE COPPER BEECHES \"To the m...\n[(Sherlock Holmes, PERSON), (the Daily Telegra...\n[Sherlock Holmes, Watson, Holmes, the King of ...\n{'Sherlock Holmes': 1, 'Watson': 7, 'Holmes': ...\n[china, Nova Scotia, America, England, Philade...\n{'Holmes': 41, 'Rucastle': 34, 'Hunter': 19, '...\n26\n10\n\n\n29\nHis Last Bow\nHIS LAST BOW An Epilogue of Sherlock Holmes It...\n[HIS LAST BOW An Epilogue of Sherlock Holmes I...\n[(Holmes, PERSON), (nine o'clock, TIME), (seco...\n[Holmes, Von Bork, Von Bork, Von Herling, Von ...\n{'Holmes': 20, 'Von Bork': 31, 'Von Herling': ...\n[Kaiser, London, Berlin, Berlin, England, Engl...\n{'Von Bork': 31, 'Holmes': 20, 'Watson': 16, '...\n25\n19\n\n\n30\nThe Adventure of the Abbey Grange\nTHE ADVENTURE OF THE ABBEY GRANGE It was on a ...\n[THE ADVENTURE OF THE ABBEY GRANGE It was on a...\n[(morning, TIME), (the winter of '97, DATE), (...\n[Holmes, Watson, Holmes, Holmes, Abbey Grange,...\n{'Holmes': 40, 'Watson': 18, 'Abbey Grange': 1...\n[London, Kentish, Marsham, E.B., Sydenham, Lon...\n{'Holmes': 40, 'Watson': 18, 'Lady Brackenstal...\n24\n13\n\n\n31\nThe Adventure of the Golden Pince-Nez\nTHE ADVENTURE OF THE GOLDEN PINCE-NEZ When I l...\n[THE ADVENTURE OF THE GOLDEN PINCE-NEZ When I ...\n[(three, CARDINAL), (the year 1894, DATE), (Cr...\n[Crosby, Holmes, Willoughby Smith, Holmes, Wat...\n{'Crosby': 1, 'Holmes': 55, 'Willoughby Smith'...\n[Huret, London, Abbey, Chatham, Cambridge, Eng...\n{'Holmes': 55, 'Coram': 9, 'Willoughby Smith':...\n24\n10\n\n\n32\nThe Reigate Squires\nTHE REIGATE SQUIRES It was some time before th...\n[THE REIGATE SQUIRES It was some time before t...\n[(Sherlock Holmes, PERSON), (the spring of '87...\n[Sherlock Holmes, Maupertuis, Holmes, Hayter, ...\n{'Sherlock Holmes': 2, 'Maupertuis': 1, 'Holme...\n[Afghanistan, shreds, England, Acton, London]\n{'Holmes': 51, 'Cunningham': 19, 'William': 13...\n24\n5\n\n\n33\nThe Musgrave Ritual\nTHE MUSGRAVE RITUAL An anomaly which often str...\n[THE MUSGRAVE RITUAL An anomaly which often st...\n[(Holmes, PERSON), (one, CARDINAL), (Afghanist...\n[Holmes, Bohemianism, Holmes, V. R., Watson, W...\n{'Holmes': 5, 'Bohemianism': 1, 'V. R.': 1, 'W...\n[Afghanistan, London, Musgraves, Sussex, Brunt...\n{'Musgrave': 15, 'Watson': 13, 'Brunton': 8, '...\n23\n6\n\n\n34\nThe Adventure of the Dancing Men\nTHE ADVENTURE OF THE DANCING MEN Holmes had be...\n[THE ADVENTURE OF THE DANCING MEN Holmes had b...\n[(Holmes, PERSON), (some hours, TIME), (Watson...\n[Holmes, Watson, Holmes, Watson, Thurston, Wat...\n{'Holmes': 60, 'Watson': 12, 'Thurston': 1, 'H...\n[Thurston, Norfolk, the County of Norfolk, Lon...\n{'Holmes': 60, 'Hilton Cubitt': 21, 'Watson': ...\n22\n14\n\n\n35\nThe Adventure of the Speckled Band\nTHE ADVENTURE OF THE SPECKLED BAND On glancing...\n[THE ADVENTURE OF THE SPECKLED BAND On glancin...\n[(the last eight years, DATE), (Holmes, PERSON...\n[Holmes, Surrey, Holmes, Grimesby Roylott, Hol...\n{'Holmes': 54, 'Surrey': 3, 'Grimesby Roylott'...\n[England, England, England, India, England, Cr...\n{'Holmes': 54, 'Roylott': 13, 'Watson': 12, 'M...\n22\n5\n\n\n36\nThe Final Problem\nTHE FINAL PROBLEM It is with a heavy heart tha...\n[THE FINAL PROBLEM It is with a heavy heart th...\n[(Sherlock Holmes, PERSON), (first, ORDINAL), ...\n[Sherlock Holmes, James Moriarty, Moriarty, Sh...\n{'Sherlock Holmes': 2, 'James Moriarty': 1, 'M...\n[Nimes, France, London, London, London, London...\n{'Holmes': 28, 'Watson': 22, 'Moriarty': 11, '...\n21\n16\n\n\n37\nA Case of Identity\nA CASE OF IDENTITY \"My dear fellow,\" said Sher...\n[A CASE OF IDENTITY \"My dear fellow,\" said She...\n[(IDENTITY, ORG), (Holmes, PERSON), (Baker Str...\n[Holmes, Holmes, Holmes, Dundas, Irene Adler, ...\n{'Holmes': 45, 'Dundas': 1, 'Irene Adler': 2, ...\n[Holland, Marseilles, London, Auckland, New Ze...\n{'Holmes': 45, 'Hosmer Angel': 15, 'Windibank'...\n21\n11\n\n\n38\nThe Adventure of the Devil's Foot\nTHE ADVENTURE OF THE DEVIL'S FOOT In recording...\n[THE ADVENTURE OF THE DEVIL'S FOOT In recordin...\n[(Sherlock Holmes, PERSON), (late years, DATE)...\n[Sherlock Holmes, Holmes, Moore Agar, Holmes, ...\n{'Sherlock Holmes': 1, 'Holmes': 58, 'Moore Ag...\n[London, Cornwall, England, London, Cornwall, ...\n{'Holmes': 58, 'Watson': 26, 'Mortimer Tregenn...\n21\n7\n\n\n39\nThe Adventure of the Norwood Builder\nTHE ADVENTURE OF THE NORWOOD BUILDER \"From the...\n[THE ADVENTURE OF THE NORWOOD BUILDER \"From th...\n[(Sherlock Holmes, PERSON), (London, GPE), (Mo...\n[Sherlock Holmes, Moriarty, Watson, Holmes, Ve...\n{'Sherlock Holmes': 6, 'Moriarty': 1, 'Watson'...\n[London, London, Kensington, Friesland, London...\n{'Holmes': 57, 'Lestrade': 51, 'Watson': 23, '...\n20\n5\n\n\n40\nThe Adventure of Charles Augustus Milverton\nTHE ADVENTURE OF CHARLES AUGUSTUS MILVERTON It...\n[THE ADVENTURE OF CHARLES AUGUSTUS MILVERTON I...\n[(CHARLES AUGUSTUS MILVERTON, PERSON), (years,...\n[CHARLES AUGUSTUS MILVERTON, Sherlock Holmes, ...\n{'CHARLES AUGUSTUS MILVERTON': 1, 'Sherlock Ho...\n[London, Milverton, Milverton, Milverton, Lond...\n{'Holmes': 53, 'Milverton': 26, 'Watson': 16, ...\n19\n6\n\n\n41\nThe Greek Interpreter\nTHE GREEK INTERPRETER During my long and intim...\n[THE GREEK INTERPRETER During my long and inti...\n[(THE GREEK INTERPRETER During, ORG), (Sherloc...\n[Sherlock Holmes, Art, Mycroft, Holmes, Watson...\n{'Sherlock Holmes': 2, 'Art': 1, 'Mycroft': 12...\n[England, London, London, India, India, London...\n{'Holmes': 22, 'Melas': 19, 'Mycroft': 12, 'My...\n18\n11\n\n\n42\nThe Adventure of the Dying Detective\nTHE ADVENTURE OF THE DYING DETECTIVE Mrs. Huds...\n[THE ADVENTURE OF THE DYING DETECTIVE Mrs., Hu...\n[(Hudson, PERSON), (Holmes, PERSON), (first, O...\n[Hudson, Holmes, Holmes, Watson, Holmes, Watso...\n{'Hudson': 4, 'Holmes': 49, 'Watson': 45, 'gau...\n[London, Sumatra, London, Formosa, London, whi...\n{'Holmes': 49, 'Watson': 45, 'Culverton Smith'...\n18\n5\n\n\n43\nThe Adventure of the Three Students\nTHE ADVENTURE OF THE THREE STUDENTS It was in ...\n[THE ADVENTURE OF THE THREE STUDENTS It was in...\n[(THE THREE STUDENTS, ORG), (the year '95, DAT...\n[Sherlock Holmes, Holmes, Hilton Soames, Soame...\n{'Sherlock Holmes': 1, 'Holmes': 46, 'Hilton S...\n[St. Luke's, South Africa, Rhodesia]\n{'Holmes': 46, 'Soames': 23, 'Watson': 11, 'Gi...\n15\n3\n\n\n44\nThe Adventure of the Beryl Coronet\nTHE ADVENTURE OF THE BERYL CORONET \"Holmes,\" s...\n[THE ADVENTURE OF THE BERYL CORONET \"Holmes,\" ...\n[(one morning, TIME), (February morning, TIME)...\n[Holmes, Holmes, Holmes, Holmes, Alexander Hol...\n{'Holmes': 27, 'Alexander Holder': 1, 'Holder'...\n[the City of London, London, England, morocco,...\n{'Holmes': 27, 'Arthur': 20, 'Holder': 13, 'Ma...\n14\n6\n\n\n45\nThe Resident Patient\nTHE RESIDENT PATIENT Glancing over the somewha...\n[THE RESIDENT PATIENT Glancing over the somewh...\n[(Memoirs, ORG), (Sherlock Holmes, PERSON), (H...\n[Sherlock Holmes, Holmes, Charybdis, Holmes, H...\n{'Sherlock Holmes': 2, 'Holmes': 37, 'Charybdi...\n[India, London, Strand, Blessington, England, ...\n{'Holmes': 37, 'Trevelyan': 13, 'Blessington':...\n13\n14\n\n\n46\nThe Adventure of the Engineer's Thumb\nTHE ADVENTURE OF THE ENGINEER'S THUMB Of all t...\n[THE ADVENTURE OF THE ENGINEER'S THUMB Of all ...\n[(Sherlock Holmes, PERSON), (the years, DATE),...\n[Sherlock Holmes, Hatherley, Warburton, Holmes...\n{'Sherlock Holmes': 3, 'Hatherley': 7, 'Warbur...\n[Paddington, lodgings, London, Greenwich, Lond...\n{'Holmes': 11, 'Hatherley': 7, 'Lysander Stark...\n13\n8\n\n\n47\nThe Yellow Face\nTHE YELLOW FACE [In publishing these short ske...\n[THE YELLOW FACE [In publishing these short sk...\n[(some half-dozen, CARDINAL), (The Adventure o...\n[Holmes, Holmes, I., Holmes, Watson, Holmes, I...\n{'Holmes': 19, 'I.': 2, 'Watson': 4, 'Grosveno...\n[London, America, Atlanta, Hebron, America, Mi...\n{'Holmes': 19, 'Jack': 12, 'Grant Munro': 6, '...\n12\n7\n\n\n\n\n\n\n\n\nconcatenated[\"word_count\"] = concatenated[\"text\"].apply(\n    lambda x: f\"{len(x.split())/1000:.1f}k\" if len(x.split()) &gt;= 1000 else str(len(x.split()))\n)\nconcatenated[\"book_with_count\"] = concatenated.apply(\n    lambda row: f\"{row['book']} ({row['word_count']})\", axis=1\n)\n\n\nconcatenated['unique_places'] = concatenated['places'].apply(lambda x: Counter(x))\nconcatenated['places_sorted'] = concatenated['unique_places'].apply(\n    lambda d: dict(sorted(d.items(), key=lambda item: item[1], reverse=True))\n)\n\n\nbg_color = '#E8F4EC'\npeople_color = '#248C54'\nplaces_color = '#89618E'\ntext_color = '#444444'\nsns.set_theme(style=\"white\", rc={\n    \"font.family\": \"monospace\", \n    \"text.color\": text_color,   \n    \"axes.labelcolor\": text_color, \n    \"xtick.color\": text_color,     \n    \"ytick.color\": text_color      \n})\nitems = [f\"{k} ({v})\" for k, v in concatenated.iloc[0]['persons_sorted'].items()][:20]\nmax_persons = \"\".join(item + (\", \" if i % 2 == 0 and i &lt; len(items)-1 else \"\\n\" if i &lt; len(items)-1 else \"\")\n                 for i, item in enumerate(items))\nitems2 = [f\"{k} ({v})\" for k, v in concatenated.iloc[-1]['persons_sorted'].items()]\nmin_persons = \"\".join(item + (\", \" if i % 2 == 0 and i &lt; len(items2)-1 else \"\\n\" if i &lt; len(items2)-1 else \"\")\n                 for i, item in enumerate(items2))\n\nplaces_items = [f\"{k} ({v})\" for k, v in concatenated.iloc[0]['places_sorted'].items()][:20]\nmax_places = \"\".join(item + (\", \" if i % 2 == 0 and i &lt; len(places_items)-1 else \"\\n\" if i &lt; len(places_items)-1 else \"\") for i, item in enumerate(places_items))\nplaces_items2 = [f\"{k} ({v})\" for k, v in concatenated.iloc[-1]['places_sorted'].items()]\nmin_places = \"\".join(item + (\", \" if i % 2 == 0 and i &lt; len(places_items2)-1 else \"\\n\" if i &lt; len(places_items2)-1 else \"\") for i, item in enumerate(places_items2))\n\nfig1, ax1 = plt.subplots(figsize=(8,10))\nsns.scatterplot(data=concatenated, x='unique_person_count', y='book_with_count', ax=ax1, color=people_color, edgecolor=None)\nsns.scatterplot(data=concatenated, x='unique_places_count', y='book_with_count', marker='s', ax=ax1, color=places_color,edgecolor=None)\nax1.annotate(max_persons, xy=(concatenated.iloc[0]['unique_person_count'], concatenated.iloc[0]['book_with_count']), xytext=(concatenated.iloc[0]['unique_person_count']-43, 0 + 17), arrowprops=dict(arrowstyle=\"-&gt;\", color=people_color), fontsize=10, color=people_color)\nax1.annotate(min_persons, xy=(concatenated.iloc[-1]['unique_person_count'], concatenated.iloc[-1]['book_with_count']), xytext=(concatenated.iloc[-1]['unique_person_count']+20, 48), arrowprops=dict(arrowstyle=\"-&gt;\", color=people_color), fontsize=10, color=people_color)\nax1.annotate(max_places, xy=(concatenated.iloc[0]['unique_places_count'], concatenated.iloc[0]['book_with_count']), xytext=(concatenated.iloc[0]['unique_places_count']-15, 0 + 17), arrowprops=dict(arrowstyle=\"-&gt;\", color=places_color), fontsize=10, color=places_color)\nax1.annotate(min_places, xy=(concatenated.iloc[-1]['unique_places_count'], concatenated.iloc[-1]['book_with_count']), xytext=(concatenated.iloc[-1]['unique_person_count']+20, 40), arrowprops=dict(arrowstyle=\"-&gt;\", color=places_color), fontsize=10, color=places_color)\nsns.despine(left=True)\nax1.tick_params(axis=\"y\", length=0, pad=-10)\nplt.xlabel(\"Unique counts\")\nplt.ylabel(\"\")\ntitle1 = \"series. Entities and their frequencies are shown for the min and max (partial) values. Word count for the text from each book is given in parenthesis.\" \n\nax1.text(90,25, textwrap.fill(\"People\", width=35),fontsize=14, family='Serif', color=people_color, fontweight='bold')\nax1.text(116,25, \"and\",fontsize=14, family='Serif')\nax1.text(130,25, \"Places\",fontsize=14, family='Serif', color=places_color, fontweight='bold')\nax1.text(155,25, \"in Sherlock Holmes\",fontsize=14, family='Serif')\n\n\nax1.text(90,31, textwrap.fill(title1, width=40),fontsize=14, family='Serif')\n\nax1.set_facecolor(bg_color)\nfig1.patch.set_facecolor(bg_color)\nplt.savefig(\"Sherlock_Holmes.png\", dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "posts/UK_weather/UK_weather.html",
    "href": "posts/UK_weather/UK_weather.html",
    "title": "Historic UK Meteorological & Climate Data",
    "section": "",
    "text": "TidyTuesday dataset of October 21, 2025\n\nimport marimo as mo\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.cm as cm\nimport matplotlib.colors as mcolors\nimport textwrap\n\n\nhistoric_station_met = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-21/historic_station_met.csv')\nstation_meta = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-21/station_meta.csv')\n\n\nhistoric_station_met\n\n\n\n\n\n\n\n\nstation\nyear\nmonth\ntmax\ntmin\naf\nrain\nsun\n\n\n\n\n0\naberporth\n1941\n1\nNaN\nNaN\nNaN\n74.7\nNaN\n\n\n1\naberporth\n1941\n2\nNaN\nNaN\nNaN\n69.1\nNaN\n\n\n2\naberporth\n1941\n3\nNaN\nNaN\nNaN\n76.2\nNaN\n\n\n3\naberporth\n1941\n4\nNaN\nNaN\nNaN\n33.7\nNaN\n\n\n4\naberporth\n1941\n5\nNaN\nNaN\nNaN\n51.3\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n39143\nyeovilton\n2024\n8\n22.2\n12.8\n0.0\n27.4\n141.1\n\n\n39144\nyeovilton\n2024\n9\n18.3\n10.7\n0.0\n142.8\n107.6\n\n\n39145\nyeovilton\n2024\n10\n16.2\n8.1\n0.0\n102.0\n85.2\n\n\n39146\nyeovilton\n2024\n11\n11.7\n5.1\n7.0\n88.6\n48.8\n\n\n39147\nyeovilton\n2024\n12\n10.5\n5.0\n1.0\n29.6\n27.9\n\n\n\n\n39148 rows × 8 columns\n\n\n\n\nhistoric_station_met['year'] = pd.to_datetime(historic_station_met['year']).astype(int)\n\n\nbins = [1850, 1925, 1950, 1975, 2000, 2025]\nlabels = ['till 1925', '1926–1950', '1951–1975', '1976–2000', '2001 onwards']\nhistoric_station_met['quarter'] = pd.cut(historic_station_met['year'], bins=bins, labels=labels)\n\n\nhistoric_station_met['tdiff'] = historic_station_met['tmax']-historic_station_met['tmin']\nhistoric_station_met['station'] = historic_station_met['station'].str.capitalize()\n\n\nhistoric_station_met.columns\n\nIndex(['station', 'year', 'month', 'tmax', 'tmin', 'af', 'rain', 'sun',\n       'quarter', 'tdiff'],\n      dtype='object')\n\n\n\ndf_grp = historic_station_met.groupby(['station','year', 'month', 'quarter']).agg({\n    'tmax': 'max',\n    'tmin': 'min',\n    'tdiff': 'mean',\n    'af': 'sum',\n    'rain': 'sum',\n     'sun': 'sum'\n}).reset_index()\ndf_grp\n\n\n\n\n\n\n\n\nstation\nyear\nmonth\nquarter\ntmax\ntmin\ntdiff\naf\nrain\nsun\n\n\n\n\n0\nAberporth\n1853\n1\ntill 1925\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n\n\n1\nAberporth\n1853\n1\n1926–1950\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n\n\n2\nAberporth\n1853\n1\n1951–1975\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n\n\n3\nAberporth\n1853\n1\n1976–2000\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n\n\n4\nAberporth\n1853\n1\n2001 onwards\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n381835\nYeovilton\n2024\n12\ntill 1925\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n\n\n381836\nYeovilton\n2024\n12\n1926–1950\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n\n\n381837\nYeovilton\n2024\n12\n1951–1975\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n\n\n381838\nYeovilton\n2024\n12\n1976–2000\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n\n\n381839\nYeovilton\n2024\n12\n2001 onwards\n10.5\n5.0\n5.5\n1.0\n29.6\n27.9\n\n\n\n\n381840 rows × 10 columns\n\n\n\n\n# Group by both station and year\ngrouped = historic_station_met.groupby(['station', 'year'])\n\n# Dictionary to store correlation results\ncorrelations = {}\n\n# Loop through each (station, year) group\nfor (station, year), df_grp1 in grouped:\n    # Compute correlation between 'rain' and 'sun'\n    corr_matrix = df_grp1[['rain', 'sun']].corr()\n    corr_value = corr_matrix.loc['rain', 'sun']\n\n    # Store result with a tuple key\n    correlations[(station, year)] = corr_value\n\ncorrelation_df = pd.DataFrame.from_dict(\n    correlations, orient='index', columns=['rain_sun_corr']\n)\n\n# Split the tuple index into two columns\ncorrelation_df.index = pd.MultiIndex.from_tuples(correlation_df.index, names=['station', 'year'])\ncorrelation_df = correlation_df.reset_index()\n\nprint(correlation_df)\n\n        station  year  rain_sun_corr\n0     Aberporth  1941            NaN\n1     Aberporth  1942      -0.432952\n2     Aberporth  1943      -0.527707\n3     Aberporth  1944      -0.440227\n4     Aberporth  1945      -0.251566\n...         ...   ...            ...\n3268  Yeovilton  2020      -0.653444\n3269  Yeovilton  2021      -0.141005\n3270  Yeovilton  2022      -0.438216\n3271  Yeovilton  2023      -0.555453\n3272  Yeovilton  2024      -0.234487\n\n[3273 rows x 3 columns]\n\n\n\nsns.scatterplot(data=correlation_df, x='year', y='rain_sun_corr', hue='station', alpha=0.5, legend=False)\nplt.show()\n\n\n\n\n\n\n\n\n\ndf_grp[df_grp['quarter'] == '1926–1950']\n\n\n\n\n\n\n\n\nstation\nyear\nmonth\nquarter\ntmax\ntmin\ntdiff\naf\nrain\nsun\n\n\n\n\n1\nAberporth\n1853\n1\n1926–1950\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n\n\n6\nAberporth\n1853\n2\n1926–1950\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n\n\n11\nAberporth\n1853\n3\n1926–1950\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n\n\n16\nAberporth\n1853\n4\n1926–1950\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n\n\n21\nAberporth\n1853\n5\n1926–1950\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n381816\nYeovilton\n2024\n8\n1926–1950\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n\n\n381821\nYeovilton\n2024\n9\n1926–1950\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n\n\n381826\nYeovilton\n2024\n10\n1926–1950\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n\n\n381831\nYeovilton\n2024\n11\n1926–1950\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n\n\n381836\nYeovilton\n2024\n12\n1926–1950\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n\n\n\n\n76368 rows × 10 columns\n\n\n\n##Plotting\n\ncol_palette = 'autumn_r' #'Wistia'\nmonth_labels = ['J', 'F', 'M', 'A', 'M', 'J', 'J', 'A', 'S', 'O', 'N', 'D']\nsns.set_context(\"talk\", font_scale=2.2)  \nbg_color = '#390099'\nfg_color = '#eef4ed'\n\n# Create a faceted stripplot\n\ng = sns.catplot(\n    data=df_grp,\n    x='month',\n    y='tmax',\n    hue='tdiff',\n    col='station',\n    kind='strip',\n    palette=col_palette,\n    dodge=False,\n    sharey=True,\n    height=5,\n    aspect=1.2,\n    col_wrap=8,\n    legend=False,\n)\ng.fig.patch.set_facecolor(bg_color)  \n\n# Set axes background color\nfor ax in g.axes.flat:\n    ax.set_facecolor(bg_color)  \n    ax.tick_params(axis='y', colors=fg_color)\n    for spine in ax.spines.values():\n        spine.set_color(fg_color)\n\ncol_wrap = 8\n\nfor i, ax in enumerate(g.axes.flat):\n    if i % col_wrap != 0:  # Not the first column in each row\n        ax.set_ylabel('')\n        ax.tick_params(axis='y', left=False, labelleft=False, colors=fg_color)\n        ax.tick_params(axis='x', colors=fg_color)\n        sns.despine(ax=ax,left=True)\n    else:\n        ax.tick_params(axis='x', colors=fg_color)\n\n# Adjust layout\ng.set_titles(\"{col_name}\", color=fg_color)\ng.set_axis_labels(\"\", \"\")\ng.set_xticklabels(month_labels, fontdict={'family': 'monospace', 'color': fg_color})\ng.fig.text(-0.005, 0.5, 'Maximum temperature (°C)', va='center', rotation='vertical', color=fg_color)\n\nnorm = mcolors.Normalize(vmin=df_grp['tdiff'].min(), vmax=df_grp['tdiff'].max())\nsm = cm.ScalarMappable(cmap='autumn_r', norm=norm)\nsm.set_array([])  # Required for colorbar\n\n#g.fig.subplots_adjust(right=0.85)\n# Add the colorbar to the figure\ncbar_ax = g.fig.add_axes([0.7, 0.08, 0.2, 0.01])  # [left, bottom, width, height]\ncbar = g.fig.colorbar(sm, cax=cbar_ax, orientation='horizontal')\ncbar.set_label('Temperature Difference', color=fg_color)\n\n# Change tick label color\ncbar.ax.xaxis.set_tick_params(color=fg_color)  # Tick marks\nfor label in cbar.ax.get_xticklabels():\n    label.set_color(fg_color)  # Tick label text\ntitle = 'Monthly variations in maximum temperature at 37 weather stations in the UK. Points are colored based on the difference in maximum and minimum temperatures.'    \n\ng.fig.text(0.63, 0.13,textwrap.fill(title, width=55), color=fg_color, family='Serif', fontweight='bold', fontsize=38)           \nplt.tight_layout()\nplt.savefig(\"UK_weather.png\", dpi=300, bbox_inches='tight', pad_inches=0.2)\nplt.show()"
  },
  {
    "objectID": "posts/Visa_requirements/Visa_requirements.html",
    "href": "posts/Visa_requirements/Visa_requirements.html",
    "title": "Henley Passport Index Data",
    "section": "",
    "text": "TidyTuesday dataset of September 9, 2025\nThe shiny app is available here.\n\nimport pandas as pd\nimport json\nfrom shiny import App, ui, render\nimport shinyswatch\nimport matplotlib.pyplot as plt\nimport io\nimport base64\n\n# Load the data\ncountry_lists = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-09/country_lists.csv')\n\n# Function to parse JSON strings in the dataframe\ndef parse_json_column(column_value):\n    if pd.isna(column_value):\n        return []\n    try:\n        # The data is already a valid JSON string, just parse it directly\n        data = json.loads(column_value)\n        # Extract the inner array which contains the country objects\n        if data and isinstance(data, list) and len(data) &gt; 0:\n            inner_data = data[0] if isinstance(data[0], list) else data\n            return [item['name'] for item in inner_data if isinstance(item, dict) and 'name' in item]\n        return []\n    except:\n        return []\n\n# Parse all visa-related columns\nvisa_columns = ['visa_required', 'visa_online', 'visa_on_arrival', 'visa_free_access', 'electronic_travel_authorisation']\nfor col in visa_columns:\n    country_lists[f'{col}_countries'] = country_lists[col].apply(parse_json_column)\n\n# Get unique country names for dropdown\ncountries = sorted(country_lists['country'].unique())\n\napp_ui = ui.page_fluid(\n    # Modern header with gradient background\n    ui.div(\n        ui.h1(\"🌍 Visa Travel Information Explorer\", style=\"color: white; text-align: center; margin-bottom: 10px;\"),\n#        ui.p(\"Discover visa requirements and travel information worldwide\", style=\"color: #e0e0e0; text-align: center; font-size: 16px;\"),\n        style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 30px 20px; border-radius: 15px; margin-bottom: 30px; box-shadow: 0 8px 32px rgba(0,0,0,0.1);\"\n    ),\n\n    # First Section - Origin/Destination Check\n    ui.div(\n        ui.h3(\"✈️ Check Visa Requirements Between Countries\", style=\"color: #2c3e50; margin-bottom: 20px;\"),\n        ui.div(\n            ui.row(\n                ui.column(6,\n                    ui.div(\n                        ui.tags.label(\"🏠 Traveler's Nationality:\", style=\"font-weight: bold; color: #34495e;\"),\n                        ui.input_select(\"traveler_nationality\", \"\",\n                                      choices=countries,\n                                      selected=countries[0] if countries else None),\n                        style=\"background: white; padding: 20px; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);\"\n                    )\n                ),\n                ui.column(6,\n                    ui.div(\n                        ui.tags.label(\"🎯 Destination Country:\", style=\"font-weight: bold; color: #34495e;\"),\n                        ui.input_select(\"destination_country\", \"\",\n                                      choices=countries,\n                                      selected=countries[1] if len(countries) &gt; 1 else countries[0] if countries else None),\n                        style=\"background: white; padding: 20px; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);\"\n                    )\n                )\n            ),\n            style=\"margin-bottom: 20px;\"\n        ),\n        ui.div(\n            ui.div(\n                ui.output_ui(\"visa_requirement\"),\n                style=\"color: #2c3e50; text-align: center; background: #f8f9fa; padding: 15px; border-radius: 8px; margin-bottom: 10px;\"\n            ),\n#            ui.p(\"💡 This shows what your HOME country requires from you before traveling to your destination.\", style=\"font-size: 14px; color: #7f8c8d; text-align: center;\"),\n            ui.output_ui(\"detailed_info\"),\n            style=\"background: white; padding: 25px; border-radius: 12px; box-shadow: 0 6px 12px rgba(0,0,0,0.1);\"\n        ),\n        style=\"margin-bottom: 40px;\"\n    ),\n\n    # Separator\n    ui.div(\n        ui.tags.hr(style=\"border: none; height: 2px; background: linear-gradient(90deg, #667eea, #764ba2); margin: 40px 0;\"),\n        style=\"text-align: center;\"\n    ),\n\n    # Second Section - Country Explorer\n    ui.div(\n        ui.h3(\"🔍 Explore All Visa Categories for a Country\", style=\"color: #2c3e50; margin-bottom: 20px;\"),\n#        ui.p(\"💡 This shows what countries the selected nation requires visas FROM (outbound travel requirements).\", style=\"font-size: 14px; color: #7f8c8d; margin-bottom: 20px;\"),\n        ui.row(\n            ui.column(12,\n                ui.div(\n                    ui.tags.label(\"🌐 Select a Country:\", style=\"font-weight: bold; color: #34495e;\"),\n                    ui.input_select(\"selected_country\", \"\",\n                                  choices=countries,\n                                  selected=countries[0] if countries else None),\n                    style=\"background: white; padding: 20px; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); margin-bottom: 20px;\"\n                )\n            )\n        ),\n        ui.div(\n            ui.output_text(\"country_info\"),\n            style=\"font-size: 16px; color: #2c3e50; text-align: center; background: #f8f9fa; padding: 15px; border-radius: 8px; margin-bottom: 20px;\"\n        ),\n        ui.row(\n            ui.column(2,\n                ui.div(\n                    ui.output_ui(\"visa_required_list\"),\n                    style=\"background: #ffeaa7; padding: 15px; border-radius: 8px; height: 100%; box-shadow: 0 4px 6px rgba(0,0,0,0.1);\"\n                )\n            ),\n            ui.column(2,\n                ui.div(\n                    ui.output_ui(\"visa_online_list\"),\n                    style=\"background: #74b9ff; padding: 15px; border-radius: 8px; height: 100%; box-shadow: 0 4px 6px rgba(0,0,0,0.1);\"\n                )\n            ),\n            ui.column(2,\n                ui.div(\n                    ui.output_ui(\"visa_on_arrival_list\"),\n                    style=\"background: #00b894; padding: 15px; border-radius: 8px; height: 100%; box-shadow: 0 4px 6px rgba(0,0,0,0.1);\"\n                )\n            ),\n            ui.column(3,\n                ui.div(\n                    ui.output_ui(\"visa_free_list\"),\n                    style=\"background: #00cec9; padding: 15px; border-radius: 8px; height: 100%; box-shadow: 0 4px 6px rgba(0,0,0,0.1);\"\n                )\n            ),\n            ui.column(3,\n                ui.div(\n                    ui.output_ui(\"electronic_travel_auth_list\"),\n                    style=\"background: #a29bfe; padding: 15px; border-radius: 8px; height: 100%; box-shadow: 0 4px 6px rgba(0,0,0,0.1);\"\n                )\n            )\n        ),\n        style=\"background: white; padding: 25px; border-radius: 12px; box-shadow: 0 6px 12px rgba(0,0,0,0.1);\"\n    ),\n\n    # Footer\n    ui.div(\n        ui.p(\"bioinfo@manishdatt.com\", style=\"text-align: center; color: #95a5a6; font-size: 12px; margin-top: 30px;\"),\n        style=\"text-align: center; margin-top: 40px;\"\n    ),\n\n    theme=shinyswatch.theme.flatly()\n)\n\ndef server(input, output, session):\n    def create_visa_bar_plot(traveler_data):\n        \"\"\"Create a horizontal bar plot showing visa policy counts\"\"\"\n        categories = [\n            'Visa Required',\n            'Visa Online',\n            'Visa on Arrival',\n            'Visa-Free Access',\n            'Electronic Travel Authorisation'\n        ]\n\n        counts = [\n            len(traveler_data['visa_required_countries']),\n            len(traveler_data['visa_online_countries']),\n            len(traveler_data['visa_on_arrival_countries']),\n            len(traveler_data['visa_free_access_countries']),\n            len(traveler_data['electronic_travel_authorisation_countries'])\n        ]\n\n        # Colors matching the UI theme\n        colors = ['#ffeaa7', '#74b9ff', '#00b894', '#00cec9', '#a29bfe']\n\n        fig, ax = plt.subplots(figsize=(8, 4))\n        bars = ax.barh(categories, counts, color=colors, edgecolor='white', linewidth=2)\n\n        # Add value labels on bars\n        for bar, count in zip(bars, counts):\n            ax.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,\n                   f'{count}', ha='left', va='center', fontweight='bold', fontsize=12)\n\n        ax.set_xlabel('Number of Countries', fontsize=12, fontweight='bold')\n#        ax.set_title('Visa Policy Distribution', fontsize=14, fontweight='bold', pad=20)\n        ax.grid(axis='x', alpha=0.3)\n        ax.invert_yaxis()  \n\n        # Remove top and right spines\n        ax.spines[['right', 'top', 'left']].set_visible(False)\n        # remove y-axis tick lines\n        ax.tick_params(axis='y', length=0)\n        ax.tick_params(axis='x', length=0)\n        plt.tight_layout()\n\n        # Convert plot to base64 string\n        buf = io.BytesIO()\n        fig.savefig(buf, format='png', dpi=100, bbox_inches='tight', facecolor='white')\n        buf.seek(0)\n        image_base64 = base64.b64encode(buf.read()).decode('utf-8')\n        buf.close()\n        plt.close(fig)\n\n        return f\"data:image/png;base64,{image_base64}\"\n\n    def get_visa_requirement(origin, destination):\n        if not origin or not destination:\n            return \"Please select both origin and destination countries\"\n\n        if origin == destination:\n            return \"No visa required (domestic travel)\"\n\n        # Get ORIGIN country data (traveler's nationality)\n        origin_data = country_lists[country_lists['country'] == origin]\n        if origin_data.empty:\n            return \"Origin country data not found\"\n\n        origin_data = origin_data.iloc[0]\n\n        # Check what the ORIGIN country requires from DESTINATION country's citizens\n        # This answers: \"What does my home country require from citizens of my destination?\"\n        if destination in origin_data['visa_free_access_countries']:\n            return \"Visa-Free Access\"\n        elif destination in origin_data['visa_on_arrival_countries']:\n            return \"Visa on Arrival\"\n        elif destination in origin_data['visa_online_countries']:\n            return \"Visa Online\"\n        elif destination in origin_data['electronic_travel_authorisation_countries']:\n            return \"Electronic Travel Authorisation Required\"\n        else:\n            return \"Visa Required\"\n\n    @output\n    @render.ui\n    def visa_requirement():\n        traveler = input.traveler_nationality()\n        destination = input.destination_country()\n\n        requirement = get_visa_requirement(traveler, destination)\n\n        if traveler and destination:\n            return ui.HTML(f'&lt;span style=\"font-size: 18px;\"&gt;Traveling from {traveler} to {destination}:&lt;/span&gt; &lt;span style=\"font-size: 22px; font-weight: bold;\"&gt;{requirement}&lt;/span&gt;')\n        else:\n            return ui.HTML('&lt;span style=\"font-size: 18px;\"&gt;Please select both traveler\\'s nationality and destination countries&lt;/span&gt;')\n\n    @output\n    @render.ui\n    def detailed_info():\n        traveler = input.traveler_nationality()\n        destination = input.destination_country()\n\n        if not traveler or not destination:\n            return ui.div()\n\n        if traveler == destination:\n            return ui.div(\n                ui.h4(\"Travel Information:\"),\n                ui.p(\"Domestic travel - no visa requirements.\")\n            )\n\n        # Get traveler's nationality country data (not destination)\n        traveler_data = country_lists[country_lists['country'] == traveler].iloc[0]\n\n        requirement = get_visa_requirement(traveler, destination)\n\n        # Create the bar plot\n        plot_url = create_visa_bar_plot(traveler_data)\n\n        return ui.div(\n#            ui.h4(\"Travel Requirements:\"),\n#            ui.p(f\"From {traveler} to {destination}: {requirement}\"),\n#            ui.br(),\n            ui.h5(f\"{traveler}'s Visa Policy Distribution:\"),\n            ui.tags.img(src=plot_url, style=\"max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);\"),\n            ui.br(),\n#            ui.p(\"📊 This chart shows how many countries fall into each visa category for citizens of your nationality.\", style=\"font-size: 12px; color: #7f8c8d; text-align: center; margin-top: 10px;\")\n        )\n\n    # Single country selection functions\n    @output\n    @render.text\n    def country_info():\n        selected = input.selected_country()\n        if not selected:\n            return \"Please select a country\"\n\n        country_data = country_lists[country_lists['country'] == selected].iloc[0]\n        return f\"Selected Country: {selected} ({country_data['code']})\"\n\n    def create_visa_list_ui(title, countries_list):\n        if not countries_list:\n            return ui.div(\n                ui.h5(f\"{title}:\"),\n                ui.p(\"None\")\n            )\n        return ui.div(\n            ui.h5(f\"{title} ({len(countries_list)}):\"),\n            ui.tags.ul([ui.tags.li(country) for country in sorted(countries_list)])\n        )\n\n    @output\n    @render.ui\n    def visa_required_list():\n        selected = input.selected_country()\n        if not selected:\n            return ui.div()\n        country_data = country_lists[country_lists['country'] == selected].iloc[0]\n        return create_visa_list_ui(\"Visa Required\", country_data['visa_required_countries'])\n\n    @output\n    @render.ui\n    def visa_online_list():\n        selected = input.selected_country()\n        if not selected:\n            return ui.div()\n        country_data = country_lists[country_lists['country'] == selected].iloc[0]\n        return create_visa_list_ui(\"Visa Online\", country_data['visa_online_countries'])\n\n    @output\n    @render.ui\n    def visa_on_arrival_list():\n        selected = input.selected_country()\n        if not selected:\n            return ui.div()\n        country_data = country_lists[country_lists['country'] == selected].iloc[0]\n        return create_visa_list_ui(\"Visa on Arrival\", country_data['visa_on_arrival_countries'])\n\n    @output\n    @render.ui\n    def visa_free_list():\n        selected = input.selected_country()\n        if not selected:\n            return ui.div()\n        country_data = country_lists[country_lists['country'] == selected].iloc[0]\n        return create_visa_list_ui(\"Visa-Free Access\", country_data['visa_free_access_countries'])\n\n    @output\n    @render.ui\n    def electronic_travel_auth_list():\n        selected = input.selected_country()\n        if not selected:\n            return ui.div()\n        country_data = country_lists[country_lists['country'] == selected].iloc[0]\n        return create_visa_list_ui(\"Electronic Travel Authorisation\", country_data['electronic_travel_authorisation_countries'])\n\napp = App(app_ui, server)"
  },
  {
    "objectID": "posts/WHO_SPI/WHO_SPI.html",
    "href": "posts/WHO_SPI/WHO_SPI.html",
    "title": "WHO Statistical Performance Indicators",
    "section": "",
    "text": "TidyTuesday dataset of November 25, 2025\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.dates as mdates\nimport matplotlib.transforms as mtransforms\n\n\nspi_indicators = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-25/spi_indicators.csv')\nspi_indicators\n\n\n\n\n\n\n\n\niso3c\ncountry\nregion\nincome\nyear\npopulation\noverall_score\ndata_use_score\ndata_services_score\ndata_products_score\ndata_sources_score\ndata_infrastructure_score\n\n\n\n\n0\nDNK\nDenmark\nEurope & Central Asia\nHigh income\n2023\n5946952\n95.255833\n100.0\n98.466667\n90.71250\n87.100\n100.0\n\n\n1\nFIN\nFinland\nEurope & Central Asia\nHigh income\n2023\n5584264\n95.115417\n100.0\n96.433333\n90.96875\n88.175\n100.0\n\n\n2\nPOL\nPoland\nEurope & Central Asia\nHigh income\n2023\n36685849\n94.653750\n100.0\n97.300000\n84.54375\n91.425\n100.0\n\n\n3\nSWE\nSweden\nEurope & Central Asia\nHigh income\n2023\n10536632\n94.410000\n100.0\n96.000000\n90.57500\n85.475\n100.0\n\n\n4\nESP\nSpain\nEurope & Central Asia\nHigh income\n2023\n48373336\n94.325000\n100.0\n91.200000\n92.62500\n87.800\n100.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4335\nVIR\nVirgin Islands (U.S.)\nLatin America & Caribbean\nHigh income\n2004\n108466\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n4336\nPSE\nWest Bank and Gaza\nMiddle East & North Africa\nLower middle income\n2004\n3236626\nNaN\n20.0\nNaN\nNaN\nNaN\nNaN\n\n\n4337\nYEM\nYemen, Rep.\nMiddle East & North Africa\nLow income\n2004\n20733406\nNaN\n20.0\nNaN\nNaN\nNaN\nNaN\n\n\n4338\nZMB\nZambia\nSub-Saharan Africa\nLower middle income\n2004\n11188040\nNaN\n40.0\nNaN\nNaN\nNaN\nNaN\n\n\n4339\nZWE\nZimbabwe\nSub-Saharan Africa\nLower middle income\n2004\n12160881\nNaN\n20.0\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n4340 rows × 12 columns\n\n\n\n\nspi_indicators['year'] = pd.to_datetime(spi_indicators['year'], format='%Y')\n\n\nspi_indicators.columns\n\nIndex(['iso3c', 'country', 'region', 'income', 'year', 'population',\n       'overall_score', 'data_use_score', 'data_services_score',\n       'data_products_score', 'data_sources_score',\n       'data_infrastructure_score'],\n      dtype='object')\n\n\n\ncountry_desc = spi_indicators.groupby(['country'])['overall_score'].describe()\ncountry_desc['diff'] = country_desc['max']-country_desc['min']\ntop10_diff = country_desc.nlargest(10, 'diff')\nspi_top10 = spi_indicators[spi_indicators['country'].isin(top10_diff.index)]\nspi_top10\n\n\n\n\n\n\n\n\niso3c\ncountry\nregion\nincome\nyear\npopulation\noverall_score\ndata_use_score\ndata_services_score\ndata_products_score\ndata_sources_score\ndata_infrastructure_score\n\n\n\n\n54\nSAU\nSaudi Arabia\nMiddle East & North Africa\nHigh income\n2023-01-01\n36947025\n81.533333\n70.0\n88.366667\n75.22500\n79.075000\n95.0\n\n\n61\nUZB\nUzbekistan\nEurope & Central Asia\nLower middle income\n2023-01-01\n36412350\n80.332083\n80.0\n76.633333\n85.04375\n64.983333\n95.0\n\n\n71\nARE\nUnited Arab Emirates\nMiddle East & North Africa\nHigh income\n2023-01-01\n9516871\n77.635000\n90.0\n79.500000\n71.15000\n62.525000\n85.0\n\n\n80\nBFA\nBurkina Faso\nSub-Saharan Africa\nLow income\n2023-01-01\n23251485\n74.309167\n100.0\n69.000000\n80.38750\n42.158333\n80.0\n\n\n92\nZWE\nZimbabwe\nSub-Saharan Africa\nLower middle income\n2023-01-01\n16665409\n69.732500\n100.0\n67.033333\n83.38750\n38.241667\n60.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4289\nSAU\nSaudi Arabia\nMiddle East & North Africa\nHigh income\n2004-01-01\n23661808\nNaN\n20.0\nNaN\nNaN\nNaN\nNaN\n\n\n4299\nSOM\nSomalia\nSub-Saharan Africa\nLow income\n2004-01-01\n10117354\nNaN\n20.0\nNaN\nNaN\nNaN\nNaN\n\n\n4327\nARE\nUnited Arab Emirates\nMiddle East & North Africa\nHigh income\n2004-01-01\n3993339\nNaN\n20.0\nNaN\nNaN\nNaN\nNaN\n\n\n4331\nUZB\nUzbekistan\nEurope & Central Asia\nLower middle income\n2004-01-01\n25864350\nNaN\n40.0\nNaN\nNaN\nNaN\nNaN\n\n\n4339\nZWE\nZimbabwe\nSub-Saharan Africa\nLower middle income\n2004-01-01\n12160881\nNaN\n20.0\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n200 rows × 12 columns\n\n\n\n\ndf_melt_top10 = pd.melt(spi_top10, id_vars=['country','year'], \\\n            value_vars=['overall_score', 'data_use_score', 'data_services_score', 'data_products_score', 'data_sources_score', 'data_infrastructure_score'],\\\n            var_name='Score_type', value_name='Scores')\ndf_melt_top10\n\n\n\n\n\n\n\n\ncountry\nyear\nScore_type\nScores\n\n\n\n\n0\nSaudi Arabia\n2023-01-01\noverall_score\n81.533333\n\n\n1\nUzbekistan\n2023-01-01\noverall_score\n80.332083\n\n\n2\nUnited Arab Emirates\n2023-01-01\noverall_score\n77.635000\n\n\n3\nBurkina Faso\n2023-01-01\noverall_score\n74.309167\n\n\n4\nZimbabwe\n2023-01-01\noverall_score\n69.732500\n\n\n...\n...\n...\n...\n...\n\n\n1195\nSaudi Arabia\n2004-01-01\ndata_infrastructure_score\nNaN\n\n\n1196\nSomalia\n2004-01-01\ndata_infrastructure_score\nNaN\n\n\n1197\nUnited Arab Emirates\n2004-01-01\ndata_infrastructure_score\nNaN\n\n\n1198\nUzbekistan\n2004-01-01\ndata_infrastructure_score\nNaN\n\n\n1199\nZimbabwe\n2004-01-01\ndata_infrastructure_score\nNaN\n\n\n\n\n1200 rows × 4 columns\n\n\n\n\nsns.set_context(\"notebook\", font_scale=1.5)\n\ng = sns.relplot(data=df_melt_top10, x='year', y='Scores', hue='country', kind='line', col='Score_type', \\\n            col_wrap=3, legend=False, col_order=['data_use_score', 'data_sources_score', 'data_infrastructure_score', 'data_products_score', 'data_services_score', 'overall_score'])\n\noriginal_ticks = g.axes.flatten()[0].get_xticks()   # or use one representative axis\n\nkeep = original_ticks#[::2]\n#if original_ticks[-1] not in keep:\n#    keep = list(keep) + [original_ticks[-1]]\n\nfor ind, ax in enumerate(g.axes.flatten()):\n    new_title = ' '.join(ax.get_title().removesuffix('_score').split()[-1].split('_')).capitalize()\n    \n    ax.set_title(new_title, y=0.9, color='#555555')\n#    ax.tick_params(axis='x', rotation=45)\n    ax.set_xlabel(\"\")\n    ax.set_xticks(keep)\n    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%y\"))\n    ax.margins(x=0)\n    ax.set_facecolor('#F5F5F5')\n    for labelx, labely in zip(ax.get_xticklabels(), ax.get_yticklabels()):\n        labelx.set_fontfamily('monospace')\n        labely.set_fontfamily('monospace')\n    if (ind not in [0,3]):\n        ax.spines['left'].set_visible(False)\n        ax.tick_params(axis='y', left=False, labelleft=False)\n        labels = ax.get_xticklabels()\n        if labels:\n            labels[1].set_visible(False)   \n\nfig = g.fig\nax = g.axes.flatten()[1]   # facet to annotate\n\n# create an offset transform: 5 points right, 0 points up\ntext_trans = mtransforms.offset_copy(ax.transData, fig=fig,\n                                     x=5, y=0, units='points')\n\ny_offset = 3\nfor line, country in zip(ax.lines, df_melt_top10['country'].unique()):\n    x_end = line.get_xdata()[-1]\n    y_end = line.get_ydata()[-1]\n\n    if (country in ['Uzbekistan', 'Lebanon']):\n        y_end += y_offset\n    if (country in ['Marshall Islands', 'Zimbabwe']):\n        y_end -= y_offset\n    \n\n    fig.text(\n        x_end,\n        y_end,\n        country,\n        transform=text_trans,   \n        ha='left',\n        va='center',\n        color=line.get_color(),\n        clip_on=False           \n    )\n\n\ng.fig.subplots_adjust(wspace=0, hspace=0)\ng.fig.suptitle(\"Top 10 countries with the greatest improvement in the overall score \\nof the World Bank Statistical Performance Indicators from 2004 \\u2013 2023\", fontfamily='Serif', y=1.025)\ng.fig.patch.set_facecolor('#F5F5F5')\nplt.savefig('WHO_SPI.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "posts/World_Food/World_food.html",
    "href": "posts/World_Food/World_food.html",
    "title": "FAO’s Suite of Food Security Indicators",
    "section": "",
    "text": "TidyTuesday dataset of October 14, 2025\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport textwrap\n\n\nfood_security = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-14/food_security.csv')\n\n\nfood_security\n\n\n\n\n\n\n\n\nYear_Start\nYear_End\nArea\nItem\nUnit\nValue\nCI_Lower\nCI_Upper\nFlag\nNote\n\n\n\n\n0\n2005\n2007\nAfghanistan\nAverage dietary energy supply adequacy (percen...\n%\n97.0\nNaN\nNaN\nEstimated value\nNaN\n\n\n1\n2006\n2008\nAfghanistan\nAverage dietary energy supply adequacy (percen...\n%\n99.0\nNaN\nNaN\nEstimated value\nNaN\n\n\n2\n2007\n2009\nAfghanistan\nAverage dietary energy supply adequacy (percen...\n%\n102.0\nNaN\nNaN\nEstimated value\nNaN\n\n\n3\n2008\n2010\nAfghanistan\nAverage dietary energy supply adequacy (percen...\n%\n104.0\nNaN\nNaN\nEstimated value\nNaN\n\n\n4\n2009\n2011\nAfghanistan\nAverage dietary energy supply adequacy (percen...\n%\n105.0\nNaN\nNaN\nEstimated value\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n171227\n2016\n2018\nUpper-middle-income economies\nAverage fat supply (g/cap/day) (3-year average)\ng/cap/d\n89.1\nNaN\nNaN\nEstimated value\nNaN\n\n\n171228\n2017\n2019\nUpper-middle-income economies\nAverage fat supply (g/cap/day) (3-year average)\ng/cap/d\n90.8\nNaN\nNaN\nEstimated value\nNaN\n\n\n171229\n2018\n2020\nUpper-middle-income economies\nAverage fat supply (g/cap/day) (3-year average)\ng/cap/d\n92.5\nNaN\nNaN\nEstimated value\nNaN\n\n\n171230\n2019\n2021\nUpper-middle-income economies\nAverage fat supply (g/cap/day) (3-year average)\ng/cap/d\n95.0\nNaN\nNaN\nEstimated value\nNaN\n\n\n171231\n2020\n2022\nUpper-middle-income economies\nAverage fat supply (g/cap/day) (3-year average)\ng/cap/d\n96.5\nNaN\nNaN\nEstimated value\nNaN\n\n\n\n\n171232 rows × 10 columns\n\n\n\n\nfood_security = food_security.assign(Year_Start = pd.to_datetime(food_security['Year_Start'], format='%Y'),\n                                     Year_End = pd.to_datetime(food_security['Year_End'], format='%Y'))\n\n\nfood_security_nona = food_security[food_security['Value'].notna()]\nfood_security_nona_mod = food_security_nona[(food_security_nona['Item'] == 'Number of children under 5 years of age who are overweight (modeled estimates) (million)') \\\n| (food_security_nona['Item']=='Number of newborns with low birthweight (million)')]\n\n\nfood_security_nona_mod2 = food_security_nona_mod[food_security_nona_mod['Value']&gt;5]\nfood_security_nona_mod2\n\n\n\n\n\n\n\n\nYear_Start\nYear_End\nArea\nItem\nUnit\nValue\nCI_Lower\nCI_Upper\nFlag\nNote\n\n\n\n\n24188\n2005-01-01\n2005-01-01\nChina\nNumber of children under 5 years of age who ar...\nmillion No\n5.2\nNaN\nNaN\nFigure from external organization\nNaN\n\n\n24189\n2006-01-01\n2006-01-01\nChina\nNumber of children under 5 years of age who ar...\nmillion No\n5.2\nNaN\nNaN\nFigure from external organization\nNaN\n\n\n24190\n2007-01-01\n2007-01-01\nChina\nNumber of children under 5 years of age who ar...\nmillion No\n5.3\nNaN\nNaN\nFigure from external organization\nNaN\n\n\n24191\n2008-01-01\n2008-01-01\nChina\nNumber of children under 5 years of age who ar...\nmillion No\n5.4\nNaN\nNaN\nFigure from external organization\nNaN\n\n\n24192\n2009-01-01\n2009-01-01\nChina\nNumber of children under 5 years of age who ar...\nmillion No\n5.5\nNaN\nNaN\nFigure from external organization\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n171123\n2020-01-01\n2020-01-01\nUpper-middle-income economies\nNumber of children under 5 years of age who ar...\nmillion No\n15.6\nNaN\nNaN\nFigure from external organization\nBased on FY 2025 classification\n\n\n171124\n2021-01-01\n2021-01-01\nUpper-middle-income economies\nNumber of children under 5 years of age who ar...\nmillion No\n15.1\nNaN\nNaN\nFigure from external organization\nBased on FY 2025 classification\n\n\n171125\n2022-01-01\n2022-01-01\nUpper-middle-income economies\nNumber of children under 5 years of age who ar...\nmillion No\n14.4\nNaN\nNaN\nFigure from external organization\nBased on FY 2025 classification\n\n\n171126\n2023-01-01\n2023-01-01\nUpper-middle-income economies\nNumber of children under 5 years of age who ar...\nmillion No\n13.7\nNaN\nNaN\nFigure from external organization\nBased on FY 2025 classification\n\n\n171127\n2024-01-01\n2024-01-01\nUpper-middle-income economies\nNumber of children under 5 years of age who ar...\nmillion No\n13.3\nNaN\nNaN\nFigure from external organization\nBased on FY 2025 classification\n\n\n\n\n374 rows × 10 columns\n\n\n\n\nfig,ax = plt.subplots(figsize=(8,5))\nsns.scatterplot(data=food_security_nona_mod2, x='Year_End', y='Value', hue='Item', alpha=0.6)\n\n\n\n\n\n\n\n\n\nitems_mi = [x for x in food_security['Item'].unique() if 'million' in x]\nitems_mi = [x for x in items_mi if 'average' not in x]\nitems_mi = [x for x in items_mi if x is not None]\nprint(len(items_mi))\nitems_mi\n\n13\n\n\n['Number of children under 5 years affected by wasting (million)',\n 'Number of children under 5 years of age who are stunted (modeled estimates) (million)',\n 'Number of children under 5 years of age who are overweight (modeled estimates) (million)',\n 'Number of obese adults (18 years and older) (million)',\n 'Number of women of reproductive age (15-49 years) affected by anemia (million)',\n 'Number of newborns with low birthweight (million)',\n 'Number of people undernourished (million) (annual value)',\n 'Number of severely food insecure people (million) (annual value)',\n 'Number of severely food insecure male adults (million) (annual value)',\n 'Number of severely food insecure female adults (million) (annual value)',\n 'Number of moderately or severely food insecure people (million) (annual value)',\n 'Number of moderately or severely food insecure male adults (million) (annual value)',\n 'Number of moderately or severely food insecure female adults (million) (annual value)']\n\n\n\ndf_world = food_security_nona[food_security_nona['Area']=='World'][food_security_nona['Item'].isin(items_mi)]\ndf_world\n\n\n\n\n\n\n\n\nYear_Start\nYear_End\nArea\nItem\nUnit\nValue\nCI_Lower\nCI_Upper\nFlag\nNote\n\n\n\n\n132089\n2005-01-01\n2005-01-01\nWorld\nNumber of people undernourished (million) (ann...\nmillion No\n788.8\nNaN\nNaN\nEstimated value\nNaN\n\n\n132090\n2006-01-01\n2006-01-01\nWorld\nNumber of people undernourished (million) (ann...\nmillion No\n733.8\nNaN\nNaN\nEstimated value\nNaN\n\n\n132091\n2007-01-01\n2007-01-01\nWorld\nNumber of people undernourished (million) (ann...\nmillion No\n672.9\nNaN\nNaN\nEstimated value\nNaN\n\n\n132092\n2008-01-01\n2008-01-01\nWorld\nNumber of people undernourished (million) (ann...\nmillion No\n639.7\nNaN\nNaN\nEstimated value\nNaN\n\n\n132093\n2009-01-01\n2009-01-01\nWorld\nNumber of people undernourished (million) (ann...\nmillion No\n630.1\nNaN\nNaN\nEstimated value\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n132781\n2016-01-01\n2016-01-01\nWorld\nNumber of newborns with low birthweight (million)\nmillion No\n20.9\nNaN\nNaN\nFigure from external organization\nNaN\n\n\n132782\n2017-01-01\n2017-01-01\nWorld\nNumber of newborns with low birthweight (million)\nmillion No\n20.6\nNaN\nNaN\nFigure from external organization\nNaN\n\n\n132783\n2018-01-01\n2018-01-01\nWorld\nNumber of newborns with low birthweight (million)\nmillion No\n20.4\nNaN\nNaN\nFigure from external organization\nNaN\n\n\n132784\n2019-01-01\n2019-01-01\nWorld\nNumber of newborns with low birthweight (million)\nmillion No\n20.1\nNaN\nNaN\nFigure from external organization\nNaN\n\n\n132785\n2020-01-01\n2020-01-01\nWorld\nNumber of newborns with low birthweight (million)\nmillion No\n19.8\nNaN\nNaN\nFigure from external organization\nNaN\n\n\n\n\n199 rows × 10 columns\n\n\n\n\ndf_world['Item_mod'] = df_world['Item'].str.split('\\(m').str[0].str.split('Number of ').str[1].str.capitalize()\ndf_world\n\n\n\n\n\n\n\n\nYear_Start\nYear_End\nArea\nItem\nUnit\nValue\nCI_Lower\nCI_Upper\nFlag\nNote\nItem_mod\n\n\n\n\n132089\n2005-01-01\n2005-01-01\nWorld\nNumber of people undernourished (million) (ann...\nmillion No\n788.8\nNaN\nNaN\nEstimated value\nNaN\nPeople undernourished\n\n\n132090\n2006-01-01\n2006-01-01\nWorld\nNumber of people undernourished (million) (ann...\nmillion No\n733.8\nNaN\nNaN\nEstimated value\nNaN\nPeople undernourished\n\n\n132091\n2007-01-01\n2007-01-01\nWorld\nNumber of people undernourished (million) (ann...\nmillion No\n672.9\nNaN\nNaN\nEstimated value\nNaN\nPeople undernourished\n\n\n132092\n2008-01-01\n2008-01-01\nWorld\nNumber of people undernourished (million) (ann...\nmillion No\n639.7\nNaN\nNaN\nEstimated value\nNaN\nPeople undernourished\n\n\n132093\n2009-01-01\n2009-01-01\nWorld\nNumber of people undernourished (million) (ann...\nmillion No\n630.1\nNaN\nNaN\nEstimated value\nNaN\nPeople undernourished\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n132781\n2016-01-01\n2016-01-01\nWorld\nNumber of newborns with low birthweight (million)\nmillion No\n20.9\nNaN\nNaN\nFigure from external organization\nNaN\nNewborns with low birthweight\n\n\n132782\n2017-01-01\n2017-01-01\nWorld\nNumber of newborns with low birthweight (million)\nmillion No\n20.6\nNaN\nNaN\nFigure from external organization\nNaN\nNewborns with low birthweight\n\n\n132783\n2018-01-01\n2018-01-01\nWorld\nNumber of newborns with low birthweight (million)\nmillion No\n20.4\nNaN\nNaN\nFigure from external organization\nNaN\nNewborns with low birthweight\n\n\n132784\n2019-01-01\n2019-01-01\nWorld\nNumber of newborns with low birthweight (million)\nmillion No\n20.1\nNaN\nNaN\nFigure from external organization\nNaN\nNewborns with low birthweight\n\n\n132785\n2020-01-01\n2020-01-01\nWorld\nNumber of newborns with low birthweight (million)\nmillion No\n19.8\nNaN\nNaN\nFigure from external organization\nNaN\nNewborns with low birthweight\n\n\n\n\n199 rows × 11 columns\n\n\n\n\nfig,ax = plt.subplots(figsize=(10,8))\nsns.stripplot(data=df_world, x='Value', y='Item_mod',hue='Year_End', size=15,\\\n alpha=0.6, legend=False, palette='Blues')\nax.yaxis.tick_right()\nax.tick_params(axis='y', which='major', pad=-320) \nax.tick_params(axis='y', length=0)\nplt.xlabel(\"Millions\")\nplt.ylabel(\"\")\ntitle=\"Number of people affected by food insecurity at the world level.\\nValues are shown for 2005 to 2024 with increasing intensity.\"\n#title_wrapped = textwrap.fill(title, width=65)\nplt.title(title, family='Serif', fontsize=14, ha='left', x=0.05)\nsns.despine(left=True)\nlabels = ax.get_yticklabels()\nwrapped_labels = []\nfor label in labels:\n    original_text = label.get_text()\n    wrapped_text = textwrap.fill(original_text, width=25) # Adjust width as needed\n    wrapped_labels.append(wrapped_text)\nax.set_yticklabels(wrapped_labels, color='#333333')\nax.set_xticklabels([int(x) for x in ax.get_xticks()], family='monospace')\nplt.savefig('world_food.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\ndf_world.describe()\n\n\n\n\n\n\n\n\nYear_Start\nYear_End\nValue\nCI_Lower\nCI_Upper\n\n\n\n\ncount\n199\n199\n199.000000\n66.000000\n66.000000\n\n\nmean\n2015-09-11 09:46:07.839195904\n2015-09-11 09:46:07.839195904\n451.684925\n738.501515\n783.675758\n\n\nmin\n2005-01-01 00:00:00\n2005-01-01 00:00:00\n19.800000\n160.100000\n185.700000\n\n\n25%\n2012-01-01 00:00:00\n2012-01-01 00:00:00\n51.350000\n273.875000\n295.175000\n\n\n50%\n2016-01-01 00:00:00\n2016-01-01 00:00:00\n457.700000\n599.650000\n650.300000\n\n\n75%\n2020-01-01 00:00:00\n2020-01-01 00:00:00\n639.750000\n806.125000\n847.150000\n\n\nmax\n2024-01-01 00:00:00\n2024-01-01 00:00:00\n2296.600000\n2260.900000\n2332.400000\n\n\nstd\nNaN\nNaN\n468.873860\n598.928099\n620.089332"
  }
]