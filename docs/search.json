[
  {
    "objectID": "posts/useR2025/useR2025.html",
    "href": "posts/useR2025/useR2025.html",
    "title": "useR2025 conference data analysis",
    "section": "",
    "text": "library(tidyverse)\nlibrary(uk2us)\nuser2025 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-29/user2025.csv')\nuser2025\n\n# A tibble: 128 × 11\n      id session date       time  room   title  content video_recording keywords\n   &lt;dbl&gt; &lt;chr&gt;   &lt;date&gt;     &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;           &lt;chr&gt;   \n 1   170 Virtual 2025-08-01 TBD   Online A Rob… In R p… ✅              statist…\n 2    79 Virtual 2025-08-01 TBD   Online A fir… Positr… ✅              ide, wo…\n 3    30 Virtual 2025-08-01 TBD   Online Analy… This t… ✅              demogra…\n 4    31 Virtual 2025-08-01 TBD   Online Autom… Webhoo… ✅              automat…\n 5    39 Virtual 2025-08-01 TBD   Online Beyon… In a w… ✅              marketi…\n 6   169 Virtual 2025-08-01 TBD   Online CSV t… CSV is… ✅              data pr…\n 7    94 Virtual 2025-08-01 TBD   Online Data … Explor… ✅              factor …\n 8   163 Virtual 2025-08-01 TBD   Online Don’t… The fa… ✅              testing…\n 9    13 Virtual 2025-08-01 TBD   Online Exper… Large … ✅              automat…\n10    51 Virtual 2025-08-01 TBD   Online From … Data S… ✅              quarto,…\n# ℹ 118 more rows\n# ℹ 2 more variables: speakers &lt;chr&gt;, co_authors &lt;chr&gt;\nnormalize_keywords_uk2us &lt;- function(phrases) {\n  phrases %&gt;%\n    str_split(\"\\\\s+\") %&gt;%                                \n    map(~ uk2us::convert_uk2us(.x)) %&gt;%                  \n    map_chr(str_c, collapse = \" \")                       \n}"
  },
  {
    "objectID": "posts/useR2025/useR2025.html#plotting",
    "href": "posts/useR2025/useR2025.html#plotting",
    "title": "useR2025 conference data analysis",
    "section": "Plotting",
    "text": "Plotting\n\nuser2025 %&gt;%\n  separate_rows(keywords, sep = \",\") %&gt;%\n  mutate(keywords = str_trim(keywords)) %&gt;%\n  mutate(keywords = normalize_keywords_uk2us(keywords)) %&gt;% \n  group_by(keywords) %&gt;% \n  summarise(count = n()) %&gt;% \n  arrange(desc(count)) %&gt;% \n  slice(1:20) %&gt;% \n  ggplot(aes(x = reorder(keywords, count), y = count, fill=count)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = reorder(keywords, count), y=count/2), hjust = 0, \n            size = 3.5, color=rep(c(\"white\", \"#555\"), times=c(8,12))) + \n  coord_flip() +\n  scale_fill_viridis_c(option=\"magma\", direction = -1)+\n  labs(title = \"Top 20 Keywords in useR! 2025\") +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.title.y = element_blank(),\n    legend.position = \"none\",\n    panel.grid.major.y = element_blank()\n  )\n\n\n\n\n\n\n\nggsave(\"user2025_keywords.png\", width = 8, height = 6, dpi = 300)"
  },
  {
    "objectID": "posts/Seismic_Vesuvius/Mt_vesuvius.html",
    "href": "posts/Seismic_Vesuvius/Mt_vesuvius.html",
    "title": "Seismic events at Mount Vesuvius",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nvesuvius = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-13/vesuvius.csv')\n\n\nvesuvius\n\n\n\n\n\n\n\n\nevent_id\ntime\nlatitude\nlongitude\ndepth_km\nduration_magnitude_md\nmd_error\narea\ntype\nreview_level\nyear\n\n\n\n\n0\n4251\n2011-04-20T00:27:24Z\n40.818000\n14.430000\n0.42\n1.2\n0.3\nMount Vesuvius\nearthquake\nrevised\n2011\n\n\n1\n4252\n2012-06-19T21:29:48Z\n40.808833\n14.427167\n1.31\n0.7\n0.3\nMount Vesuvius\nearthquake\nrevised\n2012\n\n\n2\n22547\n2013-01-01T07:34:46Z\n40.822170\n14.428000\n0.06\n2.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n\n\n3\n22546\n2013-01-03T16:06:48Z\nNaN\nNaN\nNaN\n0.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n\n\n4\n22545\n2013-01-03T16:07:37Z\nNaN\nNaN\nNaN\n0.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n12022\n40738\n2024-12-29T23:56:51Z\n40.823000\n14.428333\n0.34\n-0.1\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n12023\n40741\n2024-12-30T07:52:43Z\n40.823333\n14.423500\n0.56\n-0.1\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n12024\n40743\n2024-12-30T12:52:24Z\nNaN\nNaN\nNaN\n-0.1\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n12025\n40744\n2024-12-30T15:11:28Z\n40.819000\n14.424500\n0.55\n-0.4\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n12026\n40802\n2024-12-31T17:02:32Z\n40.822000\n14.409833\n0.41\n0.0\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n\n\n12027 rows × 11 columns\n\n\n\n\nsns.histplot(data=vesuvius,x=\"depth_km\")\n\n\n\n\n\n\n\n\n\nvesuvius[\"time\"] = pd.to_datetime(vesuvius[\"time\"])\n\n\nvesuvius[\"hour\"] = vesuvius[\"time\"].dt.hour\ntime_bins = [0, 5, 12, 17, 21, 24]  # 0-4 -&gt; Night, 5-11 -&gt; Morning, etc.\ntime_labels = [\"Night\", \"Morning\", \"Afternoon\", \"Evening\", \"Night\"]\n\n\nvesuvius[\"time_of_day\"] = pd.cut(vesuvius['hour'], bins=time_bins, labels=time_labels, right=False, ordered=False)\n\n\nvesuvius[\"time_of_day\"]\n\n0            Night\n1            Night\n2          Morning\n3        Afternoon\n4        Afternoon\n           ...    \n12022        Night\n12023      Morning\n12024    Afternoon\n12025    Afternoon\n12026      Evening\nName: time_of_day, Length: 12027, dtype: category\nCategories (4, object): ['Afternoon', 'Evening', 'Morning', 'Night']\n\n\n\nvesuvius[\"year\"].unique()\n\narray([2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021,\n       2022, 2023, 2024], dtype=int64)\n\n\n\nbin_edges = [-float('inf'), 1, 2, 3, 5, 7, float('inf')]\nvesuvius[\"depth_bins\"] = pd.cut(vesuvius['depth_km'], bins=bin_edges)\n\n\nvesuvius[\"depth_bins\"].value_counts()\n\ndepth_bins\n(-inf, 1.0]    7777\n(1.0, 2.0]      648\n(2.0, 3.0]      142\n(3.0, 5.0]       25\n(5.0, 7.0]        1\n(7.0, inf]        1\nName: count, dtype: int64\n\n\n\nvesuvius.head()\n\n\n\n\n\n\n\n\nevent_id\ntime\nlatitude\nlongitude\ndepth_km\nduration_magnitude_md\nmd_error\narea\ntype\nreview_level\nyear\nhour\ntime_of_day\ndepth_bins\n\n\n\n\n0\n4251\n2011-04-20 00:27:24+00:00\n40.818000\n14.430000\n0.42\n1.2\n0.3\nMount Vesuvius\nearthquake\nrevised\n2011\n0\nNight\n(-inf, 1.0]\n\n\n1\n4252\n2012-06-19 21:29:48+00:00\n40.808833\n14.427167\n1.31\n0.7\n0.3\nMount Vesuvius\nearthquake\nrevised\n2012\n21\nNight\n(1.0, 2.0]\n\n\n2\n22547\n2013-01-01 07:34:46+00:00\n40.822170\n14.428000\n0.06\n2.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n7\nMorning\n(-inf, 1.0]\n\n\n3\n22546\n2013-01-03 16:06:48+00:00\nNaN\nNaN\nNaN\n0.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n16\nAfternoon\nNaN\n\n\n4\n22545\n2013-01-03 16:07:37+00:00\nNaN\nNaN\nNaN\n0.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n16\nAfternoon\nNaN"
  },
  {
    "objectID": "posts/Seismic_Vesuvius/Mt_vesuvius.html#tidytuesday-data-for-2025-05-13",
    "href": "posts/Seismic_Vesuvius/Mt_vesuvius.html#tidytuesday-data-for-2025-05-13",
    "title": "Seismic events at Mount Vesuvius",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nvesuvius = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-13/vesuvius.csv')\n\n\nvesuvius\n\n\n\n\n\n\n\n\nevent_id\ntime\nlatitude\nlongitude\ndepth_km\nduration_magnitude_md\nmd_error\narea\ntype\nreview_level\nyear\n\n\n\n\n0\n4251\n2011-04-20T00:27:24Z\n40.818000\n14.430000\n0.42\n1.2\n0.3\nMount Vesuvius\nearthquake\nrevised\n2011\n\n\n1\n4252\n2012-06-19T21:29:48Z\n40.808833\n14.427167\n1.31\n0.7\n0.3\nMount Vesuvius\nearthquake\nrevised\n2012\n\n\n2\n22547\n2013-01-01T07:34:46Z\n40.822170\n14.428000\n0.06\n2.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n\n\n3\n22546\n2013-01-03T16:06:48Z\nNaN\nNaN\nNaN\n0.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n\n\n4\n22545\n2013-01-03T16:07:37Z\nNaN\nNaN\nNaN\n0.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n12022\n40738\n2024-12-29T23:56:51Z\n40.823000\n14.428333\n0.34\n-0.1\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n12023\n40741\n2024-12-30T07:52:43Z\n40.823333\n14.423500\n0.56\n-0.1\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n12024\n40743\n2024-12-30T12:52:24Z\nNaN\nNaN\nNaN\n-0.1\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n12025\n40744\n2024-12-30T15:11:28Z\n40.819000\n14.424500\n0.55\n-0.4\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n12026\n40802\n2024-12-31T17:02:32Z\n40.822000\n14.409833\n0.41\n0.0\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n\n\n12027 rows × 11 columns\n\n\n\n\nsns.histplot(data=vesuvius,x=\"depth_km\")\n\n\n\n\n\n\n\n\n\nvesuvius[\"time\"] = pd.to_datetime(vesuvius[\"time\"])\n\n\nvesuvius[\"hour\"] = vesuvius[\"time\"].dt.hour\ntime_bins = [0, 5, 12, 17, 21, 24]  # 0-4 -&gt; Night, 5-11 -&gt; Morning, etc.\ntime_labels = [\"Night\", \"Morning\", \"Afternoon\", \"Evening\", \"Night\"]\n\n\nvesuvius[\"time_of_day\"] = pd.cut(vesuvius['hour'], bins=time_bins, labels=time_labels, right=False, ordered=False)\n\n\nvesuvius[\"time_of_day\"]\n\n0            Night\n1            Night\n2          Morning\n3        Afternoon\n4        Afternoon\n           ...    \n12022        Night\n12023      Morning\n12024    Afternoon\n12025    Afternoon\n12026      Evening\nName: time_of_day, Length: 12027, dtype: category\nCategories (4, object): ['Afternoon', 'Evening', 'Morning', 'Night']\n\n\n\nvesuvius[\"year\"].unique()\n\narray([2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021,\n       2022, 2023, 2024], dtype=int64)\n\n\n\nbin_edges = [-float('inf'), 1, 2, 3, 5, 7, float('inf')]\nvesuvius[\"depth_bins\"] = pd.cut(vesuvius['depth_km'], bins=bin_edges)\n\n\nvesuvius[\"depth_bins\"].value_counts()\n\ndepth_bins\n(-inf, 1.0]    7777\n(1.0, 2.0]      648\n(2.0, 3.0]      142\n(3.0, 5.0]       25\n(5.0, 7.0]        1\n(7.0, inf]        1\nName: count, dtype: int64\n\n\n\nvesuvius.head()\n\n\n\n\n\n\n\n\nevent_id\ntime\nlatitude\nlongitude\ndepth_km\nduration_magnitude_md\nmd_error\narea\ntype\nreview_level\nyear\nhour\ntime_of_day\ndepth_bins\n\n\n\n\n0\n4251\n2011-04-20 00:27:24+00:00\n40.818000\n14.430000\n0.42\n1.2\n0.3\nMount Vesuvius\nearthquake\nrevised\n2011\n0\nNight\n(-inf, 1.0]\n\n\n1\n4252\n2012-06-19 21:29:48+00:00\n40.808833\n14.427167\n1.31\n0.7\n0.3\nMount Vesuvius\nearthquake\nrevised\n2012\n21\nNight\n(1.0, 2.0]\n\n\n2\n22547\n2013-01-01 07:34:46+00:00\n40.822170\n14.428000\n0.06\n2.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n7\nMorning\n(-inf, 1.0]\n\n\n3\n22546\n2013-01-03 16:06:48+00:00\nNaN\nNaN\nNaN\n0.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n16\nAfternoon\nNaN\n\n\n4\n22545\n2013-01-03 16:07:37+00:00\nNaN\nNaN\nNaN\n0.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n16\nAfternoon\nNaN"
  },
  {
    "objectID": "posts/Seismic_Vesuvius/Mt_vesuvius.html#annual-number-of-tremors-vs-duration",
    "href": "posts/Seismic_Vesuvius/Mt_vesuvius.html#annual-number-of-tremors-vs-duration",
    "title": "Seismic events at Mount Vesuvius",
    "section": "Annual number of tremors vs duration",
    "text": "Annual number of tremors vs duration\n\nfig, ax = plt.subplots()\nhue_order = [\"Morning\", \"Afternoon\", \"Evening\", \"Night\"]\ncolor_map = [\"lightblue\", \"dodgerblue\", \"lightgrey\", \"grey\"]\ncolors = {x: color_map[ind] for ind, x in enumerate(hue_order)}\n     \nsns.stripplot(data=vesuvius, x=\"year\", y=\"duration_magnitude_md\", \\\n              hue='time_of_day', hue_order=hue_order, palette=\"plasma\")\nplt.xticks(rotation=45)\nplt.xlabel(\"\")\n\nplt.legend(\n    title='',\n    loc='upper center',     \n    bbox_to_anchor=(0.5, 1.1), \n    ncol=4,\n    frameon=False\n)\nsns.despine()"
  },
  {
    "objectID": "posts/Seismic_Vesuvius/Mt_vesuvius.html#earthquakes-at-different-times-of-the-day",
    "href": "posts/Seismic_Vesuvius/Mt_vesuvius.html#earthquakes-at-different-times-of-the-day",
    "title": "Seismic events at Mount Vesuvius",
    "section": "Earthquakes at different times of the day",
    "text": "Earthquakes at different times of the day\n\nvesuvius[\"hour12\"] = vesuvius[\"time\"].dt.strftime(\"%I\").astype(int)    \nvesuvius[\"AMPM\"] = vesuvius[\"time\"].dt.strftime(\"%p\")              \n\n\nvesuvius_grp = vesuvius.groupby([\"hour12\", \"AMPM\"]).size().unstack(fill_value=0)\nvesuvius_grp\n\n\n\n\n\n\n\nAMPM\nAM\nPM\n\n\nhour12\n\n\n\n\n\n\n1\n691\n339\n\n\n2\n694\n384\n\n\n3\n618\n335\n\n\n4\n591\n440\n\n\n5\n510\n732\n\n\n6\n472\n555\n\n\n7\n352\n610\n\n\n8\n287\n592\n\n\n9\n284\n624\n\n\n10\n380\n598\n\n\n11\n334\n627\n\n\n12\n646\n332\n\n\n\n\n\n\n\n\nvesuvius_grp.plot(kind=\"bar\", stacked=True, color=['dodgerblue', 'salmon'])"
  },
  {
    "objectID": "posts/Seismic_Vesuvius/Mt_vesuvius.html#polar-coordinates",
    "href": "posts/Seismic_Vesuvius/Mt_vesuvius.html#polar-coordinates",
    "title": "Seismic events at Mount Vesuvius",
    "section": "Polar coordinates",
    "text": "Polar coordinates\n\nimport plotly.graph_objects as go\nimport numpy as np\nimport roman\n\n\nvesuvius_grp['angle'] = vesuvius_grp.index * 30\n\n# Create polar plot with two bar traces: AM and PM\nfig = go.Figure()\n\n# AM bars\nfig.add_trace(go.Barpolar(\n    r=vesuvius_grp['AM'],\n    theta=vesuvius_grp['angle'],\n    name='AM',\n    marker_color='dodgerblue',\n    hovertemplate='count = %{r}&lt;br&gt;time = %{theta} AM&lt;extra&gt;&lt;/extra&gt;'\n))\n\n# PM bars\nfig.add_trace(go.Barpolar(\n    r=vesuvius_grp['PM'],\n    theta=vesuvius_grp['angle'],\n    name='PM',\n    marker_color='salmon',\n    hovertemplate='count = %{r}&lt;br&gt;time = %{theta} PM&lt;extra&gt;&lt;/extra&gt;'\n))\n\n# Layout\nfig.update_layout(\n    title={\n        'text': f'Distribution of &lt;b&gt;{vesuvius_grp[\"AM\"].sum() + vesuvius_grp[\"PM\"].sum():,.0f}&lt;/b&gt; earthquakes during &lt;br&gt;&lt;span style=\"color:dodgerblue;\"&gt;AM&lt;/span&gt; and &lt;span style=\"color:red;\"&gt;PM&lt;/span&gt; at Mount Vesuvius (2011-24)',\n        'font': {\n            'size': 16  \n        },\n        \"x\" : 0.5,\n    },\n    polar=dict(\n        angularaxis=dict(direction='clockwise', rotation=90, tickmode='array',\n                         tickvals=np.arange(30, 361, 30),\n                         ticktext=[f\"&lt;b&gt;{roman.toRoman(h)}&lt;/b&gt;\" for h in range(1, 13)], \n                         showline=False, showgrid=False),\n        radialaxis=dict(showticklabels=False, ticks='', showline=False, showgrid=False)\n    ),\n    showlegend=False,\n    template='plotly_white',\n    width=500,\n    height=500,\n    margin=dict(l=0, r=0, t=100, b=20)\n)\n#fig.write_image(\"Vesuvius.png\")\nfig.show()\n\n                                                \n\n\nThere is no direct way to add labels to bars in radial axis. As a workaround, add scatterplot and show only text.\n\nvesuvius_grp['angle'] = vesuvius_grp.index * 30\n\n# Create polar plot with two bar traces: AM and PM\nfig = go.Figure()\n\n# AM bars\nfig.add_trace(go.Barpolar(\n    r=vesuvius_grp['AM'],\n    theta=vesuvius_grp['angle'],\n    name='AM',\n    marker_color='dodgerblue',\n    hovertemplate='count = %{r}&lt;br&gt;time = %{theta} AM&lt;extra&gt;&lt;/extra&gt;'\n))\n\n# PM bars\nfig.add_trace(go.Barpolar(\n    r=vesuvius_grp['PM'],\n    theta=vesuvius_grp['angle'],\n    name='PM',\n    marker_color='salmon',\n    hovertemplate='count = %{r}&lt;br&gt;time = %{theta} PM&lt;extra&gt;&lt;/extra&gt;'\n))\n\n# Layout\nfig.update_layout(\n    title={\n        'text': f'Distribution of &lt;b&gt;{vesuvius_grp[\"AM\"].sum() + vesuvius_grp[\"PM\"].sum():,.0f}&lt;/b&gt; earthquakes during &lt;br&gt;&lt;span style=\"color:dodgerblue;\"&gt;AM&lt;/span&gt; and &lt;span style=\"color:red;\"&gt;PM&lt;/span&gt; at Mount Vesuvius (2011-24)',\n        'font': {\n            'size': 16  \n        },\n        \"x\" : 0.5,\n    },\n    polar=dict(\n        angularaxis=dict(direction='clockwise', rotation=90, tickmode='array',\n                         tickvals=np.arange(30, 361, 30),\n                         ticktext=[f\"&lt;b&gt;{roman.toRoman(h)}&lt;/b&gt;\" for h in range(1, 13)], \n                         showline=False, showgrid=False),\n        radialaxis=dict(showticklabels=False, ticks='', showline=False, showgrid=False)\n    ),\n    showlegend=False,\n    template='plotly_white',\n    width=500,\n    height=500,\n    margin=dict(l=0, r=0, t=100, b=20)\n)\nfig.add_trace(go.Scatterpolar(\n    r=vesuvius_grp['AM']+50,  \n    theta=vesuvius_grp['angle'],  \n    mode=\"text\",  \n    text=vesuvius_grp['AM'],  \n    textposition=\"middle center\",  \n    textfont=dict(size=14, color=\"white\"),\n    name=\"AM Labels\",\n    hoverinfo=\"skip\",  \n    showlegend=False\n))\n\nfig.add_trace(go.Scatterpolar(\n    r=vesuvius_grp['AM']+vesuvius_grp['PM']+50,  \n    theta=vesuvius_grp['angle'],  \n    mode=\"text\",\n    text=vesuvius_grp['PM'],\n    textposition=\"middle center\",\n    name=\"PM Labels\",\n    hoverinfo=\"skip\",\n    showlegend=False\n))\n#fig.write_image(\"Vesuvius_labels.png\")\nfig.show()"
  },
  {
    "objectID": "posts/Judges_appoint/Judges_appoint.html",
    "href": "posts/Judges_appoint/Judges_appoint.html",
    "title": "The US Judges data",
    "section": "",
    "text": "import pandas as pd\n\n\njudges_appointments = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-10/judges_appointments.csv')\njudges_people = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-10/judges_people.csv')\n\n\njudges_appointments\n\n\n\n\n\n\n\n\njudge_id\ncourt_name\ncourt_type\npresident_name\npresident_party\nnomination_date\npredecessor_last_name\npredecessor_first_name\nsenate_confirmation_date\ncommission_date\nchief_judge_begin\nchief_judge_end\nretirement_from_active_service\ntermination_date\ntermination_reason\n\n\n\n\n0\n3419\nU. S. District Court, Southern District of New...\nUSDC\nBarack Obama\nDemocratic\n07/28/2011\nKaplan\nLewis A.\n03/22/2012\n03/23/2012\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n1\nU. S. District Court, Eastern District of New ...\nUSDC\nFranklin D. Roosevelt\nDemocratic\n02/03/1936\nnew\nNaN\n02/12/1936\n02/15/1936\nNaN\nNaN\n02/15/1966\n05/28/1971\nDeath\n\n\n2\n2\nU. S. District Court, Western District of Penn...\nUSDC\nRutherford B. Hayes\nRepublican\n01/06/1880\nKetcham\nWinthrop\n01/14/1880\n01/14/1880\nNaN\nNaN\nNaN\n02/09/1891\nAppointment to Another Judicial Position\n\n\n3\n3\nU. S. District Court, Northern District of Ala...\nUSDC\nRonald Reagan\nRepublican\n07/22/1982\nMcFadden\nFrank H.\n08/18/1982\n08/18/1982\nNaN\nNaN\n05/31/1996\nNaN\nNaN\n\n\n4\n4\nU. S. District Court, District of New Jersey\nUSDC\nJimmy Carter\nDemocratic\n09/28/1979\nBarlow\nGeorge H.\n10/31/1979\n11/02/1979\nNaN\nNaN\n02/15/1994\n12/02/2009\nDeath\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4197\n2201\nU. S. District Courts, Albemarle, Cape Fear & ...\nUSDC\nReassignment\nReassignment\nNaN\nnew\nNaN\nNaN\n02/13/1801\nNaN\nNaN\nNaN\n03/04/1802\nDeath\n\n\n4198\n2689\nU. S. District Court, Eastern District of Miss...\nUSDC\nHarry S Truman\nDemocratic\n01/13/1949\nHarper\nRoy Winfield\n01/31/1949\n02/02/1949\nNaN\nNaN\n01/05/1971\n02/13/1994\nDeath\n\n\n4199\n1126\nU. S. Court of Appeals for the Ninth Circuit\nUSCA\nWilliam H. Taft\nRepublican\n12/12/1910\nnew\nNaN\n01/31/1911\n02/08/1911\nNaN\nNaN\n01/31/1928\n11/30/1928\nRetirement\n\n\n4200\n1453\nU. S. Court of Appeals for the Second Circuit\nUSCA\nReassignment\nReassignment\nNaN\nnew\nNaN\nNaN\n07/01/1929\nNaN\nNaN\n09/06/1940\n09/05/1943\nDeath\n\n\n4201\n2689\nU. S. District Court, Western District of Miss...\nUSDC\nHarry S Truman\nDemocratic\n01/13/1949\nHarper\nRoy Winfield\n01/31/1949\n02/02/1949\nNaN\nNaN\n01/05/1971\n02/13/1994\nDeath\n\n\n\n\n4202 rows × 15 columns\n\n\n\n\njudges_appointments['commission_date'] = pd.to_datetime(judges_appointments['commission_date'])\njudges_appointments['commission_date'].dt.year.min()\n\n1789.0\n\n\n\njudges_people\n\n\n\n\n\n\n\n\njudge_id\nname_first\nname_middle\nname_last\nname_suffix\nbirth_date\nbirthplace_city\nbirthplace_state\ndeath_date\ndeath_city\ndeath_state\ngender\nrace\n\n\n\n\n0\n3419\nRonnie\nNaN\nAbrams\nNaN\n1968.0\nNew York\nNY\nNaN\nNaN\nNaN\nF\nWhite\n\n\n1\n1\nMatthew\nT.\nAbruzzo\nNaN\n1889.0\nBrooklyn\nNY\n1971.0\nPotomac\nMD\nM\nWhite\n\n\n2\n2\nMarcus\nWilson\nAcheson\nNaN\n1828.0\nWashington\nPA\n1906.0\nPittsburgh\nPA\nM\nWhite\n\n\n3\n3\nWilliam\nMarsh\nAcker\nJr.\n1927.0\nBirmingham\nAL\nNaN\nNaN\nNaN\nM\nWhite\n\n\n4\n4\nHarold\nArnold\nAckerman\nNaN\n1928.0\nNewark\nNJ\n2009.0\nWest Orange\nNJ\nM\nWhite\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3527\n3391\nJennifer\nGuerin\nZipps\nNaN\n1964.0\nAshland\nOH\nNaN\nNaN\nNaN\nF\nWhite\n\n\n3528\n2687\nAlfonso\nJoseph\nZirpoli\nNaN\n1905.0\nDenver\nCO\n1995.0\nSan Francisco\nCA\nM\nWhite\n\n\n3529\n2688\nWilliam\nJ.\nZloch\nNaN\n1944.0\nFort Lauderdale\nFL\nNaN\nNaN\nNaN\nM\nWhite\n\n\n3530\n2690\nRya\nWeickert\nZobel\nNaN\n1931.0\nZwickau\nGermany\nNaN\nNaN\nNaN\nF\nWhite\n\n\n3531\n3106\nJack\nNaN\nZouhary\nNaN\n1951.0\nToledo\nOH\nNaN\nNaN\nNaN\nM\nWhite\n\n\n\n\n3532 rows × 13 columns\n\n\n\n\njudges_people.groupby([\"gender\"]).size()\n\ngender\nF     389\nM    3143\ndtype: int64\n\n\n\njudges_people['race']=judges_people['race'].fillna(\"Others\")\n\n\ndf_grp = judges_people.groupby([\"gender\",\"race\"]).count().sort_values([\"gender\",\"judge_id\"], ascending=[True,False])\ndf_grp\n\n\n\n\n\n\n\n\n\njudge_id\nname_first\nname_middle\nname_last\nname_suffix\nbirth_date\nbirthplace_city\nbirthplace_state\ndeath_date\ndeath_city\ndeath_state\n\n\ngender\nrace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF\nWhite\n293\n293\n265\n293\n0\n293\n293\n293\n28\n24\n24\n\n\nAfrican American\n51\n51\n50\n51\n0\n51\n51\n51\n3\n3\n3\n\n\nHispanic\n29\n29\n21\n29\n0\n29\n29\n29\n1\n1\n1\n\n\nAsian American\n9\n9\n9\n9\n0\n9\n9\n9\n0\n0\n0\n\n\nOthers\n3\n3\n3\n3\n0\n3\n3\n3\n0\n0\n0\n\n\nAfrican Am./Hispanic\n1\n1\n1\n1\n0\n1\n1\n1\n0\n0\n0\n\n\nAmerican Indian\n1\n1\n1\n1\n0\n1\n1\n1\n0\n0\n0\n\n\nHispanic/Asian Am.\n1\n1\n0\n1\n0\n1\n1\n1\n0\n0\n0\n\n\nWhite/Asian Am.\n1\n1\n0\n1\n0\n1\n1\n1\n0\n0\n0\n\n\nM\nWhite\n2871\n2871\n2483\n2871\n299\n2870\n2848\n2869\n1906\n1350\n1352\n\n\nAfrican American\n150\n150\n135\n150\n41\n150\n150\n150\n42\n38\n38\n\n\nHispanic\n87\n87\n70\n87\n5\n87\n86\n86\n15\n9\n9\n\n\nAsian American\n22\n22\n18\n22\n0\n22\n22\n22\n6\n4\n4\n\n\nOthers\n6\n6\n6\n6\n1\n6\n6\n6\n1\n1\n1\n\n\nAmerican Indian\n2\n2\n2\n2\n0\n2\n2\n2\n0\n0\n0\n\n\nPac. Isl./Asian Am.\n2\n2\n2\n2\n0\n2\n2\n2\n0\n0\n0\n\n\nAfrican Am./Hispanic\n1\n1\n1\n1\n0\n1\n1\n1\n0\n0\n0\n\n\nHispanic/White\n1\n1\n1\n1\n0\n1\n1\n1\n0\n0\n0\n\n\nPac. Isl./White\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\n\ndf1 = df_grp[\"judge_id\"].reset_index()\ndf1\n\n\n\n\n\n\n\n\ngender\nrace\njudge_id\n\n\n\n\n0\nF\nWhite\n293\n\n\n1\nF\nAfrican American\n51\n\n\n2\nF\nHispanic\n29\n\n\n3\nF\nAsian American\n9\n\n\n4\nF\nOthers\n3\n\n\n5\nF\nAfrican Am./Hispanic\n1\n\n\n6\nF\nAmerican Indian\n1\n\n\n7\nF\nHispanic/Asian Am.\n1\n\n\n8\nF\nWhite/Asian Am.\n1\n\n\n9\nM\nWhite\n2871\n\n\n10\nM\nAfrican American\n150\n\n\n11\nM\nHispanic\n87\n\n\n12\nM\nAsian American\n22\n\n\n13\nM\nOthers\n6\n\n\n14\nM\nAmerican Indian\n2\n\n\n15\nM\nPac. Isl./Asian Am.\n2\n\n\n16\nM\nAfrican Am./Hispanic\n1\n\n\n17\nM\nHispanic/White\n1\n\n\n18\nM\nPac. Isl./White\n1\n\n\n\n\n\n\n\n\ndef process_group(group):\n    top_rows = group.nlargest(4, 'judge_id')  \n    remaining_rows = group.iloc[4:]  \n    if not remaining_rows.empty:\n        other_sum = remaining_rows['judge_id'].sum()\n        other_row = pd.DataFrame({'gender': [group.name], 'race': ['Others'], 'judge_id': [other_sum]})\n        return pd.concat([top_rows, other_row], ignore_index=True)\n    return top_rows\n\n# Apply function to each group\ndf_grouped = df1.groupby('gender', group_keys=False).apply(process_group)\n\ndf_grouped\n\n\n\n\n\n\n\n\ngender\nrace\njudge_id\n\n\n\n\n0\nF\nWhite\n293\n\n\n1\nF\nAfrican American\n51\n\n\n2\nF\nHispanic\n29\n\n\n3\nF\nAsian American\n9\n\n\n4\nF\nOthers\n7\n\n\n0\nM\nWhite\n2871\n\n\n1\nM\nAfrican American\n150\n\n\n2\nM\nHispanic\n87\n\n\n3\nM\nAsian American\n22\n\n\n4\nM\nOthers\n13\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\nfrom PIL import Image\nimport textwrap\n\n# Define race colors\nrace_colors = {\n    'White': (160, 200, 220), \n    'African American': (70, 130, 180),\n    'Hispanic': (0, 0, 128), \n    'Asian American': (30, 144, 255),  \n    'Others': (0, 0, 0)  \n}\ndf_grouped_dict = df_grouped.groupby(\"gender\").apply(lambda x: dict(zip(x[\"race\"], x[\"judge_id\"]))).to_dict()\n\ndf_percentages = {gender: {race: (count / sum(race_counts.values())) * 100 for race, count in race_counts.items()}\n                  for gender, race_counts in df_grouped_dict.items()}\n\n# Function to recolor an icon with stacked race colors\ndef recolor_icon_layers(icon_path, race_counts, race_colors):\n    img = Image.open(icon_path).convert(\"RGBA\")  # Convert to RGBA\n    data = np.array(img)  # Convert image to array\n\n    total_count = sum(race_counts.values())\n    height = data.shape[0]\n    y_start = 0\n\n    for race, count in race_counts.items():\n        layer_height = int((count / total_count) * height)\n        y_end = y_start + layer_height\n        mask = data[y_start:y_end, :, 3] &gt; 0  # Keep only non-transparent pixels\n        data[y_start:y_end, :, :-1][mask] = race_colors.get(race, (128, 128, 128))  # Apply race color\n        y_start = y_end\n\n    return Image.fromarray(data)\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Calculate total counts for scaling\ntotal_category1 = sum(df_grouped_dict.get(\"F\", {}).values())\ntotal_category2 = sum(df_grouped_dict.get(\"M\", {}).values())\n\nrace_colors_mpl = {race: (r/255, g/255, b/255) for race, (r, g, b) in race_colors.items()}\n\nfor i, (category, race_counts) in enumerate(df_grouped_dict.items()):\n    icon_path = \"person-dress.png\" if category == \"F\" else \"person.png\"\n    icon_array = recolor_icon_layers(icon_path, race_counts, race_colors)\n\n    scale_factor = sum(race_counts.values()) / max(total_category1, total_category2)\n    base_zoom = 0.8  # Adjust zoom level\n    imagebox = OffsetImage(np.array(icon_array), zoom=base_zoom * scale_factor)\n\n    ab = AnnotationBbox(imagebox, (i, 0), frameon=False, xycoords=\"data\", box_alignment=(0.5, 0))\n    ax.add_artist(ab)\n\n    percentages = df_percentages[category]\n    for j, (race, percent) in enumerate(reversed(percentages.items())):\n        if(category == \"F\"):\n            ax.text(i+0.3, j*150, f\"{percent:.1f}%\", ha='right', fontsize=10, color=race_colors_mpl[race])\n        else:            \n            ax.text(i+0.4, j*250, f\"{race}: {percent:.1f}%\", ha='left', fontsize=10, color=race_colors_mpl[race])\n\nax.text(0, 1000, f\"Female judges: {total_category1}\", ha='center', fontsize=10, color=\"black\")\nax.text(1, 4000, f\"Male judges: {total_category2:,}\", ha='center', fontsize=10, color=\"black\")\n\nax.set_xticks([0, 1])\nax.set_xlim(-0.5, 1.5)\nax.set_ylim(0, max(total_category1, total_category2) + 100)\ntitle = f\"Among the US judges appointed from {judges_appointments['commission_date'].dt.year.min():.0f} to {judges_appointments['commission_date'].dt.year.max():.0f}, there were about 8 male judges for every female judge. The proportion of the top four races is shown in different colors.\"\nwrapped_title = \"\\n\".join(textwrap.wrap(title, width=30))\nax.set_title(wrapped_title, loc='left', pad=75)\nax.axis(\"off\")\nfig.patch.set_facecolor(\"whitesmoke\")\nfig.savefig(\"judges_appoint.png\", bbox_inches=\"tight\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "posts/Dungeons_Dragons/DD_monsters.html",
    "href": "posts/Dungeons_Dragons/DD_monsters.html",
    "title": "Dungeons and Dragons Monsters (2024)",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nmonsters = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-27/monsters.csv')\n\n\nmonsters\n\n\n\n\n\n\n\n\nname\ncategory\ncr\nsize\ntype\ndescriptive_tags\nalignment\nac\ninitiative\nhp\n...\nwis_save\ncha_save\nskills\nresistances\nvulnerabilities\nimmunities\ngear\nsenses\nlanguages\nfull_text\n\n\n\n\n0\nAboleth\nAboleth\n10.000\nLarge\nAberration\nNaN\nLawful Evil\n17\n7\n150 (20d10 + 40)\n...\n6\n4\nHistory +12, Perception +10\nNaN\nNaN\nNaN\nNaN\nDarkvision 120 ft.; Passive Perception 20\nDeep Speech; telepathy 120 ft.\nAboleth\\nLarge Aberration, Lawful Evil\\nAC 17\\...\n\n\n1\nAir Elemental\nAir Elemental\n5.000\nLarge\nElemental\nNaN\nNeutral\n15\n5\n90 (12d10 + 24)\n...\n0\n-2\nNaN\nBludgeoning, Lightning, Piercing, Slashing\nNaN\nPoison, Thunder; Exhaustion, Grappled, Paralyz...\nNaN\nDarkvision 60 ft.; Passive Perception 10\nPrimordial (Auran)\nAir Elemental\\nLarge Elemental, Neutral\\nAC 15...\n\n\n2\nAnimated Armor\nAnimated Objects\n1.000\nMedium\nConstruct\nNaN\nUnaligned\n18\n2\n33 (6d8 + 6)\n...\n-4\n-5\nNaN\nNaN\nNaN\nPoison, Psychic; Charmed, Deafened, Exhaustion...\nNaN\nBlindsight 60 ft.; Passive Perception 6\nNaN\nAnimated Armor\\nMedium Construct, Unaligned\\nA...\n\n\n3\nAnimated Flying Sword\nAnimated Objects\n0.250\nSmall\nConstruct\nNaN\nUnaligned\n17\n4\n14 (4d6)\n...\n-3\n-5\nNaN\nNaN\nNaN\nPoison, Psychic; Charmed, Deafened, Exhaustion...\nNaN\nBlindsight 60 ft.; Passive Perception 7\nNaN\nAnimated Flying Sword\\nSmall Construct, Unalig...\n\n\n4\nAnimated Rug of Smothering\nAnimated Objects\n2.000\nLarge\nConstruct\nNaN\nUnaligned\n12\n4\n27 (5d10)\n...\n-4\n-5\nNaN\nNaN\nNaN\nPoison, Psychic; Charmed, Deafened, Exhaustion...\nNaN\nBlindsight 60 ft.; Passive Perception 6\nNaN\nAnimated Rug of Smothering\\nLarge Construct, U...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n325\nVenomous Snake\nAnimals\n0.125\nTiny\nBeast\nNaN\nUnaligned\n12\n2\n5 (2d4)\n...\n0\n-4\nNaN\nNaN\nNaN\nNaN\nNaN\nBlindsight 10 ft.; Passive Perception 10\nNaN\nVenomous Snake\\nTiny Beast, Unaligned\\nAC 12 \\...\n\n\n326\nVulture\nAnimals\n0.000\nMedium\nBeast\nNaN\nUnaligned\n10\n0\n5 (1d8 + 1)\n...\n1\n-3\nPerception +3\nNaN\nNaN\nNaN\nNaN\nPassive Perception 13\nNaN\nVulture\\nMedium Beast, Unaligned\\nAC 10 \\t\\t ...\n\n\n327\nWarhorse\nAnimals\n0.500\nLarge\nBeast\nNaN\nUnaligned\n11\n1\n19 (3d10 + 3)\n...\n3\n-2\nNaN\nNaN\nNaN\nNaN\nNaN\nPassive Perception 11\nNaN\nWarhorse\\nLarge Beast, Unaligned\\nAC 11 \\t\\t ...\n\n\n328\nWeasel\nAnimals\n0.000\nTiny\nBeast\nNaN\nUnaligned\n13\n3\n1 (1d4 − 1)\n...\n1\n-4\nAcrobatics +5, Perception +3, Stealth +5\nNaN\nNaN\nNaN\nNaN\nDarkvision 60 ft.; Passive Perception 13\nNaN\nWeasel\\nTiny Beast, Unaligned\\nAC 13 \\t\\t ...\n\n\n329\nWolf\nAnimals\n0.250\nMedium\nBeast\nNaN\nUnaligned\n12\n2\n11 (2d8 + 2)\n...\n1\n-2\nPerception +5, Stealth +4\nNaN\nNaN\nNaN\nNaN\nDarkvision 60 ft.; Passive Perception 15\nNaN\nWolf\\nMedium Beast, Unaligned\\nAC 12 \\t\\t ...\n\n\n\n\n330 rows × 33 columns\n\n\n\n\nmonsters.columns\n\nIndex(['name', 'category', 'cr', 'size', 'type', 'descriptive_tags',\n       'alignment', 'ac', 'initiative', 'hp', 'hp_number', 'speed',\n       'speed_base_number', 'str', 'dex', 'con', 'int', 'wis', 'cha',\n       'str_save', 'dex_save', 'con_save', 'int_save', 'wis_save', 'cha_save',\n       'skills', 'resistances', 'vulnerabilities', 'immunities', 'gear',\n       'senses', 'languages', 'full_text'],\n      dtype='object')\n\n\n\nmonsters.describe()\n\n\n\n\n\n\n\n\ncr\nac\ninitiative\nhp_number\nspeed_base_number\nstr\ndex\ncon\nint\nwis\ncha\nstr_save\ndex_save\ncon_save\nint_save\nwis_save\ncha_save\n\n\n\n\ncount\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n\n\nmean\n4.551136\n14.287879\n3.148485\n86.669697\n30.878788\n15.384848\n12.833333\n15.178788\n7.863636\n11.815152\n9.918182\n2.675758\n2.118182\n2.784848\n-1.093939\n1.872727\n0.003030\n\n\nstd\n5.797444\n3.149589\n3.944803\n102.140570\n12.339566\n6.520047\n3.261563\n4.404492\n5.675860\n2.966748\n5.969220\n3.532010\n2.452213\n2.869886\n3.224190\n2.967224\n3.524554\n\n\nmin\n0.000000\n5.000000\n-5.000000\n1.000000\n5.000000\n1.000000\n1.000000\n8.000000\n1.000000\n3.000000\n1.000000\n-5.000000\n-5.000000\n-1.000000\n-5.000000\n-4.000000\n-5.000000\n\n\n25%\n0.500000\n12.000000\n1.000000\n18.250000\n30.000000\n11.000000\n10.000000\n12.000000\n2.000000\n10.000000\n5.000000\n0.000000\n1.000000\n1.000000\n-4.000000\n0.000000\n-3.000000\n\n\n50%\n2.000000\n14.000000\n2.000000\n52.000000\n30.000000\n16.000000\n13.000000\n14.500000\n7.000000\n12.000000\n8.000000\n3.000000\n2.000000\n2.000000\n-2.000000\n1.000000\n-1.000000\n\n\n75%\n6.000000\n17.000000\n4.000000\n119.000000\n40.000000\n19.000000\n15.000000\n17.000000\n12.000000\n13.000000\n14.000000\n4.000000\n3.000000\n4.000000\n1.000000\n3.000000\n2.000000\n\n\nmax\n30.000000\n25.000000\n20.000000\n697.000000\n60.000000\n30.000000\n28.000000\n30.000000\n25.000000\n25.000000\n30.000000\n17.000000\n10.000000\n15.000000\n12.000000\n12.000000\n12.000000\n\n\n\n\n\n\n\n\nmonsters.groupby([\"senses\"]).size().sort_values(ascending=False)\n\nsenses\nDarkvision 60 ft.; Passive Perception 10                         24\nPassive Perception 10                                            23\nDarkvision 60 ft.; Passive Perception 14                         15\nDarkvision 60 ft.; Passive Perception 13                         15\nDarkvision 60 ft.; Passive Perception 15                         14\n                                                                 ..\nDarkvision 30 ft.; Passive Perception 11                          1\nDarkvision 30 ft.; Passive Perception 13                          1\nDarkvision 30 ft.; Passive Perception 9                           1\nDarkvision 60 ft., Tremorsense 120 ft.; Passive Perception 16     1\nTruesight 60 ft.; Passive Perception 19                           1\nLength: 100, dtype: int64\n\n\n\nmonsters.groupby([\"size\"]).describe()[\"cr\"] \n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nsize\n\n\n\n\n\n\n\n\n\n\n\n\nGargantuan\n15.0\n21.066667\n4.366539\n11.000\n20.0000\n22.00\n23.000\n30.0\n\n\nHuge\n34.0\n9.264706\n5.088959\n2.000\n5.0000\n8.50\n13.750\n19.0\n\n\nLarge\n107.0\n5.003505\n4.626958\n0.125\n1.5000\n4.00\n7.500\n21.0\n\n\nMedium\n90.0\n2.183333\n3.178238\n0.000\n0.2500\n1.00\n3.000\n21.0\n\n\nMedium or Small\n36.0\n3.524306\n3.635124\n0.000\n0.8750\n3.00\n5.000\n15.0\n\n\nSmall\n23.0\n0.271739\n0.254650\n0.000\n0.0625\n0.25\n0.500\n1.0\n\n\nTiny\n25.0\n0.235000\n0.491225\n0.000\n0.0000\n0.00\n0.125\n2.0\n\n\n\n\n\n\n\n\nsns.heatmap(monsters.select_dtypes(include='number').corr())\n\n\n\n\n\n\n\n\n\nmonsters['senses'].values[:10]\n\narray(['Darkvision 120 ft.; Passive Perception 20',\n       'Darkvision 60 ft.; Passive Perception 10',\n       'Blindsight 60 ft.; Passive Perception 6',\n       'Blindsight 60 ft.; Passive Perception 7',\n       'Blindsight 60 ft.; Passive Perception 6',\n       'Darkvision 60 ft., Tremorsense 60 ft.; Passive Perception 11',\n       'Passive Perception 16', 'Passive Perception 10',\n       'Passive Perception 10', 'Passive Perception 10'], dtype=object)\n\n\n\nmonsters[monsters['senses'].astype(str).str.contains('unimpeded', case=False, na=False)]\n\n\n\n\n\n\n\n\nname\ncategory\ncr\nsize\ntype\ndescriptive_tags\nalignment\nac\ninitiative\nhp\n...\nwis_save\ncha_save\nskills\nresistances\nvulnerabilities\nimmunities\ngear\nsenses\nlanguages\nfull_text\n\n\n\n\n14\nBarbed Devil\nBarbed Devil\n5.0\nMedium\nFiend\nDevil\nLawful Evil\n15\n3\n110 (13d8 + 52)\n...\n5\n5\nDeception +5, Insight +5, Perception +8\nCold\nNaN\nFire, Poison; Poisoned\nNaN\nDarkvision 120 ft. (unimpeded by magical Darkn...\nInfernal; telepathy 120 ft.\nBarbed Devil\\nMedium Fiend (Devil), Lawful Evi...\n\n\n16\nBearded Devil\nBearded Devil\n3.0\nMedium\nFiend\nDevil\nLawful Evil\n13\n2\n58 (9d8 + 18)\n...\n0\n4\nNaN\nCold\nNaN\nFire, Poison; Frightened, Poisoned\nNaN\nDarkvision 120 ft. (unimpeded by magical Darkn...\nInfernal; telepathy 120 ft.\nBearded Devil\\nMedium Fiend (Devil), Lawful Ev...\n\n\n29\nBone Devil\nBone Devil\n9.0\nLarge\nFiend\nDevil\nLawful Evil\n16\n7\n161 (17d10 + 68)\n...\n6\n7\nDeception +7, Insight +6\nCold\nNaN\nFire, Poison; Poisoned\nNaN\nDarkvision 120 ft. (unimpeded by magical Darkn...\nInfernal; telepathy 120 ft.\nBone Devil\\nLarge Fiend (Devil), Lawful Evil\\n...\n\n\n42\nChain Devil\nChain Devil\n8.0\nMedium\nFiend\nDevil\nLawful Evil\n15\n5\n85 (10d8 + 40)\n...\n4\n2\nNaN\nBludgeoning, Cold, Piercing, Slashing\nNaN\nFire, Poison; Poisoned\nNaN\nDarkvision 120 ft. (unimpeded by magical Darkn...\nInfernal; telepathy 120 ft.\nChain Devil\\nMedium Fiend (Devil), Lawful Evil...\n\n\n117\nHorned Devil\nHorned Devil\n11.0\nLarge\nFiend\nDevil\nLawful Evil\n18\n7\n199 (19d10 + 95)\n...\n7\n8\nNaN\nCold\nNaN\nFire, Poison; Poisoned\nNaN\nDarkvision 150 ft. (unimpeded by magical Darkn...\nInfernal; telepathy 120 ft.\nHorned Devil\\nLarge Fiend (Devil), Lawful Evil...\n\n\n120\nImp\nImp\n1.0\nTiny\nFiend\nDevil\nLawful Evil\n13\n3\n21 (6d4 + 6)\n...\n1\n2\nDeception +4, Insight +3, Stealth +5\nCold\nNaN\nFire, Poison; Poisoned\nNaN\nDarkvision 120 ft. (unimpeded by magical Darkn...\nCommon, Infernal\nImp\\nTiny Fiend (Devil), Lawful Evil\\nAC 13 \\t...\n\n\n128\nLemure\nLemure\n0.0\nMedium\nFiend\nDevil\nLawful Evil\n9\n-3\n9 (2d8)\n...\n0\n-4\nNaN\nCold\nNaN\nFire, Poison; Charmed, Frightened, Poisoned\nNaN\nDarkvision 120 ft. (unimpeded by magical Darkn...\nUnderstands Infernal but can’t speak\nLemure\\nMedium Fiend (Devil), Lawful Evil\\nAC ...\n\n\n\n\n7 rows × 33 columns\n\n\n\n\nimport re\n\n\ndef extract_senses(sense_str):\n    result = {}\n    if not isinstance(sense_str, str):\n        return result\n\n    # Senses to extract\n    sense_names = ['darkvision', 'blindsight', 'tremorsense', 'truesight']\n    \n    # General pattern for senses with ft.\n    for match in re.findall(r'([a-zA-Z]+)\\s+(\\d+)\\s*ft*\\.?', sense_str, flags=re.IGNORECASE):\n        name, value = match\n        name = name.strip().lower()\n        if name in sense_names:\n            result[name] = int(value)\n    \n    # Passive Perception (no ft.)\n    pp_match = re.search(r'Passive Perception\\s+(\\d+)', sense_str, flags=re.IGNORECASE)\n    if pp_match:\n        result['passive perception'] = int(pp_match.group(1))\n    \n    return result\n\n\n\nsenses_df = monsters['senses'].apply(extract_senses).apply(pd.Series).fillna(0).astype(int)\nsenses_df\n#(senses_df['passive perception'] == 0).any()\n#senses_df[(senses_df == 0).sum(axis=1) == 2]\n#senses_df[senses_df[\"tremorsense\"] &gt; 0]\n\n\n\n\n\n\n\n\ndarkvision\npassive perception\nblindsight\ntremorsense\ntruesight\n\n\n\n\n0\n120\n20\n0\n0\n0\n\n\n1\n60\n10\n0\n0\n0\n\n\n2\n0\n6\n60\n0\n0\n\n\n3\n0\n7\n60\n0\n0\n\n\n4\n0\n6\n60\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n325\n0\n10\n10\n0\n0\n\n\n326\n0\n13\n0\n0\n0\n\n\n327\n0\n11\n0\n0\n0\n\n\n328\n60\n13\n0\n0\n0\n\n\n329\n60\n15\n0\n0\n0\n\n\n\n\n330 rows × 5 columns\n\n\n\n\nsenses_df_filtered = senses_df[(senses_df == 0).sum(axis=1) != 2]\nsenses_df_filtered\n\n\n\n\n\n\n\n\ndarkvision\npassive perception\nblindsight\ntremorsense\ntruesight\n\n\n\n\n0\n120\n20\n0\n0\n0\n\n\n1\n60\n10\n0\n0\n0\n\n\n2\n0\n6\n60\n0\n0\n\n\n3\n0\n7\n60\n0\n0\n\n\n4\n0\n6\n60\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n325\n0\n10\n10\n0\n0\n\n\n326\n0\n13\n0\n0\n0\n\n\n327\n0\n11\n0\n0\n0\n\n\n328\n60\n13\n0\n0\n0\n\n\n329\n60\n15\n0\n0\n0\n\n\n\n\n279 rows × 5 columns\n\n\n\n\nmonsters_mod = pd.concat([monsters, senses_df], axis=1)\n\n\nmonsters_mod = monsters_mod[[\"size\"]+list(monsters_mod.columns[-5:])]\n\n\nimport seaborn as sns\n\n\nlong_df = pd.melt(\n    senses_df, #filtered\n    id_vars=['passive perception'], #,'size'],             \n    value_vars=['darkvision', 'blindsight', 'tremorsense', 'truesight'], \n    var_name='sense_type',\n    value_name='distance'\n)\nlong_df #[(long_df[\"sense_type\"] == \"tremorsense\") & (long_df[\"distance\"] != 0)]\n\n\n\n\n\n\n\n\npassive perception\nsense_type\ndistance\n\n\n\n\n0\n20\ndarkvision\n120\n\n\n1\n10\ndarkvision\n60\n\n\n2\n6\ndarkvision\n0\n\n\n3\n7\ndarkvision\n0\n\n\n4\n6\ndarkvision\n0\n\n\n...\n...\n...\n...\n\n\n1315\n10\ntruesight\n0\n\n\n1316\n13\ntruesight\n0\n\n\n1317\n11\ntruesight\n0\n\n\n1318\n13\ntruesight\n0\n\n\n1319\n15\ntruesight\n0\n\n\n\n\n1320 rows × 3 columns\n\n\n\n\n\n\nCases with only passive perception are not there. If only long_df is used then there are lot many extra points.\nTwo points for cases with more than two senses.\n\n\nsns.set_theme(style=\"dark\", font=\"Comic Sans MS\")\ncurrent_style = sns.axes_style()\ncolors = [\"grey\", \"orange\", \"dodgerblue\", \"salmon\"]\nplot1 = sns.catplot(data=long_df[long_df[\"distance\"] != 0], x=\"distance\", y=\"passive perception\", hue=\"sense_type\",\\\n            kind=\"strip\", dodge=True, height=4, aspect=2, size=4, native_scale=True,\\\n                   jitter=0.25, palette=colors)\nplot1._legend.remove()\nplot1.add_legend(title='', ncol=4, bbox_to_anchor=(0.5, 1.05))\nfor ind, text in enumerate(plot1._legend.texts):\n    text.set_color(colors[ind]) \nfor handle in plot1._legend.legend_handles:\n    handle.set_visible(False)\nplot1.fig.set_facecolor(current_style['axes.facecolor'])\nsns.despine(left=True, bottom=True, right=True, top=True)\nfor ax in plot1.axes.flat:\n    ax.grid(axis='y', which='major')\n\nplt.xticks(ticks=range(0,151,30))\nplt.xlabel(\"Distance (feet)\")\n#plt.savefig(\"senses.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n#plot1 = sns.catplot(data=long_df, x=\"distance\", y=\"passive perception\", hue=\"size\",\\\n#            kind=\"strip\", dodge=True, height=4, aspect=2, size=4, native_scale=True,\\\n#                   jitter=1, col=\"sense_type\", col_wrap=2)\n#plot1._legend.remove()\n#plot1.add_legend(title='', ncol=7, bbox_to_anchor=(0.5, 1.05))\n#plt.xticks(ticks=range(0,151,30))\n#plt.xlabel(\"Distance (feet)\")\n#plt.tight_layout()\n#plt.show()\n\n\nsns.dark_palette(\"xkcd:golden\", 4)\n\n\n\n\n\n\n\n\nfrom upsetplot import plot\nfrom upsetplot import UpSet\nimport textwrap\nsns.reset_defaults()\n\n\nbinary_df = senses_df &gt; 0\ncounts = binary_df.value_counts().sort_values(ascending=False)\n\n\nupset = UpSet(counts, sort_by= \"cardinality\", show_percentages=True, facecolor=\"dodgerblue\")\nupset.style_subsets(present=\"truesight\", edgecolor=\"lightgreen\", linewidth=1)\nupset.style_subsets(present=\"tremorsense\", edgecolor=\"salmon\", linewidth=1)\n\n#upset.add_catplot(value=\"progression\", kind=\"strip\", color=\"blue\")\nupset.plot()\nfor ind, ax in enumerate(plt.gcf().get_axes()):\n    if(ind == 3):\n        ax.set_yticks(range(0,151,50))\n        ax.set_facecolor(\"whitesmoke\")\n        ax.yaxis.grid(True, color=\"#D0D0D0\")\n    if(ind == 2):\n        ax.xaxis.grid(True, color=\"#D0D0D0\")\n    ax.spines['left'].set_visible(False)\n    ax.tick_params(axis='both', length=0)\n\n    for text in ax.texts:\n        if \"%\" in text.get_text():  \n            text.set_fontsize(9)\n#                text.set_fontfamily(\"Consolas\")\ntitle_text = \"Frequency of five senses and their combinations across all the characters in Dungeons and Dragons Monsters (2024)\"\nplt.suptitle(\"\\n\".join(textwrap.wrap(title_text, width=20)), x=0.075, y=0.8, ha=\"left\", fontfamily=\"Serif\")\nplt.savefig(\"senses_comb.png\", dpi=300, facecolor=\"whitesmoke\", bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "posts/Dungeons_Dragons/DD_monsters.html#upset-plot",
    "href": "posts/Dungeons_Dragons/DD_monsters.html#upset-plot",
    "title": "Dungeons and Dragons Monsters (2024)",
    "section": "",
    "text": "from upsetplot import plot\nfrom upsetplot import UpSet\nimport textwrap\nsns.reset_defaults()\n\n\nbinary_df = senses_df &gt; 0\ncounts = binary_df.value_counts().sort_values(ascending=False)\n\n\nupset = UpSet(counts, sort_by= \"cardinality\", show_percentages=True, facecolor=\"dodgerblue\")\nupset.style_subsets(present=\"truesight\", edgecolor=\"lightgreen\", linewidth=1)\nupset.style_subsets(present=\"tremorsense\", edgecolor=\"salmon\", linewidth=1)\n\n#upset.add_catplot(value=\"progression\", kind=\"strip\", color=\"blue\")\nupset.plot()\nfor ind, ax in enumerate(plt.gcf().get_axes()):\n    if(ind == 3):\n        ax.set_yticks(range(0,151,50))\n        ax.set_facecolor(\"whitesmoke\")\n        ax.yaxis.grid(True, color=\"#D0D0D0\")\n    if(ind == 2):\n        ax.xaxis.grid(True, color=\"#D0D0D0\")\n    ax.spines['left'].set_visible(False)\n    ax.tick_params(axis='both', length=0)\n\n    for text in ax.texts:\n        if \"%\" in text.get_text():  \n            text.set_fontsize(9)\n#                text.set_fontfamily(\"Consolas\")\ntitle_text = \"Frequency of five senses and their combinations across all the characters in Dungeons and Dragons Monsters (2024)\"\nplt.suptitle(\"\\n\".join(textwrap.wrap(title_text, width=20)), x=0.075, y=0.8, ha=\"left\", fontfamily=\"Serif\")\nplt.savefig(\"senses_comb.png\", dpi=300, facecolor=\"whitesmoke\", bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Visualization of various datasets using Python and R",
    "section": "",
    "text": "The US Judges data\n\n\n\nPillow\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nGender and racial diversity among the judges.\n\n\n\n\n\nJun 10, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nProject Gutenberg\n\n\n\nggplot2\n\nwordcloud\n\nTidyTuesday\n\nmagick\n\n\n\nFrequency of books in different languages.\n\n\n\n\n\nJun 3, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nDungeons and Dragons Monsters (2024)\n\n\n\nUpset\n\nseaborn\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nFrequency of five senses and their combinations across all the characters.\n\n\n\n\n\nMay 27, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nWater Quality at Sydney Beaches\n\n\n\nggplot2\n\nheatmap\n\nTidyTuesday\n\n\n\nRanking of Sydney beaches based on Enterococci concentration\n\n\n\n\n\nMay 20, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nSeismic events at Mount Vesuvius\n\n\n\nplotly\n\npolar\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nFrequency of earthquakes at different times of the day.\n\n\n\n\n\nMay 13, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nNSF funding cut in April 2025\n\n\n\nsunburst\n\nplotly\n\nNSF\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nDistribution of funding cuts across directorates and divisions.\n\n\n\n\n\nMay 7, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nuseR2025 conference data analysis\n\n\n\nggplot2\n\nword count\n\nTidyTuesday\n\n\n\nTop keywords in useR 2025 talks\n\n\n\n\n\nMay 1, 2025\n\n\nManish Datt\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Gutenberg_book/Gutenberg_books.html",
    "href": "posts/Gutenberg_book/Gutenberg_books.html",
    "title": "Project Gutenberg",
    "section": "",
    "text": "library(tidyverse)\nlibrary(wordcloud)\nlibrary(RColorBrewer)\nlibrary(ISOcodes)\nlibrary(scales)\nlibrary(magick)\n\n\ngutenberg_authors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_authors.csv')\ngutenberg_languages &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_languages.csv')\ngutenberg_metadata &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_metadata.csv')\ngutenberg_subjects &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_subjects.csv')\n\nglimpse(gutenberg_authors)\n\nRows: 26,077\nColumns: 7\n$ gutenberg_author_id &lt;dbl&gt; 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ author              &lt;chr&gt; \"United States\", \"Lincoln, Abraham\", \"Henry, Patri…\n$ alias               &lt;chr&gt; \"U.S.A.\", NA, NA, NA, \"Dodgson, Charles Lutwidge\",…\n$ birthdate           &lt;dbl&gt; NA, 1809, 1736, 1849, 1832, NA, 1819, 1860, NA, 18…\n$ deathdate           &lt;dbl&gt; NA, 1865, 1799, 1931, 1898, NA, 1891, 1937, NA, 18…\n$ wikipedia           &lt;chr&gt; \"https://en.wikipedia.org/wiki/United_States\", \"ht…\n$ aliases             &lt;chr&gt; \"U.S.A.\", \"United States President (1861-1865)/Lin…\n\nglimpse(gutenberg_languages)\n\nRows: 76,205\nColumns: 3\n$ gutenberg_id    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ language        &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ total_languages &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n\nglimpse(gutenberg_metadata)\n\nRows: 79,491\nColumns: 8\n$ gutenberg_id        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, 12, 13, 14,…\n$ title               &lt;chr&gt; \"The Declaration of Independence of the United Sta…\n$ author              &lt;chr&gt; \"Jefferson, Thomas\", \"United States\", \"Kennedy, Jo…\n$ gutenberg_author_id &lt;dbl&gt; 1638, 1, 1666, 3, 1, 4, NA, 3, 3, NA, 7, 7, 7, 7, …\n$ language            &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"e…\n$ gutenberg_bookshelf &lt;chr&gt; \"Politics/American Revolutionary War/United States…\n$ rights              &lt;chr&gt; \"Public domain in the USA.\", \"Public domain in the…\n$ has_text            &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n\nglimpse(gutenberg_subjects)\n\nRows: 255,312\nColumns: 3\n$ gutenberg_id &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, …\n$ subject_type &lt;chr&gt; \"lcsh\", \"lcsh\", \"lcc\", \"lcc\", \"lcsh\", \"lcsh\", \"lcc\", \"lcc…\n$ subject      &lt;chr&gt; \"United States -- History -- Revolution, 1775-1783 -- Sou…\n\n\n\nlang_counts &lt;- gutenberg_languages %&gt;% \n  group_by(language) %&gt;%\n  summarise(total_languages = sum(total_languages), .groups = 'drop') %&gt;%\n  arrange(desc(total_languages)) %&gt;% \n  left_join(ISO_639_2, by = c(\"language\" = \"Alpha_2\")) %&gt;% \n  select(language, Name, total_languages)\nglimpse(lang_counts)\n\nRows: 70\nColumns: 3\n$ language        &lt;chr&gt; \"en\", \"fr\", \"fi\", \"de\", \"it\", \"nl\", \"es\", \"pt\", \"hu\", …\n$ Name            &lt;chr&gt; \"English\", \"French\", \"Finnish\", \"German\", \"Italian\", \"…\n$ total_languages &lt;dbl&gt; 60875, 4019, 3314, 2363, 1061, 1053, 917, 651, 610, 45…\n\n\n\npng(\"word_cloud.png\", width = 4, height = 4, units = \"in\", res = 300, bg = \"black\")\np1 &lt;- wordcloud(words = lang_counts$Name, freq = lang_counts$total_languages,\n          min.freq = 0,\n          random.order = FALSE,\n          colors = brewer.pal(9, \"Pastel1\"))\ndev.off()\n\npng \n  2 \n\n\n\nlang_counts %&gt;% \n  filter(language != \"en\") %&gt;% \n  arrange(desc(total_languages)) %&gt;% \n  slice(1:10) %&gt;% \n  ggplot(aes(x = total_languages, y =reorder(Name, total_languages))) +\n  geom_col(alpha=0) +\n  geom_text(aes(label = comma(total_languages), x=70), color = \"grey\", hjust=1) +\n  theme_minimal() +\n  scale_y_discrete(labels = function(x) paste0(x, \":\"))+\n  coord_cartesian(xlim = c(0, 100)) +\n  theme(axis.title = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.text.y = element_text(color = \"grey\"),\n        axis.line = element_blank(),\n        panel.grid = element_blank(),\n        plot.margin = margin(75, 0, 75, 0))\n\n\n\n\n\n\n\nggsave(\"plot1.png\", width = 1.75, height = 4, units = \"in\", bg=\"black\")\n\n\nimg1 &lt;- image_read(\"word_cloud.png\")\nimg2 &lt;- image_read(\"plot1.png\")\ncombined &lt;- image_append(c(img1, img2), stack = FALSE)\n#image_write(combined, \"combined.png\")\n\ntitle_text &lt;- paste(\"In Project Gutenberg, \", comma(lang_counts$total_languages[1]), \" out of \", comma(dim(gutenberg_languages)[1]), \"books are in English. The word cloud is based on the book counts in different languages, excluding English. Counts for top ten languages are shown on the right.\")\ntitle_text &lt;- paste(strwrap(title_text, width = 75), collapse = \"\\n\")\ntitle_image &lt;- image_blank(width = image_info(combined)$width,\n                           height = 160, # adjust height as needed\n                           color = \"black\") %&gt;%\n  image_annotate(text = title_text,\n                 size = 40, # adjust font size\n                 color = \"grey\",\n                 gravity = \"center\")\n\nfinal_image &lt;- image_composite(combined, title_image, offset = \"+0+0\")\nimage_write(final_image, \"combined_with_title.png\")"
  },
  {
    "objectID": "posts/Gutenberg_book/Gutenberg_books.html#tidytuesday-data-for-2025-06-03",
    "href": "posts/Gutenberg_book/Gutenberg_books.html#tidytuesday-data-for-2025-06-03",
    "title": "Project Gutenberg",
    "section": "",
    "text": "library(tidyverse)\nlibrary(wordcloud)\nlibrary(RColorBrewer)\nlibrary(ISOcodes)\nlibrary(scales)\nlibrary(magick)\n\n\ngutenberg_authors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_authors.csv')\ngutenberg_languages &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_languages.csv')\ngutenberg_metadata &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_metadata.csv')\ngutenberg_subjects &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_subjects.csv')\n\nglimpse(gutenberg_authors)\n\nRows: 26,077\nColumns: 7\n$ gutenberg_author_id &lt;dbl&gt; 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ author              &lt;chr&gt; \"United States\", \"Lincoln, Abraham\", \"Henry, Patri…\n$ alias               &lt;chr&gt; \"U.S.A.\", NA, NA, NA, \"Dodgson, Charles Lutwidge\",…\n$ birthdate           &lt;dbl&gt; NA, 1809, 1736, 1849, 1832, NA, 1819, 1860, NA, 18…\n$ deathdate           &lt;dbl&gt; NA, 1865, 1799, 1931, 1898, NA, 1891, 1937, NA, 18…\n$ wikipedia           &lt;chr&gt; \"https://en.wikipedia.org/wiki/United_States\", \"ht…\n$ aliases             &lt;chr&gt; \"U.S.A.\", \"United States President (1861-1865)/Lin…\n\nglimpse(gutenberg_languages)\n\nRows: 76,205\nColumns: 3\n$ gutenberg_id    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ language        &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ total_languages &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n\nglimpse(gutenberg_metadata)\n\nRows: 79,491\nColumns: 8\n$ gutenberg_id        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, 12, 13, 14,…\n$ title               &lt;chr&gt; \"The Declaration of Independence of the United Sta…\n$ author              &lt;chr&gt; \"Jefferson, Thomas\", \"United States\", \"Kennedy, Jo…\n$ gutenberg_author_id &lt;dbl&gt; 1638, 1, 1666, 3, 1, 4, NA, 3, 3, NA, 7, 7, 7, 7, …\n$ language            &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"e…\n$ gutenberg_bookshelf &lt;chr&gt; \"Politics/American Revolutionary War/United States…\n$ rights              &lt;chr&gt; \"Public domain in the USA.\", \"Public domain in the…\n$ has_text            &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n\nglimpse(gutenberg_subjects)\n\nRows: 255,312\nColumns: 3\n$ gutenberg_id &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, …\n$ subject_type &lt;chr&gt; \"lcsh\", \"lcsh\", \"lcc\", \"lcc\", \"lcsh\", \"lcsh\", \"lcc\", \"lcc…\n$ subject      &lt;chr&gt; \"United States -- History -- Revolution, 1775-1783 -- Sou…\n\n\n\nlang_counts &lt;- gutenberg_languages %&gt;% \n  group_by(language) %&gt;%\n  summarise(total_languages = sum(total_languages), .groups = 'drop') %&gt;%\n  arrange(desc(total_languages)) %&gt;% \n  left_join(ISO_639_2, by = c(\"language\" = \"Alpha_2\")) %&gt;% \n  select(language, Name, total_languages)\nglimpse(lang_counts)\n\nRows: 70\nColumns: 3\n$ language        &lt;chr&gt; \"en\", \"fr\", \"fi\", \"de\", \"it\", \"nl\", \"es\", \"pt\", \"hu\", …\n$ Name            &lt;chr&gt; \"English\", \"French\", \"Finnish\", \"German\", \"Italian\", \"…\n$ total_languages &lt;dbl&gt; 60875, 4019, 3314, 2363, 1061, 1053, 917, 651, 610, 45…\n\n\n\npng(\"word_cloud.png\", width = 4, height = 4, units = \"in\", res = 300, bg = \"black\")\np1 &lt;- wordcloud(words = lang_counts$Name, freq = lang_counts$total_languages,\n          min.freq = 0,\n          random.order = FALSE,\n          colors = brewer.pal(9, \"Pastel1\"))\ndev.off()\n\npng \n  2 \n\n\n\nlang_counts %&gt;% \n  filter(language != \"en\") %&gt;% \n  arrange(desc(total_languages)) %&gt;% \n  slice(1:10) %&gt;% \n  ggplot(aes(x = total_languages, y =reorder(Name, total_languages))) +\n  geom_col(alpha=0) +\n  geom_text(aes(label = comma(total_languages), x=70), color = \"grey\", hjust=1) +\n  theme_minimal() +\n  scale_y_discrete(labels = function(x) paste0(x, \":\"))+\n  coord_cartesian(xlim = c(0, 100)) +\n  theme(axis.title = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.text.y = element_text(color = \"grey\"),\n        axis.line = element_blank(),\n        panel.grid = element_blank(),\n        plot.margin = margin(75, 0, 75, 0))\n\n\n\n\n\n\n\nggsave(\"plot1.png\", width = 1.75, height = 4, units = \"in\", bg=\"black\")\n\n\nimg1 &lt;- image_read(\"word_cloud.png\")\nimg2 &lt;- image_read(\"plot1.png\")\ncombined &lt;- image_append(c(img1, img2), stack = FALSE)\n#image_write(combined, \"combined.png\")\n\ntitle_text &lt;- paste(\"In Project Gutenberg, \", comma(lang_counts$total_languages[1]), \" out of \", comma(dim(gutenberg_languages)[1]), \"books are in English. The word cloud is based on the book counts in different languages, excluding English. Counts for top ten languages are shown on the right.\")\ntitle_text &lt;- paste(strwrap(title_text, width = 75), collapse = \"\\n\")\ntitle_image &lt;- image_blank(width = image_info(combined)$width,\n                           height = 160, # adjust height as needed\n                           color = \"black\") %&gt;%\n  image_annotate(text = title_text,\n                 size = 40, # adjust font size\n                 color = \"grey\",\n                 gravity = \"center\")\n\nfinal_image &lt;- image_composite(combined, title_image, offset = \"+0+0\")\nimage_write(final_image, \"combined_with_title.png\")"
  },
  {
    "objectID": "posts/NSF_grants/NSF_grants_cut.html",
    "href": "posts/NSF_grants/NSF_grants_cut.html",
    "title": "NSF funding cut in April 2025",
    "section": "",
    "text": "import pandas as pd\nimport plotly.express as px\nnsf_terminations = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-06/nsf_terminations.csv')\nnsf_terminations\n\n\n\n\n\n\n\n\ngrant_number\nproject_title\ntermination_letter_date\norg_name\norg_city\norg_state\norg_district\nusaspending_obligated\naward_type\ndirectorate_abbrev\n...\ndivision\nnsf_program_name\nnsf_url\nusaspending_url\nnsf_startdate\nnsf_expected_end_date\norg_zip\norg_uei\nabstract\nin_cruz_list\n\n\n\n\n0\n2135329\nCollaborative Research: Research: Early-Career...\n2025-04-25\nUniversity of New Mexico\nALBUQUERQUE\nNM\nNM01\n190725.0\nStandard Grant\nENG\n...\nEngineering Education and Centers\nER2-Ethical & Responsible Res\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_213...\n2022-09-01\n2025-04-18\n871310001\nF6XLTRUQJEN4\nTransportation systems, computing algorithms, ...\nTrue\n\n\n1\n2342099\nMyTurn: An Afterschool Social Robotics Program...\n2025-04-25\nUniversity of Illinois at Chicago\nCHICAGO\nIL\nIL07\n499999.0\nStandard Grant\nEDU\n...\nResearch on Learning in Formal and Informal Se...\nITEST-Inov Tech Exp Stu & Teac\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_234...\n2024-08-15\n2027-07-31\n606124305\nW8XEAJDKMXH3\nComputational thinking and robotics are increa...\nFalse\n\n\n2\n2201103\nCollaborative Research: The Organizational Cli...\n2025-04-25\nAmerican Society For Engineering Education\nWASHINGTON\nDC\nDC00\n124241.0\nContinuing Grant\nEDU\n...\nEquity for Excellence in STEM\nECR-EDU Core Research\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_220...\n2022-08-01\n2026-07-31\n200362476\nF6G9C4HMNHW4\nThe ongoing lack of diversity in the engineeri...\nTrue\n\n\n3\n2215382\nEngaging Rural, Latinx Youth in an After Schoo...\n2025-04-25\nTERC Inc\nCAMBRIDGE\nMA\nMA05\n2601763.0\nContinuing Grant\nEDU\n...\nResearch on Learning in Formal and Informal Se...\nAISL\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_221...\n2022-08-01\n2026-07-31\n21401339\nGSLCJ3M62XX1\nThe project will develop and research an after...\nTrue\n\n\n4\n2405633\nDesign Effective and Equitable Professional Le...\n2025-04-25\nSan Francisco State University\nSAN FRANCISCO\nCA\nCA11\n565771.0\nContinuing Grant\nEDU\n...\nResearch on Learning in Formal and Informal Se...\nDiscovery Research K-12\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_240...\n2024-10-01\n2028-09-30\n941321740\nF4SLJ5WF59F6\nProviding computer science (CS) education to s...\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1036\n2411129\nCommunity-Situated Data Practices in Multiethn...\n2025-04-18\nMichigan State University\nEAST LANSING\nMI\nMI07\n1736866.0\nContinuing Grant\nEDU\n...\nResearch on Learning in Formal and Informal Se...\nDiscovery Research K-12\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_241...\n2024-09-15\n2029-08-31\n488242600\nR28EKN92ZTZ9\nBroadening STEM research and education to incl...\nFalse\n\n\n1037\n2224674\nCollaborative Research: Engaging Marginalized ...\n2025-04-18\nGeorge Mason University\nFAIRFAX\nVA\nVA11\n439380.0\nContinuing Grant\nEDU\n...\nResearch on Learning in Formal and Informal Se...\nECR-EDU Core Research\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_222...\n2023-06-15\n2028-05-31\n220304422\nEADLFP7Z72E5\nThis collaborative project investigates the la...\nTrue\n\n\n1038\n2315024\nCollaborative Research: Overcoming Isolation a...\n2025-04-18\nOhio State University\nCOLUMBUS\nOH\nOH03\n378441.0\nStandard Grant\nEDU\n...\nGraduate Education\nADVANCE\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_231...\n2023-09-15\n2025-04-18\n432101016\nDLWBSLWAJWR1\nThere is a growing need for scholars specializ...\nTrue\n\n\n1039\n2216826\nCommunity of Neighboring and National Entrepre...\n2025-04-18\nSt. Catherine University\nSAINT PAUL\nMN\nMN04\n75000.0\nStandard Grant\nBIO\n...\nBiological Infrastructure\nUBE - Undergraduate Biology Ed\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_221...\n2022-09-01\n2025-08-31\n551051750\nRQJ5KM1LQ935\nThis project aims to serve the national intere...\nTrue\n\n\n1040\n2210842\nCollaborative Research: HCC: Designing Technol...\n2025-04-18\nUniversity of California-Irvine\nIRVINE\nCA\nCA47\n90626.0\nStandard Grant\nCISE\n...\nInformation and Intelligent Systems\nHCC-Human-Centered Computing\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_221...\n2022-10-01\n2025-04-18\n926970001\nMJC5FCYQTPE6\nThis award supports research that examines the...\nTrue\n\n\n\n\n1041 rows × 21 columns\nnsf_terminations.columns\n\nIndex(['grant_number', 'project_title', 'termination_letter_date', 'org_name',\n       'org_city', 'org_state', 'org_district', 'usaspending_obligated',\n       'award_type', 'directorate_abbrev', 'directorate', 'division',\n       'nsf_program_name', 'nsf_url', 'usaspending_url', 'nsf_startdate',\n       'nsf_expected_end_date', 'org_zip', 'org_uei', 'abstract',\n       'in_cruz_list'],\n      dtype='object')"
  },
  {
    "objectID": "posts/NSF_grants/NSF_grants_cut.html#sunburst-plot-using-plotly",
    "href": "posts/NSF_grants/NSF_grants_cut.html#sunburst-plot-using-plotly",
    "title": "NSF funding cut in April 2025",
    "section": "Sunburst plot using plotly",
    "text": "Sunburst plot using plotly\n\nnsf_grp = nsf_terminations.groupby(['directorate', 'division'], as_index=False)['usaspending_obligated'].sum()\n\ncustom_colorscale = [\n    [0.0, 'black'],       # 0 → black\n    [0.000001, 'rgb(68,1,84)'],   # Start of Viridis after 0\n    [0.25, 'rgb(59,82,139)'],\n    [0.5, 'rgb(33,145,140)'],\n    [0.75, 'rgb(94,201,98)'],\n    [1.0, 'rgb(253,231,37)']\n]\n\nfig = px.sunburst(nsf_grp, path=['directorate', 'division'], values='usaspending_obligated', \\\n                  color='usaspending_obligated', color_continuous_scale=custom_colorscale, width=900, height=700, \\\n                  title=f\"Distribution of the NSF funding cut of &lt;b&gt;${nsf_grp[\"usaspending_obligated\"].sum():,.0f}&lt;/b&gt; in April 2025.\"\n                 )\nfig.update_traces(textinfo='label+percent entry')\nfor i in range(len(fig.data[0].marker.colors) - len(nsf_grp['directorate'].unique()), len(fig.data[0].marker.colors)):\n    fig.data[0].marker.colors[i] = 0\n#fig.write_image(\"sunburst_plot.png\")\nfig.show()"
  },
  {
    "objectID": "posts/Swim_sites_Sydney/swim_sites.html",
    "href": "posts/Swim_sites_Sydney/swim_sites.html",
    "title": "Water Quality at Sydney Beaches",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggtext)\n\n\nwater_quality &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-20/water_quality.csv')\nwater_quality\n\n# A tibble: 123,530 × 10\n   region         council       swim_site date       time  enterococci_cfu_100ml\n   &lt;chr&gt;          &lt;chr&gt;         &lt;chr&gt;     &lt;date&gt;     &lt;tim&gt;                 &lt;dbl&gt;\n 1 Western Sydney Hawkesbury C… Windsor … 2025-04-28 11:00                   620\n 2 Sydney Harbour North Sydney… Hayes St… 2025-04-28 11:40                    64\n 3 Sydney Harbour Willoughby C… Northbri… 2025-04-28 10:54                   160\n 4 Sydney Harbour Northern Bea… Fairligh… 2025-04-28 09:28                    54\n 5 Western Sydney Hawkesbury C… Yarramun… 2025-04-28 10:35                   720\n 6 Sydney Harbour Northern Bea… Little M… 2025-04-28 09:19                   230\n 7 Sydney Harbour City of Cana… Chiswick… 2025-04-28 13:06                   120\n 8 Sydney Harbour Inner West C… Dawn Fra… 2025-04-28 08:04                   280\n 9 Sydney Harbour Woollahra Mu… Rose Bay… 2025-04-28 08:50                    60\n10 Sydney Harbour Woollahra Mu… Camp Cove 2025-04-28 09:09                   100\n# ℹ 123,520 more rows\n# ℹ 4 more variables: water_temperature_c &lt;dbl&gt;, conductivity_ms_cm &lt;dbl&gt;,\n#   latitude &lt;dbl&gt;, longitude &lt;dbl&gt;\n\n\n\nglimpse(water_quality)\n\nRows: 123,530\nColumns: 10\n$ region                &lt;chr&gt; \"Western Sydney\", \"Sydney Harbour\", \"Sydney Harb…\n$ council               &lt;chr&gt; \"Hawkesbury City Council\", \"North Sydney Council…\n$ swim_site             &lt;chr&gt; \"Windsor Beach\", \"Hayes Street Beach\", \"Northbri…\n$ date                  &lt;date&gt; 2025-04-28, 2025-04-28, 2025-04-28, 2025-04-28,…\n$ time                  &lt;time&gt; 11:00:00, 11:40:00, 10:54:00, 09:28:00, 10:35:0…\n$ enterococci_cfu_100ml &lt;dbl&gt; 620, 64, 160, 54, 720, 230, 120, 280, 60, 100, 1…\n$ water_temperature_c   &lt;dbl&gt; 20, 21, 21, 21, 18, 21, 21, 21, 22, 22, 20, 20, …\n$ conductivity_ms_cm    &lt;dbl&gt; 248, 45250, 48930, 52700, 64, 39140, 4845, 50600…\n$ latitude              &lt;dbl&gt; -33.60448, -33.84172, -33.80604, -33.80073, -33.…\n$ longitude             &lt;dbl&gt; 150.8170, 151.2194, 151.2228, 151.2748, 150.6979…"
  },
  {
    "objectID": "posts/Swim_sites_Sydney/swim_sites.html#tidytuesday-data-for-2025-05-20",
    "href": "posts/Swim_sites_Sydney/swim_sites.html#tidytuesday-data-for-2025-05-20",
    "title": "Water Quality at Sydney Beaches",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggtext)\n\n\nwater_quality &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-20/water_quality.csv')\nwater_quality\n\n# A tibble: 123,530 × 10\n   region         council       swim_site date       time  enterococci_cfu_100ml\n   &lt;chr&gt;          &lt;chr&gt;         &lt;chr&gt;     &lt;date&gt;     &lt;tim&gt;                 &lt;dbl&gt;\n 1 Western Sydney Hawkesbury C… Windsor … 2025-04-28 11:00                   620\n 2 Sydney Harbour North Sydney… Hayes St… 2025-04-28 11:40                    64\n 3 Sydney Harbour Willoughby C… Northbri… 2025-04-28 10:54                   160\n 4 Sydney Harbour Northern Bea… Fairligh… 2025-04-28 09:28                    54\n 5 Western Sydney Hawkesbury C… Yarramun… 2025-04-28 10:35                   720\n 6 Sydney Harbour Northern Bea… Little M… 2025-04-28 09:19                   230\n 7 Sydney Harbour City of Cana… Chiswick… 2025-04-28 13:06                   120\n 8 Sydney Harbour Inner West C… Dawn Fra… 2025-04-28 08:04                   280\n 9 Sydney Harbour Woollahra Mu… Rose Bay… 2025-04-28 08:50                    60\n10 Sydney Harbour Woollahra Mu… Camp Cove 2025-04-28 09:09                   100\n# ℹ 123,520 more rows\n# ℹ 4 more variables: water_temperature_c &lt;dbl&gt;, conductivity_ms_cm &lt;dbl&gt;,\n#   latitude &lt;dbl&gt;, longitude &lt;dbl&gt;\n\n\n\nglimpse(water_quality)\n\nRows: 123,530\nColumns: 10\n$ region                &lt;chr&gt; \"Western Sydney\", \"Sydney Harbour\", \"Sydney Harb…\n$ council               &lt;chr&gt; \"Hawkesbury City Council\", \"North Sydney Council…\n$ swim_site             &lt;chr&gt; \"Windsor Beach\", \"Hayes Street Beach\", \"Northbri…\n$ date                  &lt;date&gt; 2025-04-28, 2025-04-28, 2025-04-28, 2025-04-28,…\n$ time                  &lt;time&gt; 11:00:00, 11:40:00, 10:54:00, 09:28:00, 10:35:0…\n$ enterococci_cfu_100ml &lt;dbl&gt; 620, 64, 160, 54, 720, 230, 120, 280, 60, 100, 1…\n$ water_temperature_c   &lt;dbl&gt; 20, 21, 21, 21, 18, 21, 21, 21, 22, 22, 20, 20, …\n$ conductivity_ms_cm    &lt;dbl&gt; 248, 45250, 48930, 52700, 64, 39140, 4845, 50600…\n$ latitude              &lt;dbl&gt; -33.60448, -33.84172, -33.80604, -33.80073, -33.…\n$ longitude             &lt;dbl&gt; 150.8170, 151.2194, 151.2228, 151.2748, 150.6979…"
  },
  {
    "objectID": "posts/Swim_sites_Sydney/swim_sites.html#data-wrangling",
    "href": "posts/Swim_sites_Sydney/swim_sites.html#data-wrangling",
    "title": "Water Quality at Sydney Beaches",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nwq_grp &lt;- water_quality %&gt;%\n  filter(!is.na(enterococci_cfu_100ml)) %&gt;% \n  mutate(date_y = format(date, \"%Y\")) %&gt;% \n  group_by(date_y, swim_site) %&gt;% \n  summarise(mean_enterococci = mean(enterococci_cfu_100ml, na.rm = TRUE)) %&gt;% \n  slice_min(order_by = mean_enterococci, n = 5)\n\nsite_freq &lt;- wq_grp %&gt;%\n  ungroup() %&gt;% \n  count(swim_site, sort = TRUE)"
  },
  {
    "objectID": "posts/Swim_sites_Sydney/swim_sites.html#plotting",
    "href": "posts/Swim_sites_Sydney/swim_sites.html#plotting",
    "title": "Water Quality at Sydney Beaches",
    "section": "Plotting",
    "text": "Plotting\n\nwq_grp%&gt;% \n  mutate(swim_site = fct_rev(factor(swim_site, levels = site_freq$swim_site))) %&gt;% \n  ggplot(aes(x = date_y, y=swim_site, fill=mean_enterococci)) +\n  scale_fill_gradient(low = \"lightblue\", high = \"blue\") +\n  geom_tile(color=\"white\") +\n  labs(\n    title = \"Year-wise top five swim-sites based on the lowest *Enterococci* concentration\",\n    x = element_blank(),\n    y = element_blank(),\n    fill = \"Average *Enterococci* &lt;br&gt; CFU/100ml\",\n    caption = \"Colony Forming Units (CFU) indicate number of viable bacteria that can form colonies when grown in a lab.\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(angle = 90, hjust = 1),\n    plot.title = element_textbox_simple(\n      padding = margin(5.5, 5.5, 5.5, 5.5),\n      margin = margin(5, 0, 10, 0)),\n    legend.title = element_markdown(hjust = 0.5),\n    legend.position = \"inside\",\n    legend.position.inside = c(0.80, 0.1),\n    legend.title.position = \"top\",\n    plot.caption = element_textbox_simple(\n      margin = margin(0, 0, 10, 175),\n      padding = margin(5.5, 0, 0, 0),\n      hjust = 0,\n      size = 8,\n      color = \"gray40\",\n      lineheight = 1.2\n    ),\n    axis.title.x = element_text(family = \"Consolas\"),\n    ) +\n  guides(fill = guide_colorbar(direction = \"horizontal\", barheight=.5)) +\n   coord_fixed(ratio = 1) \n\n\n\n\n\n\n\n#ggsave(\"swim_sites2.png\", width = 8, height = 6, dpi = 300, bg = \"white\")"
  }
]