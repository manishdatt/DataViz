[
  {
    "objectID": "posts/xkcd_colors/xkcd_colors.html",
    "href": "posts/xkcd_colors/xkcd_colors.html",
    "title": "The xkcd color survey",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport colorsys\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import pairwise_distances_argmin_min\nimport textwrap\n\n\nanswers = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/answers.csv')\ncolor_ranks = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/color_ranks.csv')\nusers = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/users.csv')\n\n\nanswers\n\n\n\n\n\n\n\n\nuser_id\nhex\nrank\n\n\n\n\n0\n1\n#8240EA\n1\n\n\n1\n2\n#4B31EA\n3\n\n\n2\n2\n#584601\n5\n\n\n3\n2\n#DA239C\n4\n\n\n4\n2\n#B343E5\n1\n\n\n...\n...\n...\n...\n\n\n1058206\n152397\n#7238F0\n1\n\n\n1058207\n152398\n#8E14CD\n1\n\n\n1058208\n152398\n#0A49E7\n3\n\n\n1058209\n152400\n#38A30E\n2\n\n\n1058210\n152401\n#4D004B\n1\n\n\n\n\n1058211 rows × 3 columns\n\n\n\n\nanswers['rank'].unique()\n\narray([1, 3, 5, 4, 2])\n\n\n\ncolor_ranks\n\n\n\n\n\n\n\n\ncolor\nrank\nhex\n\n\n\n\n0\npurple\n1\n#7e1e9c\n\n\n1\ngreen\n2\n#15b01a\n\n\n2\nblue\n3\n#0343df\n\n\n3\npink\n4\n#ff81c0\n\n\n4\nbrown\n5\n#653700\n\n\n...\n...\n...\n...\n\n\n944\nfresh green\n945\n#69d84f\n\n\n945\nelectric lime\n946\n#a8ff04\n\n\n946\ndust\n947\n#b2996e\n\n\n947\ndark pastel green\n948\n#56ae57\n\n\n948\ncloudy blue\n949\n#acc2d9\n\n\n\n\n949 rows × 3 columns\n\n\n\n\ndef hex_to_hsl(hex_color):\n    rgb = colors.to_rgb(hex_color)  # Returns RGB as floats (0-1)\n    h, l, s = colorsys.rgb_to_hls(*rgb)  # Note: HLS order\n#    return round(h * 360, 1), round(s * 100, 1), round(l * 100, 1)\n    return h, s, l\n\nprint(hex_to_hsl(\"#ff5733\"))\n\n(0.02941176470588236, 1.0, 0.6)\n\n\n\n# merge answers and color_rank based on rank column\nmerged_data = pd.merge(answers, color_ranks, on='rank', how='left')\nmerged_data[['h', 's', 'l']] = merged_data['hex_x'].apply(hex_to_hsl).apply(pd.Series)\n\n\nmerged_data\n\n\n\n\n\n\n\n\nuser_id\nhex_x\nrank\ncolor\nhex_y\nh\ns\nl\n\n\n\n\n0\n1\n#8240EA\n1\npurple\n#7e1e9c\n0.731373\n0.801887\n0.584314\n\n\n1\n2\n#4B31EA\n3\nblue\n#0343df\n0.690090\n0.814978\n0.554902\n\n\n2\n2\n#584601\n5\nbrown\n#653700\n0.132184\n0.977528\n0.174510\n\n\n3\n2\n#DA239C\n4\npink\n#ff81c0\n0.889800\n0.723320\n0.496078\n\n\n4\n2\n#B343E5\n1\npurple\n#7e1e9c\n0.781893\n0.757009\n0.580392\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1058206\n152397\n#7238F0\n1\npurple\n#7e1e9c\n0.719203\n0.859813\n0.580392\n\n\n1058207\n152398\n#8E14CD\n1\npurple\n#7e1e9c\n0.776577\n0.822222\n0.441176\n\n\n1058208\n152398\n#0A49E7\n3\nblue\n#0343df\n0.619155\n0.917012\n0.472549\n\n\n1058209\n152400\n#38A30E\n2\ngreen\n#15b01a\n0.286353\n0.841808\n0.347059\n\n\n1058210\n152401\n#4D004B\n1\npurple\n#7e1e9c\n0.837662\n1.000000\n0.150980\n\n\n\n\n1058211 rows × 8 columns\n\n\n\n\nmerged_data.groupby('color').count()\n\n\n\n\n\n\n\n\nuser_id\nhex_x\nrank\nhex_y\nh\ns\nl\n\n\ncolor\n\n\n\n\n\n\n\n\n\n\n\nblue\n288015\n288015\n288015\n288015\n288015\n288015\n288015\n\n\nbrown\n75812\n75812\n75812\n75812\n75812\n75812\n75812\n\n\ngreen\n314172\n314172\n314172\n314172\n314172\n314172\n314172\n\n\npink\n131013\n131013\n131013\n131013\n131013\n131013\n131013\n\n\npurple\n249199\n249199\n249199\n249199\n249199\n249199\n249199\n\n\n\n\n\n\n\n\nf\"{merged_data.groupby('color')['hex_x'].nunique().loc['blue']:,}\"\n\n'275,337'\n\n\n\ncluster_results = {}\nfor color, group_df in merged_data.groupby('color'):\n    kmeans = KMeans(n_clusters=100, random_state=2025)\n    kmeans.fit(group_df[['h', 's', 'l']])\n\n    # Store the result: labels and cluster centers\n    cluster_results[color] = {\n        'labels': kmeans.labels_,\n        'centers': kmeans.cluster_centers_,\n        'data': group_df.copy()\n    }\n#    cluster_results[color]['data']['cluster'] = kmeans.labels_\n\n#print(cluster_results[\"blue\"][\"data\"])\n\nfor color in cluster_results:\n    group_data = cluster_results[color]['data']\n    centers = cluster_results[color]['centers']\n    closest_idxs, _ = pairwise_distances_argmin_min(centers, group_data[['h', 's', 'l']].values)\n    closest_points = group_data.iloc[closest_idxs]\n    cluster_results[color]['closest_points'] = closest_points\n\n\nprint(cluster_results['blue']['closest_points'][['h', 's', 'l']].head(10).values)\n\n[[0.56581741 0.74407583 0.58627451]\n [0.6374269  0.89528796 0.3745098 ]\n [0.50673401 0.52380952 0.62941176]\n [0.59259259 0.54418605 0.42156863]\n [0.66666667 0.88516746 0.59019608]\n [0.66333333 0.20661157 0.4745098 ]\n [0.68726592 0.78070175 0.44705882]\n [0.57042254 0.63963964 0.21764706]\n [0.59777778 0.78947368 0.81372549]\n [0.53159041 0.76884422 0.39019608]]\n\n\n\nfrom scipy.optimize import root_scalar\ndef hsl_to_rgb(h, s, l):\n    # colorsys expects H, L, S in [0,1]\n    r, g, b = colorsys.hls_to_rgb(h, l, s)\n    return r, g, b\n    \n# Arc length function of theta\ndef arc_length(theta, b):\n    return (b/2) * (theta * np.sqrt(1 + theta**2) + np.arcsinh(theta))\n\n# Inverse function: find theta given s (arc length)\ndef theta_for_s(s, b):\n    # Use root finding to solve arc_length(theta) - s = 0\n    sol = root_scalar(lambda t: arc_length(t, b) - s, bracket=[0, 100], method='bisect')\n    return sol.root\n\nd = 4  # distance from center to corner\n# Define corner offsets using Cartesian product of [-d, d]\ncorners = np.array(np.meshgrid([-d, d], [-d, d])).T.reshape(-1, 2)\noffsets = np.vstack([[0, 0],corners])\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\nfor ind, color_name in enumerate(cluster_results):\n    cp = cluster_results[color_name]['closest_points'][['h', 's', 'l']].values\n    \n    rgb_colors = [hsl_to_rgb(*hsl) for hsl in cp]\n    \n    b = 0.1  # spiral parameter\n    num_points = len(cp)\n    desired_sep = 0.3  # desired arc length between points\n        \n    # Compute theta values for uniform arc length steps\n    arc_lengths = np.arange(num_points) * desired_sep\n    theta_vals = np.array([theta_for_s(s, b) for s in arc_lengths])\n    \n    # Compute spiral coords\n    r = b * theta_vals\n    x = r * np.cos(theta_vals)\n    y = r * np.sin(theta_vals)\n    \n    #plt.scatter(x, y, color=rgb_colors, s=200, marker=\"|\", linewidths=4)\n    plt.scatter(x+offsets[ind][0], y+offsets[ind][1], color=rgb_colors, s=100) \n    ax.text(offsets[ind][0], offsets[ind][1]-3.25, f\"{color_name}\\n{ merged_data.groupby('color')['hex_x'].nunique().loc[color_name]:,}\", fontsize=14, ha='center', va='center', color=color_ranks[color_ranks['color'] == color_name]['hex'].values[0], fontfamily=\"Consolas\")\n\nax.axis(\"off\")\nplt.ylim(-7, 7)\nplt.xlim(-7, 7)\nlong_title = \"Top 100 of the specified number of color variants selected via K-means clustering for the top five colors in the xkcd color survey.\"\nwrapped_title = \"\\n\".join(textwrap.wrap(long_title, width=60))\n\nplt.title(wrapped_title, fontsize=16, fontfamily=\"Consolas\", loc='left')\nplt.tight_layout()\n#plt.ylabel(\"Random value\")\nplt.savefig(\"xkcd_colors.png\", dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "posts/xkcd_colors/xkcd_colors.html#tidytuesday-data-for-2025-07-08",
    "href": "posts/xkcd_colors/xkcd_colors.html#tidytuesday-data-for-2025-07-08",
    "title": "The xkcd color survey",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport colorsys\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import pairwise_distances_argmin_min\nimport textwrap\n\n\nanswers = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/answers.csv')\ncolor_ranks = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/color_ranks.csv')\nusers = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/users.csv')\n\n\nanswers\n\n\n\n\n\n\n\n\nuser_id\nhex\nrank\n\n\n\n\n0\n1\n#8240EA\n1\n\n\n1\n2\n#4B31EA\n3\n\n\n2\n2\n#584601\n5\n\n\n3\n2\n#DA239C\n4\n\n\n4\n2\n#B343E5\n1\n\n\n...\n...\n...\n...\n\n\n1058206\n152397\n#7238F0\n1\n\n\n1058207\n152398\n#8E14CD\n1\n\n\n1058208\n152398\n#0A49E7\n3\n\n\n1058209\n152400\n#38A30E\n2\n\n\n1058210\n152401\n#4D004B\n1\n\n\n\n\n1058211 rows × 3 columns\n\n\n\n\nanswers['rank'].unique()\n\narray([1, 3, 5, 4, 2])\n\n\n\ncolor_ranks\n\n\n\n\n\n\n\n\ncolor\nrank\nhex\n\n\n\n\n0\npurple\n1\n#7e1e9c\n\n\n1\ngreen\n2\n#15b01a\n\n\n2\nblue\n3\n#0343df\n\n\n3\npink\n4\n#ff81c0\n\n\n4\nbrown\n5\n#653700\n\n\n...\n...\n...\n...\n\n\n944\nfresh green\n945\n#69d84f\n\n\n945\nelectric lime\n946\n#a8ff04\n\n\n946\ndust\n947\n#b2996e\n\n\n947\ndark pastel green\n948\n#56ae57\n\n\n948\ncloudy blue\n949\n#acc2d9\n\n\n\n\n949 rows × 3 columns\n\n\n\n\ndef hex_to_hsl(hex_color):\n    rgb = colors.to_rgb(hex_color)  # Returns RGB as floats (0-1)\n    h, l, s = colorsys.rgb_to_hls(*rgb)  # Note: HLS order\n#    return round(h * 360, 1), round(s * 100, 1), round(l * 100, 1)\n    return h, s, l\n\nprint(hex_to_hsl(\"#ff5733\"))\n\n(0.02941176470588236, 1.0, 0.6)\n\n\n\n# merge answers and color_rank based on rank column\nmerged_data = pd.merge(answers, color_ranks, on='rank', how='left')\nmerged_data[['h', 's', 'l']] = merged_data['hex_x'].apply(hex_to_hsl).apply(pd.Series)\n\n\nmerged_data\n\n\n\n\n\n\n\n\nuser_id\nhex_x\nrank\ncolor\nhex_y\nh\ns\nl\n\n\n\n\n0\n1\n#8240EA\n1\npurple\n#7e1e9c\n0.731373\n0.801887\n0.584314\n\n\n1\n2\n#4B31EA\n3\nblue\n#0343df\n0.690090\n0.814978\n0.554902\n\n\n2\n2\n#584601\n5\nbrown\n#653700\n0.132184\n0.977528\n0.174510\n\n\n3\n2\n#DA239C\n4\npink\n#ff81c0\n0.889800\n0.723320\n0.496078\n\n\n4\n2\n#B343E5\n1\npurple\n#7e1e9c\n0.781893\n0.757009\n0.580392\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1058206\n152397\n#7238F0\n1\npurple\n#7e1e9c\n0.719203\n0.859813\n0.580392\n\n\n1058207\n152398\n#8E14CD\n1\npurple\n#7e1e9c\n0.776577\n0.822222\n0.441176\n\n\n1058208\n152398\n#0A49E7\n3\nblue\n#0343df\n0.619155\n0.917012\n0.472549\n\n\n1058209\n152400\n#38A30E\n2\ngreen\n#15b01a\n0.286353\n0.841808\n0.347059\n\n\n1058210\n152401\n#4D004B\n1\npurple\n#7e1e9c\n0.837662\n1.000000\n0.150980\n\n\n\n\n1058211 rows × 8 columns\n\n\n\n\nmerged_data.groupby('color').count()\n\n\n\n\n\n\n\n\nuser_id\nhex_x\nrank\nhex_y\nh\ns\nl\n\n\ncolor\n\n\n\n\n\n\n\n\n\n\n\nblue\n288015\n288015\n288015\n288015\n288015\n288015\n288015\n\n\nbrown\n75812\n75812\n75812\n75812\n75812\n75812\n75812\n\n\ngreen\n314172\n314172\n314172\n314172\n314172\n314172\n314172\n\n\npink\n131013\n131013\n131013\n131013\n131013\n131013\n131013\n\n\npurple\n249199\n249199\n249199\n249199\n249199\n249199\n249199\n\n\n\n\n\n\n\n\nf\"{merged_data.groupby('color')['hex_x'].nunique().loc['blue']:,}\"\n\n'275,337'\n\n\n\ncluster_results = {}\nfor color, group_df in merged_data.groupby('color'):\n    kmeans = KMeans(n_clusters=100, random_state=2025)\n    kmeans.fit(group_df[['h', 's', 'l']])\n\n    # Store the result: labels and cluster centers\n    cluster_results[color] = {\n        'labels': kmeans.labels_,\n        'centers': kmeans.cluster_centers_,\n        'data': group_df.copy()\n    }\n#    cluster_results[color]['data']['cluster'] = kmeans.labels_\n\n#print(cluster_results[\"blue\"][\"data\"])\n\nfor color in cluster_results:\n    group_data = cluster_results[color]['data']\n    centers = cluster_results[color]['centers']\n    closest_idxs, _ = pairwise_distances_argmin_min(centers, group_data[['h', 's', 'l']].values)\n    closest_points = group_data.iloc[closest_idxs]\n    cluster_results[color]['closest_points'] = closest_points\n\n\nprint(cluster_results['blue']['closest_points'][['h', 's', 'l']].head(10).values)\n\n[[0.56581741 0.74407583 0.58627451]\n [0.6374269  0.89528796 0.3745098 ]\n [0.50673401 0.52380952 0.62941176]\n [0.59259259 0.54418605 0.42156863]\n [0.66666667 0.88516746 0.59019608]\n [0.66333333 0.20661157 0.4745098 ]\n [0.68726592 0.78070175 0.44705882]\n [0.57042254 0.63963964 0.21764706]\n [0.59777778 0.78947368 0.81372549]\n [0.53159041 0.76884422 0.39019608]]\n\n\n\nfrom scipy.optimize import root_scalar\ndef hsl_to_rgb(h, s, l):\n    # colorsys expects H, L, S in [0,1]\n    r, g, b = colorsys.hls_to_rgb(h, l, s)\n    return r, g, b\n    \n# Arc length function of theta\ndef arc_length(theta, b):\n    return (b/2) * (theta * np.sqrt(1 + theta**2) + np.arcsinh(theta))\n\n# Inverse function: find theta given s (arc length)\ndef theta_for_s(s, b):\n    # Use root finding to solve arc_length(theta) - s = 0\n    sol = root_scalar(lambda t: arc_length(t, b) - s, bracket=[0, 100], method='bisect')\n    return sol.root\n\nd = 4  # distance from center to corner\n# Define corner offsets using Cartesian product of [-d, d]\ncorners = np.array(np.meshgrid([-d, d], [-d, d])).T.reshape(-1, 2)\noffsets = np.vstack([[0, 0],corners])\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\nfor ind, color_name in enumerate(cluster_results):\n    cp = cluster_results[color_name]['closest_points'][['h', 's', 'l']].values\n    \n    rgb_colors = [hsl_to_rgb(*hsl) for hsl in cp]\n    \n    b = 0.1  # spiral parameter\n    num_points = len(cp)\n    desired_sep = 0.3  # desired arc length between points\n        \n    # Compute theta values for uniform arc length steps\n    arc_lengths = np.arange(num_points) * desired_sep\n    theta_vals = np.array([theta_for_s(s, b) for s in arc_lengths])\n    \n    # Compute spiral coords\n    r = b * theta_vals\n    x = r * np.cos(theta_vals)\n    y = r * np.sin(theta_vals)\n    \n    #plt.scatter(x, y, color=rgb_colors, s=200, marker=\"|\", linewidths=4)\n    plt.scatter(x+offsets[ind][0], y+offsets[ind][1], color=rgb_colors, s=100) \n    ax.text(offsets[ind][0], offsets[ind][1]-3.25, f\"{color_name}\\n{ merged_data.groupby('color')['hex_x'].nunique().loc[color_name]:,}\", fontsize=14, ha='center', va='center', color=color_ranks[color_ranks['color'] == color_name]['hex'].values[0], fontfamily=\"Consolas\")\n\nax.axis(\"off\")\nplt.ylim(-7, 7)\nplt.xlim(-7, 7)\nlong_title = \"Top 100 of the specified number of color variants selected via K-means clustering for the top five colors in the xkcd color survey.\"\nwrapped_title = \"\\n\".join(textwrap.wrap(long_title, width=60))\n\nplt.title(wrapped_title, fontsize=16, fontfamily=\"Consolas\", loc='left')\nplt.tight_layout()\n#plt.ylabel(\"Random value\")\nplt.savefig(\"xkcd_colors.png\", dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "posts/useR2025/useR2025.html",
    "href": "posts/useR2025/useR2025.html",
    "title": "useR2025 conference data analysis",
    "section": "",
    "text": "library(tidyverse)\nlibrary(uk2us)\nuser2025 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-29/user2025.csv')\nuser2025\n\n# A tibble: 128 × 11\n      id session date       time  room   title  content video_recording keywords\n   &lt;dbl&gt; &lt;chr&gt;   &lt;date&gt;     &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;           &lt;chr&gt;   \n 1   170 Virtual 2025-08-01 TBD   Online A Rob… In R p… ✅              statist…\n 2    79 Virtual 2025-08-01 TBD   Online A fir… Positr… ✅              ide, wo…\n 3    30 Virtual 2025-08-01 TBD   Online Analy… This t… ✅              demogra…\n 4    31 Virtual 2025-08-01 TBD   Online Autom… Webhoo… ✅              automat…\n 5    39 Virtual 2025-08-01 TBD   Online Beyon… In a w… ✅              marketi…\n 6   169 Virtual 2025-08-01 TBD   Online CSV t… CSV is… ✅              data pr…\n 7    94 Virtual 2025-08-01 TBD   Online Data … Explor… ✅              factor …\n 8   163 Virtual 2025-08-01 TBD   Online Don’t… The fa… ✅              testing…\n 9    13 Virtual 2025-08-01 TBD   Online Exper… Large … ✅              automat…\n10    51 Virtual 2025-08-01 TBD   Online From … Data S… ✅              quarto,…\n# ℹ 118 more rows\n# ℹ 2 more variables: speakers &lt;chr&gt;, co_authors &lt;chr&gt;\nnormalize_keywords_uk2us &lt;- function(phrases) {\n  phrases %&gt;%\n    str_split(\"\\\\s+\") %&gt;%                                \n    map(~ uk2us::convert_uk2us(.x)) %&gt;%                  \n    map_chr(str_c, collapse = \" \")                       \n}"
  },
  {
    "objectID": "posts/useR2025/useR2025.html#plotting",
    "href": "posts/useR2025/useR2025.html#plotting",
    "title": "useR2025 conference data analysis",
    "section": "Plotting",
    "text": "Plotting\n\nuser2025 %&gt;%\n  separate_rows(keywords, sep = \",\") %&gt;%\n  mutate(keywords = str_trim(keywords)) %&gt;%\n  mutate(keywords = normalize_keywords_uk2us(keywords)) %&gt;% \n  group_by(keywords) %&gt;% \n  summarise(count = n()) %&gt;% \n  arrange(desc(count)) %&gt;% \n  slice(1:20) %&gt;% \n  ggplot(aes(x = reorder(keywords, count), y = count, fill=count)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = reorder(keywords, count), y=count/2), hjust = 0, \n            size = 3.5, color=rep(c(\"white\", \"#555\"), times=c(8,12))) + \n  coord_flip() +\n  scale_fill_viridis_c(option=\"magma\", direction = -1)+\n  labs(title = \"Top 20 Keywords in useR! 2025\") +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.title.y = element_blank(),\n    legend.position = \"none\",\n    panel.grid.major.y = element_blank()\n  )\n\n\n\n\n\n\n\nggsave(\"user2025_keywords.png\", width = 8, height = 6, dpi = 300)"
  },
  {
    "objectID": "posts/Seismic_Vesuvius/Mt_vesuvius.html",
    "href": "posts/Seismic_Vesuvius/Mt_vesuvius.html",
    "title": "Seismic events at Mount Vesuvius",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nvesuvius = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-13/vesuvius.csv')\n\n\nvesuvius\n\n\n\n\n\n\n\n\nevent_id\ntime\nlatitude\nlongitude\ndepth_km\nduration_magnitude_md\nmd_error\narea\ntype\nreview_level\nyear\n\n\n\n\n0\n4251\n2011-04-20T00:27:24Z\n40.818000\n14.430000\n0.42\n1.2\n0.3\nMount Vesuvius\nearthquake\nrevised\n2011\n\n\n1\n4252\n2012-06-19T21:29:48Z\n40.808833\n14.427167\n1.31\n0.7\n0.3\nMount Vesuvius\nearthquake\nrevised\n2012\n\n\n2\n22547\n2013-01-01T07:34:46Z\n40.822170\n14.428000\n0.06\n2.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n\n\n3\n22546\n2013-01-03T16:06:48Z\nNaN\nNaN\nNaN\n0.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n\n\n4\n22545\n2013-01-03T16:07:37Z\nNaN\nNaN\nNaN\n0.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n12022\n40738\n2024-12-29T23:56:51Z\n40.823000\n14.428333\n0.34\n-0.1\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n12023\n40741\n2024-12-30T07:52:43Z\n40.823333\n14.423500\n0.56\n-0.1\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n12024\n40743\n2024-12-30T12:52:24Z\nNaN\nNaN\nNaN\n-0.1\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n12025\n40744\n2024-12-30T15:11:28Z\n40.819000\n14.424500\n0.55\n-0.4\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n12026\n40802\n2024-12-31T17:02:32Z\n40.822000\n14.409833\n0.41\n0.0\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n\n\n12027 rows × 11 columns\n\n\n\n\nsns.histplot(data=vesuvius,x=\"depth_km\")\n\n\n\n\n\n\n\n\n\nvesuvius[\"time\"] = pd.to_datetime(vesuvius[\"time\"])\n\n\nvesuvius[\"hour\"] = vesuvius[\"time\"].dt.hour\ntime_bins = [0, 5, 12, 17, 21, 24]  # 0-4 -&gt; Night, 5-11 -&gt; Morning, etc.\ntime_labels = [\"Night\", \"Morning\", \"Afternoon\", \"Evening\", \"Night\"]\n\n\nvesuvius[\"time_of_day\"] = pd.cut(vesuvius['hour'], bins=time_bins, labels=time_labels, right=False, ordered=False)\n\n\nvesuvius[\"time_of_day\"]\n\n0            Night\n1            Night\n2          Morning\n3        Afternoon\n4        Afternoon\n           ...    \n12022        Night\n12023      Morning\n12024    Afternoon\n12025    Afternoon\n12026      Evening\nName: time_of_day, Length: 12027, dtype: category\nCategories (4, object): ['Afternoon', 'Evening', 'Morning', 'Night']\n\n\n\nvesuvius[\"year\"].unique()\n\narray([2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021,\n       2022, 2023, 2024], dtype=int64)\n\n\n\nbin_edges = [-float('inf'), 1, 2, 3, 5, 7, float('inf')]\nvesuvius[\"depth_bins\"] = pd.cut(vesuvius['depth_km'], bins=bin_edges)\n\n\nvesuvius[\"depth_bins\"].value_counts()\n\ndepth_bins\n(-inf, 1.0]    7777\n(1.0, 2.0]      648\n(2.0, 3.0]      142\n(3.0, 5.0]       25\n(5.0, 7.0]        1\n(7.0, inf]        1\nName: count, dtype: int64\n\n\n\nvesuvius.head()\n\n\n\n\n\n\n\n\nevent_id\ntime\nlatitude\nlongitude\ndepth_km\nduration_magnitude_md\nmd_error\narea\ntype\nreview_level\nyear\nhour\ntime_of_day\ndepth_bins\n\n\n\n\n0\n4251\n2011-04-20 00:27:24+00:00\n40.818000\n14.430000\n0.42\n1.2\n0.3\nMount Vesuvius\nearthquake\nrevised\n2011\n0\nNight\n(-inf, 1.0]\n\n\n1\n4252\n2012-06-19 21:29:48+00:00\n40.808833\n14.427167\n1.31\n0.7\n0.3\nMount Vesuvius\nearthquake\nrevised\n2012\n21\nNight\n(1.0, 2.0]\n\n\n2\n22547\n2013-01-01 07:34:46+00:00\n40.822170\n14.428000\n0.06\n2.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n7\nMorning\n(-inf, 1.0]\n\n\n3\n22546\n2013-01-03 16:06:48+00:00\nNaN\nNaN\nNaN\n0.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n16\nAfternoon\nNaN\n\n\n4\n22545\n2013-01-03 16:07:37+00:00\nNaN\nNaN\nNaN\n0.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n16\nAfternoon\nNaN"
  },
  {
    "objectID": "posts/Seismic_Vesuvius/Mt_vesuvius.html#tidytuesday-data-for-2025-05-13",
    "href": "posts/Seismic_Vesuvius/Mt_vesuvius.html#tidytuesday-data-for-2025-05-13",
    "title": "Seismic events at Mount Vesuvius",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nvesuvius = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-13/vesuvius.csv')\n\n\nvesuvius\n\n\n\n\n\n\n\n\nevent_id\ntime\nlatitude\nlongitude\ndepth_km\nduration_magnitude_md\nmd_error\narea\ntype\nreview_level\nyear\n\n\n\n\n0\n4251\n2011-04-20T00:27:24Z\n40.818000\n14.430000\n0.42\n1.2\n0.3\nMount Vesuvius\nearthquake\nrevised\n2011\n\n\n1\n4252\n2012-06-19T21:29:48Z\n40.808833\n14.427167\n1.31\n0.7\n0.3\nMount Vesuvius\nearthquake\nrevised\n2012\n\n\n2\n22547\n2013-01-01T07:34:46Z\n40.822170\n14.428000\n0.06\n2.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n\n\n3\n22546\n2013-01-03T16:06:48Z\nNaN\nNaN\nNaN\n0.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n\n\n4\n22545\n2013-01-03T16:07:37Z\nNaN\nNaN\nNaN\n0.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n12022\n40738\n2024-12-29T23:56:51Z\n40.823000\n14.428333\n0.34\n-0.1\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n12023\n40741\n2024-12-30T07:52:43Z\n40.823333\n14.423500\n0.56\n-0.1\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n12024\n40743\n2024-12-30T12:52:24Z\nNaN\nNaN\nNaN\n-0.1\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n12025\n40744\n2024-12-30T15:11:28Z\n40.819000\n14.424500\n0.55\n-0.4\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n12026\n40802\n2024-12-31T17:02:32Z\n40.822000\n14.409833\n0.41\n0.0\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2024\n\n\n\n\n12027 rows × 11 columns\n\n\n\n\nsns.histplot(data=vesuvius,x=\"depth_km\")\n\n\n\n\n\n\n\n\n\nvesuvius[\"time\"] = pd.to_datetime(vesuvius[\"time\"])\n\n\nvesuvius[\"hour\"] = vesuvius[\"time\"].dt.hour\ntime_bins = [0, 5, 12, 17, 21, 24]  # 0-4 -&gt; Night, 5-11 -&gt; Morning, etc.\ntime_labels = [\"Night\", \"Morning\", \"Afternoon\", \"Evening\", \"Night\"]\n\n\nvesuvius[\"time_of_day\"] = pd.cut(vesuvius['hour'], bins=time_bins, labels=time_labels, right=False, ordered=False)\n\n\nvesuvius[\"time_of_day\"]\n\n0            Night\n1            Night\n2          Morning\n3        Afternoon\n4        Afternoon\n           ...    \n12022        Night\n12023      Morning\n12024    Afternoon\n12025    Afternoon\n12026      Evening\nName: time_of_day, Length: 12027, dtype: category\nCategories (4, object): ['Afternoon', 'Evening', 'Morning', 'Night']\n\n\n\nvesuvius[\"year\"].unique()\n\narray([2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021,\n       2022, 2023, 2024], dtype=int64)\n\n\n\nbin_edges = [-float('inf'), 1, 2, 3, 5, 7, float('inf')]\nvesuvius[\"depth_bins\"] = pd.cut(vesuvius['depth_km'], bins=bin_edges)\n\n\nvesuvius[\"depth_bins\"].value_counts()\n\ndepth_bins\n(-inf, 1.0]    7777\n(1.0, 2.0]      648\n(2.0, 3.0]      142\n(3.0, 5.0]       25\n(5.0, 7.0]        1\n(7.0, inf]        1\nName: count, dtype: int64\n\n\n\nvesuvius.head()\n\n\n\n\n\n\n\n\nevent_id\ntime\nlatitude\nlongitude\ndepth_km\nduration_magnitude_md\nmd_error\narea\ntype\nreview_level\nyear\nhour\ntime_of_day\ndepth_bins\n\n\n\n\n0\n4251\n2011-04-20 00:27:24+00:00\n40.818000\n14.430000\n0.42\n1.2\n0.3\nMount Vesuvius\nearthquake\nrevised\n2011\n0\nNight\n(-inf, 1.0]\n\n\n1\n4252\n2012-06-19 21:29:48+00:00\n40.808833\n14.427167\n1.31\n0.7\n0.3\nMount Vesuvius\nearthquake\nrevised\n2012\n21\nNight\n(1.0, 2.0]\n\n\n2\n22547\n2013-01-01 07:34:46+00:00\n40.822170\n14.428000\n0.06\n2.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n7\nMorning\n(-inf, 1.0]\n\n\n3\n22546\n2013-01-03 16:06:48+00:00\nNaN\nNaN\nNaN\n0.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n16\nAfternoon\nNaN\n\n\n4\n22545\n2013-01-03 16:07:37+00:00\nNaN\nNaN\nNaN\n0.2\n0.3\nMount Vesuvius\nearthquake\npreliminary\n2013\n16\nAfternoon\nNaN"
  },
  {
    "objectID": "posts/Seismic_Vesuvius/Mt_vesuvius.html#annual-number-of-tremors-vs-duration",
    "href": "posts/Seismic_Vesuvius/Mt_vesuvius.html#annual-number-of-tremors-vs-duration",
    "title": "Seismic events at Mount Vesuvius",
    "section": "Annual number of tremors vs duration",
    "text": "Annual number of tremors vs duration\n\nfig, ax = plt.subplots()\nhue_order = [\"Morning\", \"Afternoon\", \"Evening\", \"Night\"]\ncolor_map = [\"lightblue\", \"dodgerblue\", \"lightgrey\", \"grey\"]\ncolors = {x: color_map[ind] for ind, x in enumerate(hue_order)}\n     \nsns.stripplot(data=vesuvius, x=\"year\", y=\"duration_magnitude_md\", \\\n              hue='time_of_day', hue_order=hue_order, palette=\"plasma\")\nplt.xticks(rotation=45)\nplt.xlabel(\"\")\n\nplt.legend(\n    title='',\n    loc='upper center',     \n    bbox_to_anchor=(0.5, 1.1), \n    ncol=4,\n    frameon=False\n)\nsns.despine()"
  },
  {
    "objectID": "posts/Seismic_Vesuvius/Mt_vesuvius.html#earthquakes-at-different-times-of-the-day",
    "href": "posts/Seismic_Vesuvius/Mt_vesuvius.html#earthquakes-at-different-times-of-the-day",
    "title": "Seismic events at Mount Vesuvius",
    "section": "Earthquakes at different times of the day",
    "text": "Earthquakes at different times of the day\n\nvesuvius[\"hour12\"] = vesuvius[\"time\"].dt.strftime(\"%I\").astype(int)    \nvesuvius[\"AMPM\"] = vesuvius[\"time\"].dt.strftime(\"%p\")              \n\n\nvesuvius_grp = vesuvius.groupby([\"hour12\", \"AMPM\"]).size().unstack(fill_value=0)\nvesuvius_grp\n\n\n\n\n\n\n\nAMPM\nAM\nPM\n\n\nhour12\n\n\n\n\n\n\n1\n691\n339\n\n\n2\n694\n384\n\n\n3\n618\n335\n\n\n4\n591\n440\n\n\n5\n510\n732\n\n\n6\n472\n555\n\n\n7\n352\n610\n\n\n8\n287\n592\n\n\n9\n284\n624\n\n\n10\n380\n598\n\n\n11\n334\n627\n\n\n12\n646\n332\n\n\n\n\n\n\n\n\nvesuvius_grp.plot(kind=\"bar\", stacked=True, color=['dodgerblue', 'salmon'])"
  },
  {
    "objectID": "posts/Seismic_Vesuvius/Mt_vesuvius.html#polar-coordinates",
    "href": "posts/Seismic_Vesuvius/Mt_vesuvius.html#polar-coordinates",
    "title": "Seismic events at Mount Vesuvius",
    "section": "Polar coordinates",
    "text": "Polar coordinates\n\nimport plotly.graph_objects as go\nimport numpy as np\nimport roman\n\n\nvesuvius_grp['angle'] = vesuvius_grp.index * 30\n\n# Create polar plot with two bar traces: AM and PM\nfig = go.Figure()\n\n# AM bars\nfig.add_trace(go.Barpolar(\n    r=vesuvius_grp['AM'],\n    theta=vesuvius_grp['angle'],\n    name='AM',\n    marker_color='dodgerblue',\n    hovertemplate='count = %{r}&lt;br&gt;time = %{theta} AM&lt;extra&gt;&lt;/extra&gt;'\n))\n\n# PM bars\nfig.add_trace(go.Barpolar(\n    r=vesuvius_grp['PM'],\n    theta=vesuvius_grp['angle'],\n    name='PM',\n    marker_color='salmon',\n    hovertemplate='count = %{r}&lt;br&gt;time = %{theta} PM&lt;extra&gt;&lt;/extra&gt;'\n))\n\n# Layout\nfig.update_layout(\n    title={\n        'text': f'Distribution of &lt;b&gt;{vesuvius_grp[\"AM\"].sum() + vesuvius_grp[\"PM\"].sum():,.0f}&lt;/b&gt; earthquakes during &lt;br&gt;&lt;span style=\"color:dodgerblue;\"&gt;AM&lt;/span&gt; and &lt;span style=\"color:red;\"&gt;PM&lt;/span&gt; at Mount Vesuvius (2011-24)',\n        'font': {\n            'size': 16  \n        },\n        \"x\" : 0.5,\n    },\n    polar=dict(\n        angularaxis=dict(direction='clockwise', rotation=90, tickmode='array',\n                         tickvals=np.arange(30, 361, 30),\n                         ticktext=[f\"&lt;b&gt;{roman.toRoman(h)}&lt;/b&gt;\" for h in range(1, 13)], \n                         showline=False, showgrid=False),\n        radialaxis=dict(showticklabels=False, ticks='', showline=False, showgrid=False)\n    ),\n    showlegend=False,\n    template='plotly_white',\n    width=500,\n    height=500,\n    margin=dict(l=0, r=0, t=100, b=20)\n)\n#fig.write_image(\"Vesuvius.png\")\nfig.show()\n\n                                                \n\n\nThere is no direct way to add labels to bars in radial axis. As a workaround, add scatterplot and show only text.\n\nvesuvius_grp['angle'] = vesuvius_grp.index * 30\n\n# Create polar plot with two bar traces: AM and PM\nfig = go.Figure()\n\n# AM bars\nfig.add_trace(go.Barpolar(\n    r=vesuvius_grp['AM'],\n    theta=vesuvius_grp['angle'],\n    name='AM',\n    marker_color='dodgerblue',\n    hovertemplate='count = %{r}&lt;br&gt;time = %{theta} AM&lt;extra&gt;&lt;/extra&gt;'\n))\n\n# PM bars\nfig.add_trace(go.Barpolar(\n    r=vesuvius_grp['PM'],\n    theta=vesuvius_grp['angle'],\n    name='PM',\n    marker_color='salmon',\n    hovertemplate='count = %{r}&lt;br&gt;time = %{theta} PM&lt;extra&gt;&lt;/extra&gt;'\n))\n\n# Layout\nfig.update_layout(\n    title={\n        'text': f'Distribution of &lt;b&gt;{vesuvius_grp[\"AM\"].sum() + vesuvius_grp[\"PM\"].sum():,.0f}&lt;/b&gt; earthquakes during &lt;br&gt;&lt;span style=\"color:dodgerblue;\"&gt;AM&lt;/span&gt; and &lt;span style=\"color:red;\"&gt;PM&lt;/span&gt; at Mount Vesuvius (2011-24)',\n        'font': {\n            'size': 16  \n        },\n        \"x\" : 0.5,\n    },\n    polar=dict(\n        angularaxis=dict(direction='clockwise', rotation=90, tickmode='array',\n                         tickvals=np.arange(30, 361, 30),\n                         ticktext=[f\"&lt;b&gt;{roman.toRoman(h)}&lt;/b&gt;\" for h in range(1, 13)], \n                         showline=False, showgrid=False),\n        radialaxis=dict(showticklabels=False, ticks='', showline=False, showgrid=False)\n    ),\n    showlegend=False,\n    template='plotly_white',\n    width=500,\n    height=500,\n    margin=dict(l=0, r=0, t=100, b=20)\n)\nfig.add_trace(go.Scatterpolar(\n    r=vesuvius_grp['AM']+50,  \n    theta=vesuvius_grp['angle'],  \n    mode=\"text\",  \n    text=vesuvius_grp['AM'],  \n    textposition=\"middle center\",  \n    textfont=dict(size=14, color=\"white\"),\n    name=\"AM Labels\",\n    hoverinfo=\"skip\",  \n    showlegend=False\n))\n\nfig.add_trace(go.Scatterpolar(\n    r=vesuvius_grp['AM']+vesuvius_grp['PM']+50,  \n    theta=vesuvius_grp['angle'],  \n    mode=\"text\",\n    text=vesuvius_grp['PM'],\n    textposition=\"middle center\",\n    name=\"PM Labels\",\n    hoverinfo=\"skip\",\n    showlegend=False\n))\n#fig.write_image(\"Vesuvius_labels.png\")\nfig.show()"
  },
  {
    "objectID": "posts/NSF_grants/NSF_grants_cut.html",
    "href": "posts/NSF_grants/NSF_grants_cut.html",
    "title": "NSF funding cut in April 2025",
    "section": "",
    "text": "import pandas as pd\nimport plotly.express as px\nnsf_terminations = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-06/nsf_terminations.csv')\nnsf_terminations\n\n\n\n\n\n\n\n\ngrant_number\nproject_title\ntermination_letter_date\norg_name\norg_city\norg_state\norg_district\nusaspending_obligated\naward_type\ndirectorate_abbrev\n...\ndivision\nnsf_program_name\nnsf_url\nusaspending_url\nnsf_startdate\nnsf_expected_end_date\norg_zip\norg_uei\nabstract\nin_cruz_list\n\n\n\n\n0\n2135329\nCollaborative Research: Research: Early-Career...\n2025-04-25\nUniversity of New Mexico\nALBUQUERQUE\nNM\nNM01\n190725.0\nStandard Grant\nENG\n...\nEngineering Education and Centers\nER2-Ethical & Responsible Res\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_213...\n2022-09-01\n2025-04-18\n871310001\nF6XLTRUQJEN4\nTransportation systems, computing algorithms, ...\nTrue\n\n\n1\n2342099\nMyTurn: An Afterschool Social Robotics Program...\n2025-04-25\nUniversity of Illinois at Chicago\nCHICAGO\nIL\nIL07\n499999.0\nStandard Grant\nEDU\n...\nResearch on Learning in Formal and Informal Se...\nITEST-Inov Tech Exp Stu & Teac\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_234...\n2024-08-15\n2027-07-31\n606124305\nW8XEAJDKMXH3\nComputational thinking and robotics are increa...\nFalse\n\n\n2\n2201103\nCollaborative Research: The Organizational Cli...\n2025-04-25\nAmerican Society For Engineering Education\nWASHINGTON\nDC\nDC00\n124241.0\nContinuing Grant\nEDU\n...\nEquity for Excellence in STEM\nECR-EDU Core Research\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_220...\n2022-08-01\n2026-07-31\n200362476\nF6G9C4HMNHW4\nThe ongoing lack of diversity in the engineeri...\nTrue\n\n\n3\n2215382\nEngaging Rural, Latinx Youth in an After Schoo...\n2025-04-25\nTERC Inc\nCAMBRIDGE\nMA\nMA05\n2601763.0\nContinuing Grant\nEDU\n...\nResearch on Learning in Formal and Informal Se...\nAISL\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_221...\n2022-08-01\n2026-07-31\n21401339\nGSLCJ3M62XX1\nThe project will develop and research an after...\nTrue\n\n\n4\n2405633\nDesign Effective and Equitable Professional Le...\n2025-04-25\nSan Francisco State University\nSAN FRANCISCO\nCA\nCA11\n565771.0\nContinuing Grant\nEDU\n...\nResearch on Learning in Formal and Informal Se...\nDiscovery Research K-12\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_240...\n2024-10-01\n2028-09-30\n941321740\nF4SLJ5WF59F6\nProviding computer science (CS) education to s...\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1036\n2411129\nCommunity-Situated Data Practices in Multiethn...\n2025-04-18\nMichigan State University\nEAST LANSING\nMI\nMI07\n1736866.0\nContinuing Grant\nEDU\n...\nResearch on Learning in Formal and Informal Se...\nDiscovery Research K-12\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_241...\n2024-09-15\n2029-08-31\n488242600\nR28EKN92ZTZ9\nBroadening STEM research and education to incl...\nFalse\n\n\n1037\n2224674\nCollaborative Research: Engaging Marginalized ...\n2025-04-18\nGeorge Mason University\nFAIRFAX\nVA\nVA11\n439380.0\nContinuing Grant\nEDU\n...\nResearch on Learning in Formal and Informal Se...\nECR-EDU Core Research\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_222...\n2023-06-15\n2028-05-31\n220304422\nEADLFP7Z72E5\nThis collaborative project investigates the la...\nTrue\n\n\n1038\n2315024\nCollaborative Research: Overcoming Isolation a...\n2025-04-18\nOhio State University\nCOLUMBUS\nOH\nOH03\n378441.0\nStandard Grant\nEDU\n...\nGraduate Education\nADVANCE\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_231...\n2023-09-15\n2025-04-18\n432101016\nDLWBSLWAJWR1\nThere is a growing need for scholars specializ...\nTrue\n\n\n1039\n2216826\nCommunity of Neighboring and National Entrepre...\n2025-04-18\nSt. Catherine University\nSAINT PAUL\nMN\nMN04\n75000.0\nStandard Grant\nBIO\n...\nBiological Infrastructure\nUBE - Undergraduate Biology Ed\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_221...\n2022-09-01\n2025-08-31\n551051750\nRQJ5KM1LQ935\nThis project aims to serve the national intere...\nTrue\n\n\n1040\n2210842\nCollaborative Research: HCC: Designing Technol...\n2025-04-18\nUniversity of California-Irvine\nIRVINE\nCA\nCA47\n90626.0\nStandard Grant\nCISE\n...\nInformation and Intelligent Systems\nHCC-Human-Centered Computing\nhttps://www.nsf.gov/awardsearch/showAward?AWD_...\nhttps://www.usaspending.gov/award/ASST_NON_221...\n2022-10-01\n2025-04-18\n926970001\nMJC5FCYQTPE6\nThis award supports research that examines the...\nTrue\n\n\n\n\n1041 rows × 21 columns\nnsf_terminations.columns\n\nIndex(['grant_number', 'project_title', 'termination_letter_date', 'org_name',\n       'org_city', 'org_state', 'org_district', 'usaspending_obligated',\n       'award_type', 'directorate_abbrev', 'directorate', 'division',\n       'nsf_program_name', 'nsf_url', 'usaspending_url', 'nsf_startdate',\n       'nsf_expected_end_date', 'org_zip', 'org_uei', 'abstract',\n       'in_cruz_list'],\n      dtype='object')"
  },
  {
    "objectID": "posts/NSF_grants/NSF_grants_cut.html#sunburst-plot-using-plotly",
    "href": "posts/NSF_grants/NSF_grants_cut.html#sunburst-plot-using-plotly",
    "title": "NSF funding cut in April 2025",
    "section": "Sunburst plot using plotly",
    "text": "Sunburst plot using plotly\n\nnsf_grp = nsf_terminations.groupby(['directorate', 'division'], as_index=False)['usaspending_obligated'].sum()\n\ncustom_colorscale = [\n    [0.0, 'black'],       # 0 → black\n    [0.000001, 'rgb(68,1,84)'],   # Start of Viridis after 0\n    [0.25, 'rgb(59,82,139)'],\n    [0.5, 'rgb(33,145,140)'],\n    [0.75, 'rgb(94,201,98)'],\n    [1.0, 'rgb(253,231,37)']\n]\n\nfig = px.sunburst(nsf_grp, path=['directorate', 'division'], values='usaspending_obligated', \\\n                  color='usaspending_obligated', color_continuous_scale=custom_colorscale, width=900, height=700, \\\n                  title=f\"Distribution of the NSF funding cut of &lt;b&gt;${nsf_grp[\"usaspending_obligated\"].sum():,.0f}&lt;/b&gt; in April 2025.\"\n                 )\nfig.update_traces(textinfo='label+percent entry')\nfor i in range(len(fig.data[0].marker.colors) - len(nsf_grp['directorate'].unique()), len(fig.data[0].marker.colors)):\n    fig.data[0].marker.colors[i] = 0\n#fig.write_image(\"sunburst_plot.png\")\nfig.show()"
  },
  {
    "objectID": "posts/MTA_art/MTA_art.html",
    "href": "posts/MTA_art/MTA_art.html",
    "title": "MTA Permanent Art Catalog",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n\nmta_art = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-22/mta_art.csv')\nstation_lines = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-22/station_lines.csv')\n\n\nmta_art\n\n\n\n\n\n\n\n\nagency\nstation_name\nline\nartist\nart_title\nart_date\nart_material\nart_description\nart_image_link\n\n\n\n\n0\nNYCT\nClark St\n2,3\nRay Ring\nClark Street Passage\n1987\nTerrazzo floor tile\nThe first model that Brooklyn-born artist Ray ...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n1\nNYCT\n125 St\n4,5,6\nHouston Conwill\nThe Open Secret\n1986\nBronze - polychromed\nThe Open Secret, in the 125th Street and Lexin...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n2\nNYCT\nAstor Pl\n6\nMilton Glaser\nUntitled\n1986\nPorcelain enamel murals\nMilton Glaser, best known for his work in grap...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n3\nNYCT\nKings Hwy\nB,Q\nRhoda Andors\nKings Highway Hieroglyphs\n1987\nPorcelain Enamel Murals on Steel\nThe artist discusses her work: ÒIf public art...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n4\nNYCT\nNewkirk Av\nB,Q\nDavid Wilson\nTransit Skylight\n1988\nZinc-glazed Apolycarbonate skylight\nThe artist recalls, ÒAbout the same time that ...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n376\nNYCT\nAlabama Av\nJ,Z\nScott Redden\nUntitled\n2007\nFaceted glass\nImages of rural America evoke a nostalgia past...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n377\nNYCT\nWoodside-61 St\n7\nJohn Cavanagh\nCommuting/Community\n1986\nPorcelain enamel photomontage murals on steel\nCommuting/Community reflects John Cavanagh's i...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n378\nNYCT\nFulton St\nA, C, J, Z, 2, 3, 4, 5\nNancy Holt\nAstral Grating\n1987\nWrought iron, light elements\nNancy Holt's steel ceiling sculpture incorpora...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n379\nNYCT\n5 Av/53 St\nE,M\nRalph Fasanella\nSubway Riders\n1995\nPainting - Oil\nAll of us find ourselves from time to time mus...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n380\nNYCT\nCleveland St\nJ\nAmy Cheng\nLas Flores\n2007\nFaceted glass\nLocated within the platform windscreens, color...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n\n\n381 rows × 9 columns\n\n\n\n\nmta_art_date = mta_art.groupby('art_date').nunique()\nmta_art_date\n\n\n\n\n\n\n\n\nagency\nstation_name\nline\nartist\nart_title\nart_material\nart_description\nart_image_link\n\n\nart_date\n\n\n\n\n\n\n\n\n\n\n\n\n1980\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n1986\n1\n3\n3\n3\n3\n3\n3\n3\n\n\n1987\n1\n3\n3\n3\n3\n3\n3\n3\n\n\n1988\n1\n3\n3\n3\n3\n3\n3\n3\n\n\n1989\n1\n2\n2\n2\n2\n2\n2\n2\n\n\n1990\n2\n10\n5\n5\n6\n6\n5\n5\n\n\n1991\n2\n12\n7\n10\n13\n11\n11\n10\n\n\n1992\n3\n6\n5\n6\n6\n6\n6\n6\n\n\n1993\n2\n3\n2\n2\n3\n2\n2\n2\n\n\n1994\n2\n11\n7\n9\n9\n8\n9\n9\n\n\n1995\n2\n4\n3\n4\n4\n4\n4\n4\n\n\n1996\n2\n11\n10\n11\n11\n9\n10\n9\n\n\n1997\n2\n6\n6\n6\n6\n6\n6\n6\n\n\n1998\n3\n9\n9\n10\n10\n9\n10\n10\n\n\n1999\n3\n15\n10\n13\n13\n10\n13\n13\n\n\n2000\n2\n9\n9\n9\n9\n9\n9\n9\n\n\n2001\n2\n9\n9\n10\n10\n8\n10\n10\n\n\n2002\n3\n15\n13\n17\n17\n10\n17\n17\n\n\n2003\n1\n3\n3\n3\n3\n3\n3\n3\n\n\n2004\n1\n14\n13\n14\n14\n11\n14\n14\n\n\n2005\n3\n12\n11\n12\n12\n10\n12\n11\n\n\n2006\n2\n15\n6\n15\n15\n8\n15\n15\n\n\n2007\n2\n14\n7\n14\n14\n6\n14\n14\n\n\n2008\n2\n11\n8\n11\n11\n8\n11\n11\n\n\n2009\n3\n8\n7\n8\n8\n7\n8\n8\n\n\n2010\n2\n7\n5\n7\n7\n7\n7\n7\n\n\n2011\n2\n21\n10\n20\n21\n14\n20\n20\n\n\n2012\n2\n15\n5\n15\n15\n8\n15\n15\n\n\n2013\n3\n8\n3\n6\n6\n8\n6\n6\n\n\n2014\n2\n3\n2\n3\n3\n3\n3\n3\n\n\n2015\n3\n11\n6\n11\n11\n8\n11\n11\n\n\n2016\n2\n5\n4\n5\n5\n5\n5\n5\n\n\n2017\n3\n16\n5\n13\n14\n11\n14\n13\n\n\n2018\n4\n35\n20\n36\n36\n16\n36\n36\n\n\n2019\n3\n12\n7\n12\n12\n11\n12\n12\n\n\n2020\n3\n7\n7\n8\n8\n6\n8\n8\n\n\n2021\n3\n8\n4\n8\n8\n7\n8\n8\n\n\n2022\n2\n5\n5\n6\n6\n4\n6\n6\n\n\n2023\n3\n9\n6\n9\n9\n5\n8\n8\n\n\n\n\n\n\n\n\n# change art_matrial to sentence case\nmta_art['art_material'] = mta_art['art_material'].str.title()\nam_vc = mta_art['art_material'].value_counts()\n#am_vc\n# get am_vc greater than 1\nam_vc[am_vc &gt; 1]\n\nart_material\nFaceted Glass                                                                               50\nGlass Mosaic                                                                                49\nLaminated Glass                                                                             46\nStainless Steel                                                                             15\nCeramic Mosaic                                                                              11\nBronze                                                                                       6\nGlass And Ceramic Mosaic                                                                     6\nCopper Wire Mesh And Stainless Steel                                                         5\nMosaic                                                                                       4\nPainted Stainless Steel                                                                      4\nGlass Or Ceramic Mosaic                                                                      4\nAluminum                                                                                     4\nFused Glass                                                                                  3\nGlass Mosaics                                                                                3\nPowder-Coated Aluminum Fence Panels                                                          3\nSteel                                                                                        3\nCeramic - Porcelain Tiles                                                                    2\nSteel, Ceramic Tile, Granite, Fiberglass                                                     2\nSandblasted And Laminated Glass Windows, Steel Railings (Stair Railing & Exterior Fence)     2\nLaminated Glass And Mosaic                                                                   2\nMosaic/Glass                                                                                 2\nGlass Mosaic And Laminated Glass                                                             2\nPorcelain Enamel On Steel                                                                    2\nWrought Iron                                                                                 2\nCeramic Tile,  Glass Mosaic                                                                  2\nGlass Blocks                                                                                 2\nCeramic Tiles                                                                                2\nCeramic Tile                                                                                 2\nMosaic On Board                                                                              2\nCast Concrete                                                                                2\nCor-Ten Steel                                                                                2\nForged Steel                                                                                 2\nBronze - Patinated                                                                           2\nLaminated Glass; Stainless Steel                                                             2\nName: count, dtype: int64\n\n\n\n# Filter art_materials that appear more than once\nfiltered_materials = am_vc[am_vc &gt; 1].index\n\n# Filter the dataset for these materials\nfiltered_mta_art = mta_art[mta_art['art_material'].isin(filtered_materials)]\n\n# Find the first date for each art_material\nmaterial_timeline = filtered_mta_art.groupby('art_material')['art_date'].min().reset_index()\nmaterial_timeline = material_timeline.sort_values(by='art_date')\n# split art_material by \"(\" and keep only the first part\nmaterial_timeline['art_material'] = material_timeline['art_material'].str.split('(').str[0]\nmaterial_timeline\n\n\n\n\n\n\n\n\nart_material\nart_date\n\n\n\n\n1\nBronze\n1988\n\n\n6\nCeramic Tile\n1989\n\n\n2\nBronze - Patinated\n1990\n\n\n9\nCopper Wire Mesh And Stainless Steel\n1990\n\n\n3\nCast Concrete\n1991\n\n\n31\nSteel\n1991\n\n\n14\nGlass And Ceramic Mosaic\n1991\n\n\n10\nCor-Ten Steel\n1991\n\n\n8\nCeramic Tiles\n1991\n\n\n0\nAluminum\n1992\n\n\n16\nGlass Mosaic\n1992\n\n\n7\nCeramic Tile, Glass Mosaic\n1992\n\n\n11\nFaceted Glass\n1993\n\n\n4\nCeramic - Porcelain Tiles\n1993\n\n\n29\nSandblasted And Laminated Glass Windows, Steel...\n1994\n\n\n5\nCeramic Mosaic\n1994\n\n\n27\nPorcelain Enamel On Steel\n1997\n\n\n33\nWrought Iron\n1999\n\n\n32\nSteel, Ceramic Tile, Granite, Fiberglass\n2002\n\n\n18\nGlass Mosaics\n2003\n\n\n20\nLaminated Glass\n2006\n\n\n13\nFused Glass\n2007\n\n\n30\nStainless Steel\n2010\n\n\n12\nForged Steel\n2010\n\n\n19\nGlass Or Ceramic Mosaic\n2011\n\n\n21\nLaminated Glass And Mosaic\n2011\n\n\n24\nMosaic On Board\n2011\n\n\n15\nGlass Blocks\n2011\n\n\n17\nGlass Mosaic And Laminated Glass\n2013\n\n\n23\nMosaic\n2015\n\n\n26\nPainted Stainless Steel\n2017\n\n\n25\nMosaic/Glass\n2018\n\n\n28\nPowder-Coated Aluminum Fence Panels\n2018\n\n\n22\nLaminated Glass; Stainless Steel\n2022"
  },
  {
    "objectID": "posts/MTA_art/MTA_art.html#tidytuesday-data-for-2025-07-08",
    "href": "posts/MTA_art/MTA_art.html#tidytuesday-data-for-2025-07-08",
    "title": "MTA Permanent Art Catalog",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n\nmta_art = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-22/mta_art.csv')\nstation_lines = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-22/station_lines.csv')\n\n\nmta_art\n\n\n\n\n\n\n\n\nagency\nstation_name\nline\nartist\nart_title\nart_date\nart_material\nart_description\nart_image_link\n\n\n\n\n0\nNYCT\nClark St\n2,3\nRay Ring\nClark Street Passage\n1987\nTerrazzo floor tile\nThe first model that Brooklyn-born artist Ray ...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n1\nNYCT\n125 St\n4,5,6\nHouston Conwill\nThe Open Secret\n1986\nBronze - polychromed\nThe Open Secret, in the 125th Street and Lexin...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n2\nNYCT\nAstor Pl\n6\nMilton Glaser\nUntitled\n1986\nPorcelain enamel murals\nMilton Glaser, best known for his work in grap...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n3\nNYCT\nKings Hwy\nB,Q\nRhoda Andors\nKings Highway Hieroglyphs\n1987\nPorcelain Enamel Murals on Steel\nThe artist discusses her work: ÒIf public art...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n4\nNYCT\nNewkirk Av\nB,Q\nDavid Wilson\nTransit Skylight\n1988\nZinc-glazed Apolycarbonate skylight\nThe artist recalls, ÒAbout the same time that ...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n376\nNYCT\nAlabama Av\nJ,Z\nScott Redden\nUntitled\n2007\nFaceted glass\nImages of rural America evoke a nostalgia past...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n377\nNYCT\nWoodside-61 St\n7\nJohn Cavanagh\nCommuting/Community\n1986\nPorcelain enamel photomontage murals on steel\nCommuting/Community reflects John Cavanagh's i...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n378\nNYCT\nFulton St\nA, C, J, Z, 2, 3, 4, 5\nNancy Holt\nAstral Grating\n1987\nWrought iron, light elements\nNancy Holt's steel ceiling sculpture incorpora...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n379\nNYCT\n5 Av/53 St\nE,M\nRalph Fasanella\nSubway Riders\n1995\nPainting - Oil\nAll of us find ourselves from time to time mus...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n380\nNYCT\nCleveland St\nJ\nAmy Cheng\nLas Flores\n2007\nFaceted glass\nLocated within the platform windscreens, color...\nhttps://new.mta.info/agency/arts-design/collec...\n\n\n\n\n381 rows × 9 columns\n\n\n\n\nmta_art_date = mta_art.groupby('art_date').nunique()\nmta_art_date\n\n\n\n\n\n\n\n\nagency\nstation_name\nline\nartist\nart_title\nart_material\nart_description\nart_image_link\n\n\nart_date\n\n\n\n\n\n\n\n\n\n\n\n\n1980\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n1986\n1\n3\n3\n3\n3\n3\n3\n3\n\n\n1987\n1\n3\n3\n3\n3\n3\n3\n3\n\n\n1988\n1\n3\n3\n3\n3\n3\n3\n3\n\n\n1989\n1\n2\n2\n2\n2\n2\n2\n2\n\n\n1990\n2\n10\n5\n5\n6\n6\n5\n5\n\n\n1991\n2\n12\n7\n10\n13\n11\n11\n10\n\n\n1992\n3\n6\n5\n6\n6\n6\n6\n6\n\n\n1993\n2\n3\n2\n2\n3\n2\n2\n2\n\n\n1994\n2\n11\n7\n9\n9\n8\n9\n9\n\n\n1995\n2\n4\n3\n4\n4\n4\n4\n4\n\n\n1996\n2\n11\n10\n11\n11\n9\n10\n9\n\n\n1997\n2\n6\n6\n6\n6\n6\n6\n6\n\n\n1998\n3\n9\n9\n10\n10\n9\n10\n10\n\n\n1999\n3\n15\n10\n13\n13\n10\n13\n13\n\n\n2000\n2\n9\n9\n9\n9\n9\n9\n9\n\n\n2001\n2\n9\n9\n10\n10\n8\n10\n10\n\n\n2002\n3\n15\n13\n17\n17\n10\n17\n17\n\n\n2003\n1\n3\n3\n3\n3\n3\n3\n3\n\n\n2004\n1\n14\n13\n14\n14\n11\n14\n14\n\n\n2005\n3\n12\n11\n12\n12\n10\n12\n11\n\n\n2006\n2\n15\n6\n15\n15\n8\n15\n15\n\n\n2007\n2\n14\n7\n14\n14\n6\n14\n14\n\n\n2008\n2\n11\n8\n11\n11\n8\n11\n11\n\n\n2009\n3\n8\n7\n8\n8\n7\n8\n8\n\n\n2010\n2\n7\n5\n7\n7\n7\n7\n7\n\n\n2011\n2\n21\n10\n20\n21\n14\n20\n20\n\n\n2012\n2\n15\n5\n15\n15\n8\n15\n15\n\n\n2013\n3\n8\n3\n6\n6\n8\n6\n6\n\n\n2014\n2\n3\n2\n3\n3\n3\n3\n3\n\n\n2015\n3\n11\n6\n11\n11\n8\n11\n11\n\n\n2016\n2\n5\n4\n5\n5\n5\n5\n5\n\n\n2017\n3\n16\n5\n13\n14\n11\n14\n13\n\n\n2018\n4\n35\n20\n36\n36\n16\n36\n36\n\n\n2019\n3\n12\n7\n12\n12\n11\n12\n12\n\n\n2020\n3\n7\n7\n8\n8\n6\n8\n8\n\n\n2021\n3\n8\n4\n8\n8\n7\n8\n8\n\n\n2022\n2\n5\n5\n6\n6\n4\n6\n6\n\n\n2023\n3\n9\n6\n9\n9\n5\n8\n8\n\n\n\n\n\n\n\n\n# change art_matrial to sentence case\nmta_art['art_material'] = mta_art['art_material'].str.title()\nam_vc = mta_art['art_material'].value_counts()\n#am_vc\n# get am_vc greater than 1\nam_vc[am_vc &gt; 1]\n\nart_material\nFaceted Glass                                                                               50\nGlass Mosaic                                                                                49\nLaminated Glass                                                                             46\nStainless Steel                                                                             15\nCeramic Mosaic                                                                              11\nBronze                                                                                       6\nGlass And Ceramic Mosaic                                                                     6\nCopper Wire Mesh And Stainless Steel                                                         5\nMosaic                                                                                       4\nPainted Stainless Steel                                                                      4\nGlass Or Ceramic Mosaic                                                                      4\nAluminum                                                                                     4\nFused Glass                                                                                  3\nGlass Mosaics                                                                                3\nPowder-Coated Aluminum Fence Panels                                                          3\nSteel                                                                                        3\nCeramic - Porcelain Tiles                                                                    2\nSteel, Ceramic Tile, Granite, Fiberglass                                                     2\nSandblasted And Laminated Glass Windows, Steel Railings (Stair Railing & Exterior Fence)     2\nLaminated Glass And Mosaic                                                                   2\nMosaic/Glass                                                                                 2\nGlass Mosaic And Laminated Glass                                                             2\nPorcelain Enamel On Steel                                                                    2\nWrought Iron                                                                                 2\nCeramic Tile,  Glass Mosaic                                                                  2\nGlass Blocks                                                                                 2\nCeramic Tiles                                                                                2\nCeramic Tile                                                                                 2\nMosaic On Board                                                                              2\nCast Concrete                                                                                2\nCor-Ten Steel                                                                                2\nForged Steel                                                                                 2\nBronze - Patinated                                                                           2\nLaminated Glass; Stainless Steel                                                             2\nName: count, dtype: int64\n\n\n\n# Filter art_materials that appear more than once\nfiltered_materials = am_vc[am_vc &gt; 1].index\n\n# Filter the dataset for these materials\nfiltered_mta_art = mta_art[mta_art['art_material'].isin(filtered_materials)]\n\n# Find the first date for each art_material\nmaterial_timeline = filtered_mta_art.groupby('art_material')['art_date'].min().reset_index()\nmaterial_timeline = material_timeline.sort_values(by='art_date')\n# split art_material by \"(\" and keep only the first part\nmaterial_timeline['art_material'] = material_timeline['art_material'].str.split('(').str[0]\nmaterial_timeline\n\n\n\n\n\n\n\n\nart_material\nart_date\n\n\n\n\n1\nBronze\n1988\n\n\n6\nCeramic Tile\n1989\n\n\n2\nBronze - Patinated\n1990\n\n\n9\nCopper Wire Mesh And Stainless Steel\n1990\n\n\n3\nCast Concrete\n1991\n\n\n31\nSteel\n1991\n\n\n14\nGlass And Ceramic Mosaic\n1991\n\n\n10\nCor-Ten Steel\n1991\n\n\n8\nCeramic Tiles\n1991\n\n\n0\nAluminum\n1992\n\n\n16\nGlass Mosaic\n1992\n\n\n7\nCeramic Tile, Glass Mosaic\n1992\n\n\n11\nFaceted Glass\n1993\n\n\n4\nCeramic - Porcelain Tiles\n1993\n\n\n29\nSandblasted And Laminated Glass Windows, Steel...\n1994\n\n\n5\nCeramic Mosaic\n1994\n\n\n27\nPorcelain Enamel On Steel\n1997\n\n\n33\nWrought Iron\n1999\n\n\n32\nSteel, Ceramic Tile, Granite, Fiberglass\n2002\n\n\n18\nGlass Mosaics\n2003\n\n\n20\nLaminated Glass\n2006\n\n\n13\nFused Glass\n2007\n\n\n30\nStainless Steel\n2010\n\n\n12\nForged Steel\n2010\n\n\n19\nGlass Or Ceramic Mosaic\n2011\n\n\n21\nLaminated Glass And Mosaic\n2011\n\n\n24\nMosaic On Board\n2011\n\n\n15\nGlass Blocks\n2011\n\n\n17\nGlass Mosaic And Laminated Glass\n2013\n\n\n23\nMosaic\n2015\n\n\n26\nPainted Stainless Steel\n2017\n\n\n25\nMosaic/Glass\n2018\n\n\n28\nPowder-Coated Aluminum Fence Panels\n2018\n\n\n22\nLaminated Glass; Stainless Steel\n2022"
  },
  {
    "objectID": "posts/MTA_art/MTA_art.html#plotting",
    "href": "posts/MTA_art/MTA_art.html#plotting",
    "title": "MTA Permanent Art Catalog",
    "section": "Plotting",
    "text": "Plotting\n\nfig, ax = plt.subplots(figsize=(7,7))\nml,sl,bl = ax.stem(material_timeline['art_date'], material_timeline['art_material'],\\\n basefmt=' ', bottom=34)\nplt.setp(sl, linestyle=':', color='lightblue')\nplt.setp(ml,  marker=\"*\")\nfor i, txt in enumerate(material_timeline['art_material']):\n    ax.annotate(txt, (material_timeline['art_date'][i]+0.5, material_timeline['art_material'][i]), xytext=(0, -2), textcoords='offset points')\nax.get_yaxis().set_visible(False)\n#ax.invert_yaxis()\nax.set_xticks(material_timeline['art_date'].unique())\nax.set_xticklabels(material_timeline['art_date'].unique(), rotation=90)\nax.xaxis.set_ticks_position('top')\nax.xaxis.set_label_position('top')\nax.tick_params(axis='x', which='both', length=0)\nplt.xticks(fontfamily='monospace', color=\"gray\")\nplt.tick_params(axis='x', pad=-15)\nax.spines[['top', 'right', 'bottom', 'left']].set_visible(False)\nplt.title(\"First appearance of art materials in the MTA art data.\\nArt materials that appear more than once are shown.\", fontfamily='serif', fontsize=12, loc='left')\n#plt.savefig(\"MTA_art.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "posts/Judges_appoint/Judges_appoint.html",
    "href": "posts/Judges_appoint/Judges_appoint.html",
    "title": "The US Judges data",
    "section": "",
    "text": "import pandas as pd\n\n\njudges_appointments = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-10/judges_appointments.csv')\njudges_people = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-10/judges_people.csv')\n\n\njudges_appointments\n\n\n\n\n\n\n\n\njudge_id\ncourt_name\ncourt_type\npresident_name\npresident_party\nnomination_date\npredecessor_last_name\npredecessor_first_name\nsenate_confirmation_date\ncommission_date\nchief_judge_begin\nchief_judge_end\nretirement_from_active_service\ntermination_date\ntermination_reason\n\n\n\n\n0\n3419\nU. S. District Court, Southern District of New...\nUSDC\nBarack Obama\nDemocratic\n07/28/2011\nKaplan\nLewis A.\n03/22/2012\n03/23/2012\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n1\nU. S. District Court, Eastern District of New ...\nUSDC\nFranklin D. Roosevelt\nDemocratic\n02/03/1936\nnew\nNaN\n02/12/1936\n02/15/1936\nNaN\nNaN\n02/15/1966\n05/28/1971\nDeath\n\n\n2\n2\nU. S. District Court, Western District of Penn...\nUSDC\nRutherford B. Hayes\nRepublican\n01/06/1880\nKetcham\nWinthrop\n01/14/1880\n01/14/1880\nNaN\nNaN\nNaN\n02/09/1891\nAppointment to Another Judicial Position\n\n\n3\n3\nU. S. District Court, Northern District of Ala...\nUSDC\nRonald Reagan\nRepublican\n07/22/1982\nMcFadden\nFrank H.\n08/18/1982\n08/18/1982\nNaN\nNaN\n05/31/1996\nNaN\nNaN\n\n\n4\n4\nU. S. District Court, District of New Jersey\nUSDC\nJimmy Carter\nDemocratic\n09/28/1979\nBarlow\nGeorge H.\n10/31/1979\n11/02/1979\nNaN\nNaN\n02/15/1994\n12/02/2009\nDeath\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4197\n2201\nU. S. District Courts, Albemarle, Cape Fear & ...\nUSDC\nReassignment\nReassignment\nNaN\nnew\nNaN\nNaN\n02/13/1801\nNaN\nNaN\nNaN\n03/04/1802\nDeath\n\n\n4198\n2689\nU. S. District Court, Eastern District of Miss...\nUSDC\nHarry S Truman\nDemocratic\n01/13/1949\nHarper\nRoy Winfield\n01/31/1949\n02/02/1949\nNaN\nNaN\n01/05/1971\n02/13/1994\nDeath\n\n\n4199\n1126\nU. S. Court of Appeals for the Ninth Circuit\nUSCA\nWilliam H. Taft\nRepublican\n12/12/1910\nnew\nNaN\n01/31/1911\n02/08/1911\nNaN\nNaN\n01/31/1928\n11/30/1928\nRetirement\n\n\n4200\n1453\nU. S. Court of Appeals for the Second Circuit\nUSCA\nReassignment\nReassignment\nNaN\nnew\nNaN\nNaN\n07/01/1929\nNaN\nNaN\n09/06/1940\n09/05/1943\nDeath\n\n\n4201\n2689\nU. S. District Court, Western District of Miss...\nUSDC\nHarry S Truman\nDemocratic\n01/13/1949\nHarper\nRoy Winfield\n01/31/1949\n02/02/1949\nNaN\nNaN\n01/05/1971\n02/13/1994\nDeath\n\n\n\n\n4202 rows × 15 columns\n\n\n\n\njudges_appointments['commission_date'] = pd.to_datetime(judges_appointments['commission_date'])\njudges_appointments['commission_date'].dt.year.min()\n\n1789.0\n\n\n\njudges_people\n\n\n\n\n\n\n\n\njudge_id\nname_first\nname_middle\nname_last\nname_suffix\nbirth_date\nbirthplace_city\nbirthplace_state\ndeath_date\ndeath_city\ndeath_state\ngender\nrace\n\n\n\n\n0\n3419\nRonnie\nNaN\nAbrams\nNaN\n1968.0\nNew York\nNY\nNaN\nNaN\nNaN\nF\nWhite\n\n\n1\n1\nMatthew\nT.\nAbruzzo\nNaN\n1889.0\nBrooklyn\nNY\n1971.0\nPotomac\nMD\nM\nWhite\n\n\n2\n2\nMarcus\nWilson\nAcheson\nNaN\n1828.0\nWashington\nPA\n1906.0\nPittsburgh\nPA\nM\nWhite\n\n\n3\n3\nWilliam\nMarsh\nAcker\nJr.\n1927.0\nBirmingham\nAL\nNaN\nNaN\nNaN\nM\nWhite\n\n\n4\n4\nHarold\nArnold\nAckerman\nNaN\n1928.0\nNewark\nNJ\n2009.0\nWest Orange\nNJ\nM\nWhite\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3527\n3391\nJennifer\nGuerin\nZipps\nNaN\n1964.0\nAshland\nOH\nNaN\nNaN\nNaN\nF\nWhite\n\n\n3528\n2687\nAlfonso\nJoseph\nZirpoli\nNaN\n1905.0\nDenver\nCO\n1995.0\nSan Francisco\nCA\nM\nWhite\n\n\n3529\n2688\nWilliam\nJ.\nZloch\nNaN\n1944.0\nFort Lauderdale\nFL\nNaN\nNaN\nNaN\nM\nWhite\n\n\n3530\n2690\nRya\nWeickert\nZobel\nNaN\n1931.0\nZwickau\nGermany\nNaN\nNaN\nNaN\nF\nWhite\n\n\n3531\n3106\nJack\nNaN\nZouhary\nNaN\n1951.0\nToledo\nOH\nNaN\nNaN\nNaN\nM\nWhite\n\n\n\n\n3532 rows × 13 columns\n\n\n\n\njudges_people.groupby([\"gender\"]).size()\n\ngender\nF     389\nM    3143\ndtype: int64\n\n\n\njudges_people['race']=judges_people['race'].fillna(\"Others\")\n\n\ndf_grp = judges_people.groupby([\"gender\",\"race\"]).count().sort_values([\"gender\",\"judge_id\"], ascending=[True,False])\ndf_grp\n\n\n\n\n\n\n\n\n\njudge_id\nname_first\nname_middle\nname_last\nname_suffix\nbirth_date\nbirthplace_city\nbirthplace_state\ndeath_date\ndeath_city\ndeath_state\n\n\ngender\nrace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF\nWhite\n293\n293\n265\n293\n0\n293\n293\n293\n28\n24\n24\n\n\nAfrican American\n51\n51\n50\n51\n0\n51\n51\n51\n3\n3\n3\n\n\nHispanic\n29\n29\n21\n29\n0\n29\n29\n29\n1\n1\n1\n\n\nAsian American\n9\n9\n9\n9\n0\n9\n9\n9\n0\n0\n0\n\n\nOthers\n3\n3\n3\n3\n0\n3\n3\n3\n0\n0\n0\n\n\nAfrican Am./Hispanic\n1\n1\n1\n1\n0\n1\n1\n1\n0\n0\n0\n\n\nAmerican Indian\n1\n1\n1\n1\n0\n1\n1\n1\n0\n0\n0\n\n\nHispanic/Asian Am.\n1\n1\n0\n1\n0\n1\n1\n1\n0\n0\n0\n\n\nWhite/Asian Am.\n1\n1\n0\n1\n0\n1\n1\n1\n0\n0\n0\n\n\nM\nWhite\n2871\n2871\n2483\n2871\n299\n2870\n2848\n2869\n1906\n1350\n1352\n\n\nAfrican American\n150\n150\n135\n150\n41\n150\n150\n150\n42\n38\n38\n\n\nHispanic\n87\n87\n70\n87\n5\n87\n86\n86\n15\n9\n9\n\n\nAsian American\n22\n22\n18\n22\n0\n22\n22\n22\n6\n4\n4\n\n\nOthers\n6\n6\n6\n6\n1\n6\n6\n6\n1\n1\n1\n\n\nAmerican Indian\n2\n2\n2\n2\n0\n2\n2\n2\n0\n0\n0\n\n\nPac. Isl./Asian Am.\n2\n2\n2\n2\n0\n2\n2\n2\n0\n0\n0\n\n\nAfrican Am./Hispanic\n1\n1\n1\n1\n0\n1\n1\n1\n0\n0\n0\n\n\nHispanic/White\n1\n1\n1\n1\n0\n1\n1\n1\n0\n0\n0\n\n\nPac. Isl./White\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\n\ndf1 = df_grp[\"judge_id\"].reset_index()\ndf1\n\n\n\n\n\n\n\n\ngender\nrace\njudge_id\n\n\n\n\n0\nF\nWhite\n293\n\n\n1\nF\nAfrican American\n51\n\n\n2\nF\nHispanic\n29\n\n\n3\nF\nAsian American\n9\n\n\n4\nF\nOthers\n3\n\n\n5\nF\nAfrican Am./Hispanic\n1\n\n\n6\nF\nAmerican Indian\n1\n\n\n7\nF\nHispanic/Asian Am.\n1\n\n\n8\nF\nWhite/Asian Am.\n1\n\n\n9\nM\nWhite\n2871\n\n\n10\nM\nAfrican American\n150\n\n\n11\nM\nHispanic\n87\n\n\n12\nM\nAsian American\n22\n\n\n13\nM\nOthers\n6\n\n\n14\nM\nAmerican Indian\n2\n\n\n15\nM\nPac. Isl./Asian Am.\n2\n\n\n16\nM\nAfrican Am./Hispanic\n1\n\n\n17\nM\nHispanic/White\n1\n\n\n18\nM\nPac. Isl./White\n1\n\n\n\n\n\n\n\n\ndef process_group(group):\n    top_rows = group.nlargest(4, 'judge_id')  \n    remaining_rows = group.iloc[4:]  \n    if not remaining_rows.empty:\n        other_sum = remaining_rows['judge_id'].sum()\n        other_row = pd.DataFrame({'gender': [group.name], 'race': ['Others'], 'judge_id': [other_sum]})\n        return pd.concat([top_rows, other_row], ignore_index=True)\n    return top_rows\n\n# Apply function to each group\ndf_grouped = df1.groupby('gender', group_keys=False).apply(process_group)\n\ndf_grouped\n\n\n\n\n\n\n\n\ngender\nrace\njudge_id\n\n\n\n\n0\nF\nWhite\n293\n\n\n1\nF\nAfrican American\n51\n\n\n2\nF\nHispanic\n29\n\n\n3\nF\nAsian American\n9\n\n\n4\nF\nOthers\n7\n\n\n0\nM\nWhite\n2871\n\n\n1\nM\nAfrican American\n150\n\n\n2\nM\nHispanic\n87\n\n\n3\nM\nAsian American\n22\n\n\n4\nM\nOthers\n13\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\nfrom PIL import Image\nimport textwrap\n\n# Define race colors\nrace_colors = {\n    'White': (160, 200, 220), \n    'African American': (70, 130, 180),\n    'Hispanic': (0, 0, 128), \n    'Asian American': (30, 144, 255),  \n    'Others': (0, 0, 0)  \n}\ndf_grouped_dict = df_grouped.groupby(\"gender\").apply(lambda x: dict(zip(x[\"race\"], x[\"judge_id\"]))).to_dict()\n\ndf_percentages = {gender: {race: (count / sum(race_counts.values())) * 100 for race, count in race_counts.items()}\n                  for gender, race_counts in df_grouped_dict.items()}\n\n# Function to recolor an icon with stacked race colors\ndef recolor_icon_layers(icon_path, race_counts, race_colors):\n    img = Image.open(icon_path).convert(\"RGBA\")  # Convert to RGBA\n    data = np.array(img)  # Convert image to array\n\n    total_count = sum(race_counts.values())\n    height = data.shape[0]\n    y_start = 0\n\n    for race, count in race_counts.items():\n        layer_height = int((count / total_count) * height)\n        y_end = y_start + layer_height\n        mask = data[y_start:y_end, :, 3] &gt; 0  # Keep only non-transparent pixels\n        data[y_start:y_end, :, :-1][mask] = race_colors.get(race, (128, 128, 128))  # Apply race color\n        y_start = y_end\n\n    return Image.fromarray(data)\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Calculate total counts for scaling\ntotal_category1 = sum(df_grouped_dict.get(\"F\", {}).values())\ntotal_category2 = sum(df_grouped_dict.get(\"M\", {}).values())\n\nrace_colors_mpl = {race: (r/255, g/255, b/255) for race, (r, g, b) in race_colors.items()}\n\nfor i, (category, race_counts) in enumerate(df_grouped_dict.items()):\n    icon_path = \"person-dress.png\" if category == \"F\" else \"person.png\"\n    icon_array = recolor_icon_layers(icon_path, race_counts, race_colors)\n\n    scale_factor = sum(race_counts.values()) / max(total_category1, total_category2)\n    base_zoom = 0.8  # Adjust zoom level\n    imagebox = OffsetImage(np.array(icon_array), zoom=base_zoom * scale_factor)\n\n    ab = AnnotationBbox(imagebox, (i, 0), frameon=False, xycoords=\"data\", box_alignment=(0.5, 0))\n    ax.add_artist(ab)\n\n    percentages = df_percentages[category]\n    for j, (race, percent) in enumerate(reversed(percentages.items())):\n        if(category == \"F\"):\n            ax.text(i+0.3, j*150, f\"{percent:.1f}%\", ha='right', fontsize=10, color=race_colors_mpl[race])\n        else:            \n            ax.text(i+0.4, j*250, f\"{race}: {percent:.1f}%\", ha='left', fontsize=10, color=race_colors_mpl[race])\n\nax.text(0, 1000, f\"Female judges: {total_category1}\", ha='center', fontsize=10, color=\"black\")\nax.text(1, 4000, f\"Male judges: {total_category2:,}\", ha='center', fontsize=10, color=\"black\")\n\nax.set_xticks([0, 1])\nax.set_xlim(-0.5, 1.5)\nax.set_ylim(0, max(total_category1, total_category2) + 100)\ntitle = f\"Among the US judges appointed from {judges_appointments['commission_date'].dt.year.min():.0f} to {judges_appointments['commission_date'].dt.year.max():.0f}, there were about 8 male judges for every female judge. The proportion of the top four races is shown in different colors.\"\nwrapped_title = \"\\n\".join(textwrap.wrap(title, width=30))\nax.set_title(wrapped_title, loc='left', pad=75)\nax.axis(\"off\")\nfig.patch.set_facecolor(\"whitesmoke\")\nfig.savefig(\"judges_appoint.png\", bbox_inches=\"tight\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "posts/Gutenberg_book/Gutenberg_books.html",
    "href": "posts/Gutenberg_book/Gutenberg_books.html",
    "title": "Project Gutenberg",
    "section": "",
    "text": "library(tidyverse)\nlibrary(wordcloud)\nlibrary(RColorBrewer)\nlibrary(ISOcodes)\nlibrary(scales)\nlibrary(magick)\n\n\ngutenberg_authors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_authors.csv')\ngutenberg_languages &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_languages.csv')\ngutenberg_metadata &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_metadata.csv')\ngutenberg_subjects &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_subjects.csv')\n\nglimpse(gutenberg_authors)\n\nRows: 26,077\nColumns: 7\n$ gutenberg_author_id &lt;dbl&gt; 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ author              &lt;chr&gt; \"United States\", \"Lincoln, Abraham\", \"Henry, Patri…\n$ alias               &lt;chr&gt; \"U.S.A.\", NA, NA, NA, \"Dodgson, Charles Lutwidge\",…\n$ birthdate           &lt;dbl&gt; NA, 1809, 1736, 1849, 1832, NA, 1819, 1860, NA, 18…\n$ deathdate           &lt;dbl&gt; NA, 1865, 1799, 1931, 1898, NA, 1891, 1937, NA, 18…\n$ wikipedia           &lt;chr&gt; \"https://en.wikipedia.org/wiki/United_States\", \"ht…\n$ aliases             &lt;chr&gt; \"U.S.A.\", \"United States President (1861-1865)/Lin…\n\nglimpse(gutenberg_languages)\n\nRows: 76,205\nColumns: 3\n$ gutenberg_id    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ language        &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ total_languages &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n\nglimpse(gutenberg_metadata)\n\nRows: 79,491\nColumns: 8\n$ gutenberg_id        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, 12, 13, 14,…\n$ title               &lt;chr&gt; \"The Declaration of Independence of the United Sta…\n$ author              &lt;chr&gt; \"Jefferson, Thomas\", \"United States\", \"Kennedy, Jo…\n$ gutenberg_author_id &lt;dbl&gt; 1638, 1, 1666, 3, 1, 4, NA, 3, 3, NA, 7, 7, 7, 7, …\n$ language            &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"e…\n$ gutenberg_bookshelf &lt;chr&gt; \"Politics/American Revolutionary War/United States…\n$ rights              &lt;chr&gt; \"Public domain in the USA.\", \"Public domain in the…\n$ has_text            &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n\nglimpse(gutenberg_subjects)\n\nRows: 255,312\nColumns: 3\n$ gutenberg_id &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, …\n$ subject_type &lt;chr&gt; \"lcsh\", \"lcsh\", \"lcc\", \"lcc\", \"lcsh\", \"lcsh\", \"lcc\", \"lcc…\n$ subject      &lt;chr&gt; \"United States -- History -- Revolution, 1775-1783 -- Sou…\n\n\n\nlang_counts &lt;- gutenberg_languages %&gt;% \n  group_by(language) %&gt;%\n  summarise(total_languages = sum(total_languages), .groups = 'drop') %&gt;%\n  arrange(desc(total_languages)) %&gt;% \n  left_join(ISO_639_2, by = c(\"language\" = \"Alpha_2\")) %&gt;% \n  select(language, Name, total_languages)\nglimpse(lang_counts)\n\nRows: 70\nColumns: 3\n$ language        &lt;chr&gt; \"en\", \"fr\", \"fi\", \"de\", \"it\", \"nl\", \"es\", \"pt\", \"hu\", …\n$ Name            &lt;chr&gt; \"English\", \"French\", \"Finnish\", \"German\", \"Italian\", \"…\n$ total_languages &lt;dbl&gt; 60875, 4019, 3314, 2363, 1061, 1053, 917, 651, 610, 45…\n\n\n\npng(\"word_cloud.png\", width = 4, height = 4, units = \"in\", res = 300, bg = \"black\")\np1 &lt;- wordcloud(words = lang_counts$Name, freq = lang_counts$total_languages,\n          min.freq = 0,\n          random.order = FALSE,\n          colors = brewer.pal(9, \"Pastel1\"))\ndev.off()\n\npng \n  2 \n\n\n\nlang_counts %&gt;% \n  filter(language != \"en\") %&gt;% \n  arrange(desc(total_languages)) %&gt;% \n  slice(1:10) %&gt;% \n  ggplot(aes(x = total_languages, y =reorder(Name, total_languages))) +\n  geom_col(alpha=0) +\n  geom_text(aes(label = comma(total_languages), x=70), color = \"grey\", hjust=1) +\n  theme_minimal() +\n  scale_y_discrete(labels = function(x) paste0(x, \":\"))+\n  coord_cartesian(xlim = c(0, 100)) +\n  theme(axis.title = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.text.y = element_text(color = \"grey\"),\n        axis.line = element_blank(),\n        panel.grid = element_blank(),\n        plot.margin = margin(75, 0, 75, 0))\n\n\n\n\n\n\n\nggsave(\"plot1.png\", width = 1.75, height = 4, units = \"in\", bg=\"black\")\n\n\nimg1 &lt;- image_read(\"word_cloud.png\")\nimg2 &lt;- image_read(\"plot1.png\")\ncombined &lt;- image_append(c(img1, img2), stack = FALSE)\n#image_write(combined, \"combined.png\")\n\ntitle_text &lt;- paste(\"In Project Gutenberg, \", comma(lang_counts$total_languages[1]), \" out of \", comma(dim(gutenberg_languages)[1]), \"books are in English. The word cloud is based on the book counts in different languages, excluding English. Counts for top ten languages are shown on the right.\")\ntitle_text &lt;- paste(strwrap(title_text, width = 75), collapse = \"\\n\")\ntitle_image &lt;- image_blank(width = image_info(combined)$width,\n                           height = 160, # adjust height as needed\n                           color = \"black\") %&gt;%\n  image_annotate(text = title_text,\n                 size = 40, # adjust font size\n                 color = \"grey\",\n                 gravity = \"center\")\n\nfinal_image &lt;- image_composite(combined, title_image, offset = \"+0+0\")\nimage_write(final_image, \"combined_with_title.png\")"
  },
  {
    "objectID": "posts/Gutenberg_book/Gutenberg_books.html#tidytuesday-data-for-2025-06-03",
    "href": "posts/Gutenberg_book/Gutenberg_books.html#tidytuesday-data-for-2025-06-03",
    "title": "Project Gutenberg",
    "section": "",
    "text": "library(tidyverse)\nlibrary(wordcloud)\nlibrary(RColorBrewer)\nlibrary(ISOcodes)\nlibrary(scales)\nlibrary(magick)\n\n\ngutenberg_authors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_authors.csv')\ngutenberg_languages &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_languages.csv')\ngutenberg_metadata &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_metadata.csv')\ngutenberg_subjects &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_subjects.csv')\n\nglimpse(gutenberg_authors)\n\nRows: 26,077\nColumns: 7\n$ gutenberg_author_id &lt;dbl&gt; 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ author              &lt;chr&gt; \"United States\", \"Lincoln, Abraham\", \"Henry, Patri…\n$ alias               &lt;chr&gt; \"U.S.A.\", NA, NA, NA, \"Dodgson, Charles Lutwidge\",…\n$ birthdate           &lt;dbl&gt; NA, 1809, 1736, 1849, 1832, NA, 1819, 1860, NA, 18…\n$ deathdate           &lt;dbl&gt; NA, 1865, 1799, 1931, 1898, NA, 1891, 1937, NA, 18…\n$ wikipedia           &lt;chr&gt; \"https://en.wikipedia.org/wiki/United_States\", \"ht…\n$ aliases             &lt;chr&gt; \"U.S.A.\", \"United States President (1861-1865)/Lin…\n\nglimpse(gutenberg_languages)\n\nRows: 76,205\nColumns: 3\n$ gutenberg_id    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ language        &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ total_languages &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n\nglimpse(gutenberg_metadata)\n\nRows: 79,491\nColumns: 8\n$ gutenberg_id        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, 12, 13, 14,…\n$ title               &lt;chr&gt; \"The Declaration of Independence of the United Sta…\n$ author              &lt;chr&gt; \"Jefferson, Thomas\", \"United States\", \"Kennedy, Jo…\n$ gutenberg_author_id &lt;dbl&gt; 1638, 1, 1666, 3, 1, 4, NA, 3, 3, NA, 7, 7, 7, 7, …\n$ language            &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"e…\n$ gutenberg_bookshelf &lt;chr&gt; \"Politics/American Revolutionary War/United States…\n$ rights              &lt;chr&gt; \"Public domain in the USA.\", \"Public domain in the…\n$ has_text            &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n\nglimpse(gutenberg_subjects)\n\nRows: 255,312\nColumns: 3\n$ gutenberg_id &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, …\n$ subject_type &lt;chr&gt; \"lcsh\", \"lcsh\", \"lcc\", \"lcc\", \"lcsh\", \"lcsh\", \"lcc\", \"lcc…\n$ subject      &lt;chr&gt; \"United States -- History -- Revolution, 1775-1783 -- Sou…\n\n\n\nlang_counts &lt;- gutenberg_languages %&gt;% \n  group_by(language) %&gt;%\n  summarise(total_languages = sum(total_languages), .groups = 'drop') %&gt;%\n  arrange(desc(total_languages)) %&gt;% \n  left_join(ISO_639_2, by = c(\"language\" = \"Alpha_2\")) %&gt;% \n  select(language, Name, total_languages)\nglimpse(lang_counts)\n\nRows: 70\nColumns: 3\n$ language        &lt;chr&gt; \"en\", \"fr\", \"fi\", \"de\", \"it\", \"nl\", \"es\", \"pt\", \"hu\", …\n$ Name            &lt;chr&gt; \"English\", \"French\", \"Finnish\", \"German\", \"Italian\", \"…\n$ total_languages &lt;dbl&gt; 60875, 4019, 3314, 2363, 1061, 1053, 917, 651, 610, 45…\n\n\n\npng(\"word_cloud.png\", width = 4, height = 4, units = \"in\", res = 300, bg = \"black\")\np1 &lt;- wordcloud(words = lang_counts$Name, freq = lang_counts$total_languages,\n          min.freq = 0,\n          random.order = FALSE,\n          colors = brewer.pal(9, \"Pastel1\"))\ndev.off()\n\npng \n  2 \n\n\n\nlang_counts %&gt;% \n  filter(language != \"en\") %&gt;% \n  arrange(desc(total_languages)) %&gt;% \n  slice(1:10) %&gt;% \n  ggplot(aes(x = total_languages, y =reorder(Name, total_languages))) +\n  geom_col(alpha=0) +\n  geom_text(aes(label = comma(total_languages), x=70), color = \"grey\", hjust=1) +\n  theme_minimal() +\n  scale_y_discrete(labels = function(x) paste0(x, \":\"))+\n  coord_cartesian(xlim = c(0, 100)) +\n  theme(axis.title = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.text.y = element_text(color = \"grey\"),\n        axis.line = element_blank(),\n        panel.grid = element_blank(),\n        plot.margin = margin(75, 0, 75, 0))\n\n\n\n\n\n\n\nggsave(\"plot1.png\", width = 1.75, height = 4, units = \"in\", bg=\"black\")\n\n\nimg1 &lt;- image_read(\"word_cloud.png\")\nimg2 &lt;- image_read(\"plot1.png\")\ncombined &lt;- image_append(c(img1, img2), stack = FALSE)\n#image_write(combined, \"combined.png\")\n\ntitle_text &lt;- paste(\"In Project Gutenberg, \", comma(lang_counts$total_languages[1]), \" out of \", comma(dim(gutenberg_languages)[1]), \"books are in English. The word cloud is based on the book counts in different languages, excluding English. Counts for top ten languages are shown on the right.\")\ntitle_text &lt;- paste(strwrap(title_text, width = 75), collapse = \"\\n\")\ntitle_image &lt;- image_blank(width = image_info(combined)$width,\n                           height = 160, # adjust height as needed\n                           color = \"black\") %&gt;%\n  image_annotate(text = title_text,\n                 size = 40, # adjust font size\n                 color = \"grey\",\n                 gravity = \"center\")\n\nfinal_image &lt;- image_composite(combined, title_image, offset = \"+0+0\")\nimage_write(final_image, \"combined_with_title.png\")"
  },
  {
    "objectID": "posts/Dungeons_Dragons/DD_monsters.html",
    "href": "posts/Dungeons_Dragons/DD_monsters.html",
    "title": "Dungeons and Dragons Monsters (2024)",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nmonsters = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-27/monsters.csv')\n\n\nmonsters\n\n\n\n\n\n\n\n\nname\ncategory\ncr\nsize\ntype\ndescriptive_tags\nalignment\nac\ninitiative\nhp\n...\nwis_save\ncha_save\nskills\nresistances\nvulnerabilities\nimmunities\ngear\nsenses\nlanguages\nfull_text\n\n\n\n\n0\nAboleth\nAboleth\n10.000\nLarge\nAberration\nNaN\nLawful Evil\n17\n7\n150 (20d10 + 40)\n...\n6\n4\nHistory +12, Perception +10\nNaN\nNaN\nNaN\nNaN\nDarkvision 120 ft.; Passive Perception 20\nDeep Speech; telepathy 120 ft.\nAboleth\\nLarge Aberration, Lawful Evil\\nAC 17\\...\n\n\n1\nAir Elemental\nAir Elemental\n5.000\nLarge\nElemental\nNaN\nNeutral\n15\n5\n90 (12d10 + 24)\n...\n0\n-2\nNaN\nBludgeoning, Lightning, Piercing, Slashing\nNaN\nPoison, Thunder; Exhaustion, Grappled, Paralyz...\nNaN\nDarkvision 60 ft.; Passive Perception 10\nPrimordial (Auran)\nAir Elemental\\nLarge Elemental, Neutral\\nAC 15...\n\n\n2\nAnimated Armor\nAnimated Objects\n1.000\nMedium\nConstruct\nNaN\nUnaligned\n18\n2\n33 (6d8 + 6)\n...\n-4\n-5\nNaN\nNaN\nNaN\nPoison, Psychic; Charmed, Deafened, Exhaustion...\nNaN\nBlindsight 60 ft.; Passive Perception 6\nNaN\nAnimated Armor\\nMedium Construct, Unaligned\\nA...\n\n\n3\nAnimated Flying Sword\nAnimated Objects\n0.250\nSmall\nConstruct\nNaN\nUnaligned\n17\n4\n14 (4d6)\n...\n-3\n-5\nNaN\nNaN\nNaN\nPoison, Psychic; Charmed, Deafened, Exhaustion...\nNaN\nBlindsight 60 ft.; Passive Perception 7\nNaN\nAnimated Flying Sword\\nSmall Construct, Unalig...\n\n\n4\nAnimated Rug of Smothering\nAnimated Objects\n2.000\nLarge\nConstruct\nNaN\nUnaligned\n12\n4\n27 (5d10)\n...\n-4\n-5\nNaN\nNaN\nNaN\nPoison, Psychic; Charmed, Deafened, Exhaustion...\nNaN\nBlindsight 60 ft.; Passive Perception 6\nNaN\nAnimated Rug of Smothering\\nLarge Construct, U...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n325\nVenomous Snake\nAnimals\n0.125\nTiny\nBeast\nNaN\nUnaligned\n12\n2\n5 (2d4)\n...\n0\n-4\nNaN\nNaN\nNaN\nNaN\nNaN\nBlindsight 10 ft.; Passive Perception 10\nNaN\nVenomous Snake\\nTiny Beast, Unaligned\\nAC 12 \\...\n\n\n326\nVulture\nAnimals\n0.000\nMedium\nBeast\nNaN\nUnaligned\n10\n0\n5 (1d8 + 1)\n...\n1\n-3\nPerception +3\nNaN\nNaN\nNaN\nNaN\nPassive Perception 13\nNaN\nVulture\\nMedium Beast, Unaligned\\nAC 10 \\t\\t ...\n\n\n327\nWarhorse\nAnimals\n0.500\nLarge\nBeast\nNaN\nUnaligned\n11\n1\n19 (3d10 + 3)\n...\n3\n-2\nNaN\nNaN\nNaN\nNaN\nNaN\nPassive Perception 11\nNaN\nWarhorse\\nLarge Beast, Unaligned\\nAC 11 \\t\\t ...\n\n\n328\nWeasel\nAnimals\n0.000\nTiny\nBeast\nNaN\nUnaligned\n13\n3\n1 (1d4 − 1)\n...\n1\n-4\nAcrobatics +5, Perception +3, Stealth +5\nNaN\nNaN\nNaN\nNaN\nDarkvision 60 ft.; Passive Perception 13\nNaN\nWeasel\\nTiny Beast, Unaligned\\nAC 13 \\t\\t ...\n\n\n329\nWolf\nAnimals\n0.250\nMedium\nBeast\nNaN\nUnaligned\n12\n2\n11 (2d8 + 2)\n...\n1\n-2\nPerception +5, Stealth +4\nNaN\nNaN\nNaN\nNaN\nDarkvision 60 ft.; Passive Perception 15\nNaN\nWolf\\nMedium Beast, Unaligned\\nAC 12 \\t\\t ...\n\n\n\n\n330 rows × 33 columns\n\n\n\n\nmonsters.columns\n\nIndex(['name', 'category', 'cr', 'size', 'type', 'descriptive_tags',\n       'alignment', 'ac', 'initiative', 'hp', 'hp_number', 'speed',\n       'speed_base_number', 'str', 'dex', 'con', 'int', 'wis', 'cha',\n       'str_save', 'dex_save', 'con_save', 'int_save', 'wis_save', 'cha_save',\n       'skills', 'resistances', 'vulnerabilities', 'immunities', 'gear',\n       'senses', 'languages', 'full_text'],\n      dtype='object')\n\n\n\nmonsters.describe()\n\n\n\n\n\n\n\n\ncr\nac\ninitiative\nhp_number\nspeed_base_number\nstr\ndex\ncon\nint\nwis\ncha\nstr_save\ndex_save\ncon_save\nint_save\nwis_save\ncha_save\n\n\n\n\ncount\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n330.000000\n\n\nmean\n4.551136\n14.287879\n3.148485\n86.669697\n30.878788\n15.384848\n12.833333\n15.178788\n7.863636\n11.815152\n9.918182\n2.675758\n2.118182\n2.784848\n-1.093939\n1.872727\n0.003030\n\n\nstd\n5.797444\n3.149589\n3.944803\n102.140570\n12.339566\n6.520047\n3.261563\n4.404492\n5.675860\n2.966748\n5.969220\n3.532010\n2.452213\n2.869886\n3.224190\n2.967224\n3.524554\n\n\nmin\n0.000000\n5.000000\n-5.000000\n1.000000\n5.000000\n1.000000\n1.000000\n8.000000\n1.000000\n3.000000\n1.000000\n-5.000000\n-5.000000\n-1.000000\n-5.000000\n-4.000000\n-5.000000\n\n\n25%\n0.500000\n12.000000\n1.000000\n18.250000\n30.000000\n11.000000\n10.000000\n12.000000\n2.000000\n10.000000\n5.000000\n0.000000\n1.000000\n1.000000\n-4.000000\n0.000000\n-3.000000\n\n\n50%\n2.000000\n14.000000\n2.000000\n52.000000\n30.000000\n16.000000\n13.000000\n14.500000\n7.000000\n12.000000\n8.000000\n3.000000\n2.000000\n2.000000\n-2.000000\n1.000000\n-1.000000\n\n\n75%\n6.000000\n17.000000\n4.000000\n119.000000\n40.000000\n19.000000\n15.000000\n17.000000\n12.000000\n13.000000\n14.000000\n4.000000\n3.000000\n4.000000\n1.000000\n3.000000\n2.000000\n\n\nmax\n30.000000\n25.000000\n20.000000\n697.000000\n60.000000\n30.000000\n28.000000\n30.000000\n25.000000\n25.000000\n30.000000\n17.000000\n10.000000\n15.000000\n12.000000\n12.000000\n12.000000\n\n\n\n\n\n\n\n\nmonsters.groupby([\"senses\"]).size().sort_values(ascending=False)\n\nsenses\nDarkvision 60 ft.; Passive Perception 10                         24\nPassive Perception 10                                            23\nDarkvision 60 ft.; Passive Perception 14                         15\nDarkvision 60 ft.; Passive Perception 13                         15\nDarkvision 60 ft.; Passive Perception 15                         14\n                                                                 ..\nDarkvision 30 ft.; Passive Perception 11                          1\nDarkvision 30 ft.; Passive Perception 13                          1\nDarkvision 30 ft.; Passive Perception 9                           1\nDarkvision 60 ft., Tremorsense 120 ft.; Passive Perception 16     1\nTruesight 60 ft.; Passive Perception 19                           1\nLength: 100, dtype: int64\n\n\n\nmonsters.groupby([\"size\"]).describe()[\"cr\"] \n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nsize\n\n\n\n\n\n\n\n\n\n\n\n\nGargantuan\n15.0\n21.066667\n4.366539\n11.000\n20.0000\n22.00\n23.000\n30.0\n\n\nHuge\n34.0\n9.264706\n5.088959\n2.000\n5.0000\n8.50\n13.750\n19.0\n\n\nLarge\n107.0\n5.003505\n4.626958\n0.125\n1.5000\n4.00\n7.500\n21.0\n\n\nMedium\n90.0\n2.183333\n3.178238\n0.000\n0.2500\n1.00\n3.000\n21.0\n\n\nMedium or Small\n36.0\n3.524306\n3.635124\n0.000\n0.8750\n3.00\n5.000\n15.0\n\n\nSmall\n23.0\n0.271739\n0.254650\n0.000\n0.0625\n0.25\n0.500\n1.0\n\n\nTiny\n25.0\n0.235000\n0.491225\n0.000\n0.0000\n0.00\n0.125\n2.0\n\n\n\n\n\n\n\n\nsns.heatmap(monsters.select_dtypes(include='number').corr())\n\n\n\n\n\n\n\n\n\nmonsters['senses'].values[:10]\n\narray(['Darkvision 120 ft.; Passive Perception 20',\n       'Darkvision 60 ft.; Passive Perception 10',\n       'Blindsight 60 ft.; Passive Perception 6',\n       'Blindsight 60 ft.; Passive Perception 7',\n       'Blindsight 60 ft.; Passive Perception 6',\n       'Darkvision 60 ft., Tremorsense 60 ft.; Passive Perception 11',\n       'Passive Perception 16', 'Passive Perception 10',\n       'Passive Perception 10', 'Passive Perception 10'], dtype=object)\n\n\n\nmonsters[monsters['senses'].astype(str).str.contains('unimpeded', case=False, na=False)]\n\n\n\n\n\n\n\n\nname\ncategory\ncr\nsize\ntype\ndescriptive_tags\nalignment\nac\ninitiative\nhp\n...\nwis_save\ncha_save\nskills\nresistances\nvulnerabilities\nimmunities\ngear\nsenses\nlanguages\nfull_text\n\n\n\n\n14\nBarbed Devil\nBarbed Devil\n5.0\nMedium\nFiend\nDevil\nLawful Evil\n15\n3\n110 (13d8 + 52)\n...\n5\n5\nDeception +5, Insight +5, Perception +8\nCold\nNaN\nFire, Poison; Poisoned\nNaN\nDarkvision 120 ft. (unimpeded by magical Darkn...\nInfernal; telepathy 120 ft.\nBarbed Devil\\nMedium Fiend (Devil), Lawful Evi...\n\n\n16\nBearded Devil\nBearded Devil\n3.0\nMedium\nFiend\nDevil\nLawful Evil\n13\n2\n58 (9d8 + 18)\n...\n0\n4\nNaN\nCold\nNaN\nFire, Poison; Frightened, Poisoned\nNaN\nDarkvision 120 ft. (unimpeded by magical Darkn...\nInfernal; telepathy 120 ft.\nBearded Devil\\nMedium Fiend (Devil), Lawful Ev...\n\n\n29\nBone Devil\nBone Devil\n9.0\nLarge\nFiend\nDevil\nLawful Evil\n16\n7\n161 (17d10 + 68)\n...\n6\n7\nDeception +7, Insight +6\nCold\nNaN\nFire, Poison; Poisoned\nNaN\nDarkvision 120 ft. (unimpeded by magical Darkn...\nInfernal; telepathy 120 ft.\nBone Devil\\nLarge Fiend (Devil), Lawful Evil\\n...\n\n\n42\nChain Devil\nChain Devil\n8.0\nMedium\nFiend\nDevil\nLawful Evil\n15\n5\n85 (10d8 + 40)\n...\n4\n2\nNaN\nBludgeoning, Cold, Piercing, Slashing\nNaN\nFire, Poison; Poisoned\nNaN\nDarkvision 120 ft. (unimpeded by magical Darkn...\nInfernal; telepathy 120 ft.\nChain Devil\\nMedium Fiend (Devil), Lawful Evil...\n\n\n117\nHorned Devil\nHorned Devil\n11.0\nLarge\nFiend\nDevil\nLawful Evil\n18\n7\n199 (19d10 + 95)\n...\n7\n8\nNaN\nCold\nNaN\nFire, Poison; Poisoned\nNaN\nDarkvision 150 ft. (unimpeded by magical Darkn...\nInfernal; telepathy 120 ft.\nHorned Devil\\nLarge Fiend (Devil), Lawful Evil...\n\n\n120\nImp\nImp\n1.0\nTiny\nFiend\nDevil\nLawful Evil\n13\n3\n21 (6d4 + 6)\n...\n1\n2\nDeception +4, Insight +3, Stealth +5\nCold\nNaN\nFire, Poison; Poisoned\nNaN\nDarkvision 120 ft. (unimpeded by magical Darkn...\nCommon, Infernal\nImp\\nTiny Fiend (Devil), Lawful Evil\\nAC 13 \\t...\n\n\n128\nLemure\nLemure\n0.0\nMedium\nFiend\nDevil\nLawful Evil\n9\n-3\n9 (2d8)\n...\n0\n-4\nNaN\nCold\nNaN\nFire, Poison; Charmed, Frightened, Poisoned\nNaN\nDarkvision 120 ft. (unimpeded by magical Darkn...\nUnderstands Infernal but can’t speak\nLemure\\nMedium Fiend (Devil), Lawful Evil\\nAC ...\n\n\n\n\n7 rows × 33 columns\n\n\n\n\nimport re\n\n\ndef extract_senses(sense_str):\n    result = {}\n    if not isinstance(sense_str, str):\n        return result\n\n    # Senses to extract\n    sense_names = ['darkvision', 'blindsight', 'tremorsense', 'truesight']\n    \n    # General pattern for senses with ft.\n    for match in re.findall(r'([a-zA-Z]+)\\s+(\\d+)\\s*ft*\\.?', sense_str, flags=re.IGNORECASE):\n        name, value = match\n        name = name.strip().lower()\n        if name in sense_names:\n            result[name] = int(value)\n    \n    # Passive Perception (no ft.)\n    pp_match = re.search(r'Passive Perception\\s+(\\d+)', sense_str, flags=re.IGNORECASE)\n    if pp_match:\n        result['passive perception'] = int(pp_match.group(1))\n    \n    return result\n\n\n\nsenses_df = monsters['senses'].apply(extract_senses).apply(pd.Series).fillna(0).astype(int)\nsenses_df\n#(senses_df['passive perception'] == 0).any()\n#senses_df[(senses_df == 0).sum(axis=1) == 2]\n#senses_df[senses_df[\"tremorsense\"] &gt; 0]\n\n\n\n\n\n\n\n\ndarkvision\npassive perception\nblindsight\ntremorsense\ntruesight\n\n\n\n\n0\n120\n20\n0\n0\n0\n\n\n1\n60\n10\n0\n0\n0\n\n\n2\n0\n6\n60\n0\n0\n\n\n3\n0\n7\n60\n0\n0\n\n\n4\n0\n6\n60\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n325\n0\n10\n10\n0\n0\n\n\n326\n0\n13\n0\n0\n0\n\n\n327\n0\n11\n0\n0\n0\n\n\n328\n60\n13\n0\n0\n0\n\n\n329\n60\n15\n0\n0\n0\n\n\n\n\n330 rows × 5 columns\n\n\n\n\nsenses_df_filtered = senses_df[(senses_df == 0).sum(axis=1) != 2]\nsenses_df_filtered\n\n\n\n\n\n\n\n\ndarkvision\npassive perception\nblindsight\ntremorsense\ntruesight\n\n\n\n\n0\n120\n20\n0\n0\n0\n\n\n1\n60\n10\n0\n0\n0\n\n\n2\n0\n6\n60\n0\n0\n\n\n3\n0\n7\n60\n0\n0\n\n\n4\n0\n6\n60\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n325\n0\n10\n10\n0\n0\n\n\n326\n0\n13\n0\n0\n0\n\n\n327\n0\n11\n0\n0\n0\n\n\n328\n60\n13\n0\n0\n0\n\n\n329\n60\n15\n0\n0\n0\n\n\n\n\n279 rows × 5 columns\n\n\n\n\nmonsters_mod = pd.concat([monsters, senses_df], axis=1)\n\n\nmonsters_mod = monsters_mod[[\"size\"]+list(monsters_mod.columns[-5:])]\n\n\nimport seaborn as sns\n\n\nlong_df = pd.melt(\n    senses_df, #filtered\n    id_vars=['passive perception'], #,'size'],             \n    value_vars=['darkvision', 'blindsight', 'tremorsense', 'truesight'], \n    var_name='sense_type',\n    value_name='distance'\n)\nlong_df #[(long_df[\"sense_type\"] == \"tremorsense\") & (long_df[\"distance\"] != 0)]\n\n\n\n\n\n\n\n\npassive perception\nsense_type\ndistance\n\n\n\n\n0\n20\ndarkvision\n120\n\n\n1\n10\ndarkvision\n60\n\n\n2\n6\ndarkvision\n0\n\n\n3\n7\ndarkvision\n0\n\n\n4\n6\ndarkvision\n0\n\n\n...\n...\n...\n...\n\n\n1315\n10\ntruesight\n0\n\n\n1316\n13\ntruesight\n0\n\n\n1317\n11\ntruesight\n0\n\n\n1318\n13\ntruesight\n0\n\n\n1319\n15\ntruesight\n0\n\n\n\n\n1320 rows × 3 columns\n\n\n\n\n\n\nCases with only passive perception are not there. If only long_df is used then there are lot many extra points.\nTwo points for cases with more than two senses.\n\n\nsns.set_theme(style=\"dark\", font=\"Comic Sans MS\")\ncurrent_style = sns.axes_style()\ncolors = [\"grey\", \"orange\", \"dodgerblue\", \"salmon\"]\nplot1 = sns.catplot(data=long_df[long_df[\"distance\"] != 0], x=\"distance\", y=\"passive perception\", hue=\"sense_type\",\\\n            kind=\"strip\", dodge=True, height=4, aspect=2, size=4, native_scale=True,\\\n                   jitter=0.25, palette=colors)\nplot1._legend.remove()\nplot1.add_legend(title='', ncol=4, bbox_to_anchor=(0.5, 1.05))\nfor ind, text in enumerate(plot1._legend.texts):\n    text.set_color(colors[ind]) \nfor handle in plot1._legend.legend_handles:\n    handle.set_visible(False)\nplot1.fig.set_facecolor(current_style['axes.facecolor'])\nsns.despine(left=True, bottom=True, right=True, top=True)\nfor ax in plot1.axes.flat:\n    ax.grid(axis='y', which='major')\n\nplt.xticks(ticks=range(0,151,30))\nplt.xlabel(\"Distance (feet)\")\n#plt.savefig(\"senses.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n#plot1 = sns.catplot(data=long_df, x=\"distance\", y=\"passive perception\", hue=\"size\",\\\n#            kind=\"strip\", dodge=True, height=4, aspect=2, size=4, native_scale=True,\\\n#                   jitter=1, col=\"sense_type\", col_wrap=2)\n#plot1._legend.remove()\n#plot1.add_legend(title='', ncol=7, bbox_to_anchor=(0.5, 1.05))\n#plt.xticks(ticks=range(0,151,30))\n#plt.xlabel(\"Distance (feet)\")\n#plt.tight_layout()\n#plt.show()\n\n\nsns.dark_palette(\"xkcd:golden\", 4)\n\n\n\n\n\n\n\n\nfrom upsetplot import plot\nfrom upsetplot import UpSet\nimport textwrap\nsns.reset_defaults()\n\n\nbinary_df = senses_df &gt; 0\ncounts = binary_df.value_counts().sort_values(ascending=False)\n\n\nupset = UpSet(counts, sort_by= \"cardinality\", show_percentages=True, facecolor=\"dodgerblue\")\nupset.style_subsets(present=\"truesight\", edgecolor=\"lightgreen\", linewidth=1)\nupset.style_subsets(present=\"tremorsense\", edgecolor=\"salmon\", linewidth=1)\n\n#upset.add_catplot(value=\"progression\", kind=\"strip\", color=\"blue\")\nupset.plot()\nfor ind, ax in enumerate(plt.gcf().get_axes()):\n    if(ind == 3):\n        ax.set_yticks(range(0,151,50))\n        ax.set_facecolor(\"whitesmoke\")\n        ax.yaxis.grid(True, color=\"#D0D0D0\")\n    if(ind == 2):\n        ax.xaxis.grid(True, color=\"#D0D0D0\")\n    ax.spines['left'].set_visible(False)\n    ax.tick_params(axis='both', length=0)\n\n    for text in ax.texts:\n        if \"%\" in text.get_text():  \n            text.set_fontsize(9)\n#                text.set_fontfamily(\"Consolas\")\ntitle_text = \"Frequency of five senses and their combinations across all the characters in Dungeons and Dragons Monsters (2024)\"\nplt.suptitle(\"\\n\".join(textwrap.wrap(title_text, width=20)), x=0.075, y=0.8, ha=\"left\", fontfamily=\"Serif\")\nplt.savefig(\"senses_comb.png\", dpi=300, facecolor=\"whitesmoke\", bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "posts/Dungeons_Dragons/DD_monsters.html#upset-plot",
    "href": "posts/Dungeons_Dragons/DD_monsters.html#upset-plot",
    "title": "Dungeons and Dragons Monsters (2024)",
    "section": "",
    "text": "from upsetplot import plot\nfrom upsetplot import UpSet\nimport textwrap\nsns.reset_defaults()\n\n\nbinary_df = senses_df &gt; 0\ncounts = binary_df.value_counts().sort_values(ascending=False)\n\n\nupset = UpSet(counts, sort_by= \"cardinality\", show_percentages=True, facecolor=\"dodgerblue\")\nupset.style_subsets(present=\"truesight\", edgecolor=\"lightgreen\", linewidth=1)\nupset.style_subsets(present=\"tremorsense\", edgecolor=\"salmon\", linewidth=1)\n\n#upset.add_catplot(value=\"progression\", kind=\"strip\", color=\"blue\")\nupset.plot()\nfor ind, ax in enumerate(plt.gcf().get_axes()):\n    if(ind == 3):\n        ax.set_yticks(range(0,151,50))\n        ax.set_facecolor(\"whitesmoke\")\n        ax.yaxis.grid(True, color=\"#D0D0D0\")\n    if(ind == 2):\n        ax.xaxis.grid(True, color=\"#D0D0D0\")\n    ax.spines['left'].set_visible(False)\n    ax.tick_params(axis='both', length=0)\n\n    for text in ax.texts:\n        if \"%\" in text.get_text():  \n            text.set_fontsize(9)\n#                text.set_fontfamily(\"Consolas\")\ntitle_text = \"Frequency of five senses and their combinations across all the characters in Dungeons and Dragons Monsters (2024)\"\nplt.suptitle(\"\\n\".join(textwrap.wrap(title_text, width=20)), x=0.075, y=0.8, ha=\"left\", fontfamily=\"Serif\")\nplt.savefig(\"senses_comb.png\", dpi=300, facecolor=\"whitesmoke\", bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "posts/API_gurus/API_gurus.html",
    "href": "posts/API_gurus/API_gurus.html",
    "title": "Web API dataset",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggtext)\n\n\napi_categories &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_categories.csv')\napi_info &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_info.csv')\napi_logos &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_logos.csv')\napi_origins &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_origins.csv')\napisguru_apis &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/apisguru_apis.csv')\n\n\nglimpse(api_logos)\n\nRows: 2,529\nColumns: 4\n$ name             &lt;chr&gt; \"1forge.com\", \"1password.com:events\", \"1password.loca…\n$ background_color &lt;chr&gt; \"#24292e\", NA, NA, \"#F26641\", NA, NA, NA, NA, NA, NA,…\n$ url              &lt;chr&gt; \"https://api.apis.guru/v2/cache/logo/https_1forge.com…\n$ alt_text         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\n\nnormalize_to_hex &lt;- function(color_str) {\n  if (is.na(color_str)) return(NA_character_)\n  \n  color_str &lt;- trimws(tolower(color_str))  # Normalize spacing and case\n  \n  if (grepl(\"^rgb\\\\(\", color_str)) {\n    nums &lt;- as.numeric(unlist(regmatches(color_str, gregexpr(\"\\\\d+\", color_str))))\n    if (length(nums) &lt; 3) return(NA_character_)\n    return(rgb(nums[1], nums[2], nums[3], maxColorValue = 255))\n  }\n  \n  # If it's already hex return it as-is\n  if (grepl(\"^#[0-9a-f]{6}$\", color_str)) {\n    return(toupper(color_str))  \n  }\n  \n  # Handle named colors or invalid inputs\n  tryCatch({\n    return(rgb(col2rgb(color_str)[, 1], maxColorValue = 255))\n  }, error = function(e) {\n    return(NA_character_)\n  })\n}\n\napi_logos &lt;- api_logos %&gt;% \n  mutate(hex_color = map_chr(background_color, normalize_to_hex))\n\nFunction for arranging colors by brightness.\n\nhex_brightness &lt;- function(hex) {\n  rgb &lt;- col2rgb(hex)\n  # Use relative luminance formula\n  0.299 * rgb[1, ] + 0.587 * rgb[2, ] + 0.114 * rgb[3, ]\n}\n\n\napi_logos_mod &lt;- api_logos %&gt;%\n  filter(!is.na(hex_color)) %&gt;%\n  select(name, hex_color) %&gt;%\n  group_by(hex_color) %&gt;%\n  summarize(n = n()) %&gt;% \n  arrange(hex_color) %&gt;% \n  filter(!grepl(\"transparent\", hex_color, ignore.case=T)) %&gt;% \n  mutate(brightness = hex_brightness(hex_color),\n         text_color = ifelse(brightness &gt; 128, \"black\", \"white\")) %&gt;%\n  arrange(desc(brightness))"
  },
  {
    "objectID": "posts/API_gurus/API_gurus.html#tidytuesday-data-for-2025-06-17",
    "href": "posts/API_gurus/API_gurus.html#tidytuesday-data-for-2025-06-17",
    "title": "Web API dataset",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggtext)\n\n\napi_categories &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_categories.csv')\napi_info &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_info.csv')\napi_logos &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_logos.csv')\napi_origins &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_origins.csv')\napisguru_apis &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/apisguru_apis.csv')\n\n\nglimpse(api_logos)\n\nRows: 2,529\nColumns: 4\n$ name             &lt;chr&gt; \"1forge.com\", \"1password.com:events\", \"1password.loca…\n$ background_color &lt;chr&gt; \"#24292e\", NA, NA, \"#F26641\", NA, NA, NA, NA, NA, NA,…\n$ url              &lt;chr&gt; \"https://api.apis.guru/v2/cache/logo/https_1forge.com…\n$ alt_text         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\n\nnormalize_to_hex &lt;- function(color_str) {\n  if (is.na(color_str)) return(NA_character_)\n  \n  color_str &lt;- trimws(tolower(color_str))  # Normalize spacing and case\n  \n  if (grepl(\"^rgb\\\\(\", color_str)) {\n    nums &lt;- as.numeric(unlist(regmatches(color_str, gregexpr(\"\\\\d+\", color_str))))\n    if (length(nums) &lt; 3) return(NA_character_)\n    return(rgb(nums[1], nums[2], nums[3], maxColorValue = 255))\n  }\n  \n  # If it's already hex return it as-is\n  if (grepl(\"^#[0-9a-f]{6}$\", color_str)) {\n    return(toupper(color_str))  \n  }\n  \n  # Handle named colors or invalid inputs\n  tryCatch({\n    return(rgb(col2rgb(color_str)[, 1], maxColorValue = 255))\n  }, error = function(e) {\n    return(NA_character_)\n  })\n}\n\napi_logos &lt;- api_logos %&gt;% \n  mutate(hex_color = map_chr(background_color, normalize_to_hex))\n\nFunction for arranging colors by brightness.\n\nhex_brightness &lt;- function(hex) {\n  rgb &lt;- col2rgb(hex)\n  # Use relative luminance formula\n  0.299 * rgb[1, ] + 0.587 * rgb[2, ] + 0.114 * rgb[3, ]\n}\n\n\napi_logos_mod &lt;- api_logos %&gt;%\n  filter(!is.na(hex_color)) %&gt;%\n  select(name, hex_color) %&gt;%\n  group_by(hex_color) %&gt;%\n  summarize(n = n()) %&gt;% \n  arrange(hex_color) %&gt;% \n  filter(!grepl(\"transparent\", hex_color, ignore.case=T)) %&gt;% \n  mutate(brightness = hex_brightness(hex_color),\n         text_color = ifelse(brightness &gt; 128, \"black\", \"white\")) %&gt;%\n  arrange(desc(brightness))"
  },
  {
    "objectID": "posts/API_gurus/API_gurus.html#plotting",
    "href": "posts/API_gurus/API_gurus.html#plotting",
    "title": "Web API dataset",
    "section": "Plotting",
    "text": "Plotting\n\ntitle &lt;- \"&lt;span style='color:white;'&gt;White&lt;/span&gt; is the preferred background &lt;br&gt;color in the API logos. Only one &lt;br&gt;logo has a transparent background.\"\n\napi_logos_mod %&gt;% \nggplot(aes(y = factor(seq_len(nrow(api_logos_mod))),x=1, fill = hex_color)) +\n  geom_tile(aes(width=log(n+1))) +\n  geom_text(aes(label = hex_color, color = text_color), size = 3, family=\"Consolas\") +\n  geom_text(aes(label = n, color = text_color), size = 3, nudge_x = 0.5, family=\"Consolas\") +\n  scale_fill_identity() +\n  scale_color_identity() +\n  theme_void() +\n#  labs(title = \"Background color for the API logos\") +\n  geom_richtext(x = -2.15, y = 20, label = title, size = 4, family=\"Serif\", hjust=0, fill=NA, label.color=NA) +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank(),\n        panel.background = element_rect(fill = \"darkgrey\"))\n\n\n\n\n\n\n\n#ggsave(\"API_logos.png\", dpi=300)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Visualization of various datasets using Python and R",
    "section": "",
    "text": "Scottish Munro Classification\n\n\n\nHistogram\n\nFacet\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nVariations in Munro and Munro Top assignment over the years.\n\n\n\n\n\nAug 19, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nExtreme Weather Attribution Studies\n\n\n\nsunburst\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nClassification of effect of climate change on different extreme events.\n\n\n\n\n\nAug 12, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nIncome Inequality Before and After Taxes\n\n\n\nParallel coordinates\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nGini coefficient for different countries before and after taxes.\n\n\n\n\n\nAug 5, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nNetflix viewing data\n\n\n\nWordcloud\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nWord cloud for movie and show titles.\n\n\n\n\n\nJul 29, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nMTA Permanent Art Catalog\n\n\n\nTimeline\n\nstem plot\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nEmergence of the different art materials over time.\n\n\n\n\n\nJul 22, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nBritish Library Funding\n\n\n\nTimeline\n\nno-code\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nDecline in the UK govenment funding to the British Library.\n\n\n\n\n\nJul 15, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nThe xkcd color survey\n\n\n\nsklearn\n\ncolors\n\nTidyTuesday\n\nPydyTuesday\n\n\n\nIdentification of colors by the participants.\n\n\n\n\n\nJul 8, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nGas prices in the US\n\n\n\nTimeseries\n\nTidyTuesday\n\nPydyTuesday\n\n\n\nTimeline of gasoline and diesel prices.\n\n\n\n\n\nJul 1, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nMeasles cases across the world\n\n\n\nTimeline\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nTimeline for the global Measles and Rubella cases.\n\n\n\n\n\nJun 24, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nWeb API dataset\n\n\n\nggplot2\n\ncolors\n\nTidyTuesday\n\n\n\nBackground color frequency in the API logos.\n\n\n\n\n\nJun 17, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nThe US Judges data\n\n\n\nPillow\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nGender and racial diversity among the judges.\n\n\n\n\n\nJun 10, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nProject Gutenberg\n\n\n\nggplot2\n\nwordcloud\n\nTidyTuesday\n\nmagick\n\n\n\nFrequency of books in different languages.\n\n\n\n\n\nJun 3, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nDungeons and Dragons Monsters (2024)\n\n\n\nUpset\n\nseaborn\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nFrequency of five senses and their combinations across all the characters.\n\n\n\n\n\nMay 27, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nWater Quality at Sydney Beaches\n\n\n\nggplot2\n\nheatmap\n\nTidyTuesday\n\n\n\nRanking of Sydney beaches based on Enterococci concentration\n\n\n\n\n\nMay 20, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nSeismic events at Mount Vesuvius\n\n\n\nplotly\n\npolar\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nFrequency of earthquakes at different times of the day.\n\n\n\n\n\nMay 13, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nNSF funding cut in April 2025\n\n\n\nsunburst\n\nplotly\n\nNSF\n\nPyDyTuesday\n\nTidyTuesday\n\n\n\nDistribution of funding cuts across directorates and divisions.\n\n\n\n\n\nMay 7, 2025\n\n\nManish Datt\n\n\n\n\n\n\n\n\n\n\n\n\nuseR2025 conference data analysis\n\n\n\nggplot2\n\nword count\n\nTidyTuesday\n\n\n\nTop keywords in useR 2025 talks\n\n\n\n\n\nMay 1, 2025\n\n\nManish Datt\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/BL_funding/BL_funding.html",
    "href": "posts/BL_funding/BL_funding.html",
    "title": "British Library Funding",
    "section": "",
    "text": "import pandas as pd\nbl_funding = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-15/bl_funding.csv')\nbl_funding\n\n\n\n\n\n\n\n\nyear\nnominal_gbp_millions\ngia_gbp_millions\nvoluntary_gbp_millions\ninvestment_gbp_millions\nservices_gbp_millions\nother_gbp_millions\nyear_2000_gbp_millions\ninflation_adjustment\ntotal_y2000_gbp_millions\npercentage_of_y2000_income\ngia_y2000_gbp_millions\nvoluntary_y2000_gbp_millions\ninvestment_y2000_gbp_millions\nservices_y2000_gbp_millions\nother_y2000_gbp_millions\ngia_as_percent_of_peak_gia\n\n\n\n\n0\n2023\n151.800\n127.800\n8.200\n3.000\n12.800\n0.000\nNaN\n1818796.76\n83.461772\n0.757366\n70.266235\n4.508475\n1.649442\n7.037620\n0.000000\n0.743019\n\n\n1\n2022\n147.600\n116.500\n14.900\n0.800\n15.600\n0.000\nNaN\n1674338.62\n88.154211\n0.799947\n69.579713\n8.899036\n0.477801\n9.317112\n0.000000\n0.735759\n\n\n2\n2021\n141.400\n110.412\n20.086\n0.080\n9.753\n1.042\nNaN\n1534702.63\n92.135113\n0.836072\n71.943579\n13.087877\n0.052127\n6.354977\n0.678959\n0.760756\n\n\n3\n2020\n125.900\n108.226\n9.524\n0.196\n7.584\n0.383\nNaN\n1495983.26\n84.158696\n0.763691\n72.344392\n6.366381\n0.131018\n5.069575\n0.256019\n0.764994\n\n\n4\n2019\n121.100\n96.899\n8.626\n0.426\n13.249\n1.925\nNaN\n1483364.96\n81.638709\n0.740823\n65.323776\n5.815157\n0.287185\n8.931720\n1.297725\n0.690756\n\n\n5\n2018\n122.200\n93.443\n10.754\n0.332\n14.558\n3.116\nNaN\n1457265.04\n83.855714\n0.760941\n64.122172\n7.379577\n0.227824\n9.989947\n2.138252\n0.678049\n\n\n6\n2017\n120.800\n93.443\n9.634\n0.269\n16.152\n1.328\nNaN\n1422026.43\n84.949195\n0.770864\n65.711156\n6.774839\n0.189167\n11.358439\n0.933879\n0.694852\n\n\n7\n2016\n118.000\n93.911\n9.672\n0.649\n13.780\n0.000\n76.39\n1384868.58\n85.206641\n0.773200\n67.812211\n6.984056\n0.468637\n9.950403\n0.000000\n0.717069\n\n\n8\n2015\n117.800\n93.043\n9.919\n1.003\n13.799\n0.000\n77.59\n1375792.94\n85.623350\n0.776981\n67.628636\n7.209660\n0.729034\n10.029852\n0.000000\n0.715128\n\n\n9\n2014\n118.900\n93.893\n8.931\n0.707\n15.352\n0.000\n79.09\n1375241.48\n86.457543\n0.784551\n68.273828\n6.494132\n0.514092\n11.163130\n0.000000\n0.721950\n\n\n10\n2013\n124.700\n95.106\n14.993\n0.623\n13.980\n0.000\n84.90\n1355438.10\n91.999775\n0.834844\n70.166244\n11.061368\n0.459630\n10.314008\n0.000000\n0.741961\n\n\n11\n2012\n126.100\n103.868\n6.265\n0.620\n15.349\n0.000\n88.46\n1321543.18\n95.418751\n0.865869\n78.595994\n4.740670\n0.469148\n11.614452\n0.000000\n0.831100\n\n\n12\n2011\n140.100\n101.873\n6.738\n0.531\n17.619\n13.339\n101.44\n1285194.76\n109.010715\n0.989208\n79.266585\n5.242785\n0.413167\n13.709206\n10.378972\n0.838192\n\n\n13\n2010\n137.900\n105.847\n11.774\n0.415\n19.906\n0.000\n105.05\n1230278.88\n112.088407\n1.017136\n86.034965\n9.570188\n0.337322\n16.180071\n0.000000\n0.909763\n\n\n14\n2009\n142.200\n109.464\n9.616\n0.418\n22.674\n0.000\n113.32\n1190997.74\n119.395693\n1.083445\n91.909494\n8.073903\n0.350966\n19.037820\n0.000000\n0.971882\n\n\n15\n2008\n140.500\n106.947\n7.993\n1.492\n24.030\n0.000\n111.37\n1165755.40\n120.522710\n1.093673\n91.740514\n6.856498\n1.279857\n20.613244\n0.000000\n0.970095\n\n\n16\n2007\n141.200\n106.411\n9.789\n1.878\n23.076\n0.000\n116.39\n1125225.72\n125.485934\n1.138711\n94.568581\n8.699588\n1.668998\n20.507885\n0.000000\n1.000000\n\n\n17\n2006\n159.200\n102.639\n31.879\n1.904\n22.768\n0.000\n136.85\n1099681.85\n144.769144\n1.313695\n93.335177\n28.989294\n1.731410\n20.704170\n0.000000\n0.986958\n\n\n18\n2005\n136.900\n97.562\n10.663\n1.770\n24.309\n2.602\n121.44\n1074656.20\n127.389578\n1.155985\n90.784383\n9.922243\n1.647038\n22.620258\n2.421239\n0.959985\n\n\n19\n2004\n121.600\n88.501\n6.399\n1.586\n24.520\n0.000\n110.92\n1052996.65\n115.479950\n1.047912\n84.046801\n6.076942\n1.506178\n23.285924\n0.000000\n0.888739\n\n\n20\n2003\n119.500\n89.263\n4.004\n0.856\n25.423\n0.000\n112.25\n1039036.95\n115.010347\n1.043651\n85.909361\n3.853568\n0.823840\n24.467850\n0.000000\n0.908434\n\n\n21\n2002\n119.200\n85.187\n6.162\n0.795\n27.068\n0.000\n115.20\n1025079.54\n116.283659\n1.055206\n83.102820\n6.011241\n0.775550\n26.405756\n0.000000\n0.878757\n\n\n22\n2001\n120.900\n88.617\n3.283\n0.710\n28.290\n0.000\n118.80\n1012335.12\n119.426855\n1.083728\n87.537218\n3.242997\n0.701349\n27.945291\n0.000000\n0.925648\n\n\n23\n2000\n110.200\n82.266\n2.852\n0.654\n24.491\n0.000\n110.20\n1000000.00\n110.200000\n1.000000\n82.266000\n2.852000\n0.654000\n24.491000\n0.000000\n0.869908\n\n\n24\n1999\n112.300\n78.465\n4.944\n0.455\n28.468\n0.000\n115.62\n992098.37\n113.194420\n1.027173\n79.089939\n4.983377\n0.458624\n28.694735\n0.000000\n0.836324\n\n\n25\n1998\n120.459\n80.450\n8.487\n0.474\n31.048\n0.000\nNaN\n979089.11\n123.031702\n1.116440\n82.168210\n8.668261\n0.484123\n31.711107\n0.000000\n0.868874\n\n\n\n\n\n\n\n\n\n\n\n\nNo-code plot\n\n\nUse the code below to generate the above plot with Deepnote notebook.\n    _dntk.DeepnoteChart(bl_funding, spec_dict={\n  \"layer\": [\n    {\n      \"layer\": [\n        {\n          \"layer\": [\n            {\n              \"mark\": {\n                \"clip\": True,\n                \"type\": \"circle\",\n                \"color\": \"#CC8BFF\",\n                \"tooltip\": True\n              },\n              \"encoding\": {\n                \"x\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": None,\n                    \"format\": \".0f\"\n                  },\n                  \"sort\": None,\n                  \"type\": \"quantitative\",\n                  \"field\": \"year\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"number\",\n                    \"decimals\": 0\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"y\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": \"GBP (millions)\",\n                    \"format\": {\n                      \"type\": \"default\",\n                      \"decimals\": None\n                    },\n                    \"formatType\": \"numberFormatFromNumberType\"\n                  },\n                  \"type\": \"quantitative\",\n                  \"field\": \"gia_y2000_gbp_millions\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"default\",\n                    \"decimals\": None\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"color\": {\n                  \"type\": \"nominal\",\n                  \"datum\": \"UK government (adjusted)\",\n                  \"scale\": {\n                    \"range\": [\n                      \"#CC8BFF\"\n                    ],\n                    \"domain\": [\n                      \"UK government (adjusted)\"\n                    ]\n                  }\n                }\n              },\n              \"transform\": []\n            }\n          ]\n        },\n        {\n          \"layer\": [\n            {\n              \"mark\": {\n                \"clip\": True,\n                \"type\": \"circle\",\n                \"color\": \"#8018D3\",\n                \"tooltip\": True\n              },\n              \"encoding\": {\n                \"x\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": None,\n                    \"format\": \".0f\"\n                  },\n                  \"sort\": None,\n                  \"type\": \"quantitative\",\n                  \"field\": \"year\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"number\",\n                    \"decimals\": 0\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"y\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": \"GBP (millions)\",\n                    \"format\": {\n                      \"type\": \"default\",\n                      \"decimals\": None\n                    },\n                    \"formatType\": \"numberFormatFromNumberType\"\n                  },\n                  \"type\": \"quantitative\",\n                  \"field\": \"total_y2000_gbp_millions\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"default\",\n                    \"decimals\": None\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"color\": {\n                  \"type\": \"nominal\",\n                  \"datum\": \"Total funding (adjusted)\",\n                  \"scale\": {\n                    \"range\": [\n                      \"#8018D3\"\n                    ],\n                    \"domain\": [\n                      \"Total funding (adjusted)\"\n                    ]\n                  }\n                }\n              },\n              \"transform\": []\n            }\n          ]\n        },\n        {\n          \"layer\": [\n            {\n              \"mark\": {\n                \"clip\": True,\n                \"type\": \"line\",\n                \"color\": \"#2266D3\",\n                \"tooltip\": True\n              },\n              \"encoding\": {\n                \"x\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": None,\n                    \"format\": \".0f\"\n                  },\n                  \"sort\": None,\n                  \"type\": \"quantitative\",\n                  \"field\": \"year\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"number\",\n                    \"decimals\": 0\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"y\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": \"GBP (millions)\",\n                    \"format\": {\n                      \"type\": \"default\",\n                      \"decimals\": None\n                    },\n                    \"formatType\": \"numberFormatFromNumberType\"\n                  },\n                  \"type\": \"quantitative\",\n                  \"field\": \"nominal_gbp_millions\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"default\",\n                    \"decimals\": None\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"color\": {\n                  \"type\": \"nominal\",\n                  \"datum\": \"Total funding\",\n                  \"scale\": {\n                    \"range\": [\n                      \"#2266D3\"\n                    ],\n                    \"domain\": [\n                      \"Total funding\"\n                    ]\n                  }\n                }\n              },\n              \"transform\": []\n            }\n          ]\n        },\n        {\n          \"layer\": [\n            {\n              \"mark\": {\n                \"clip\": True,\n                \"type\": \"line\",\n                \"color\": \"#83AFF6\",\n                \"tooltip\": True\n              },\n              \"encoding\": {\n                \"x\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": None,\n                    \"format\": \".0f\"\n                  },\n                  \"sort\": None,\n                  \"type\": \"quantitative\",\n                  \"field\": \"year\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"number\",\n                    \"decimals\": 0\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"y\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": \"GBP (millions)\",\n                    \"format\": {\n                      \"type\": \"default\",\n                      \"decimals\": None\n                    },\n                    \"formatType\": \"numberFormatFromNumberType\"\n                  },\n                  \"type\": \"quantitative\",\n                  \"field\": \"gia_gbp_millions\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"default\",\n                    \"decimals\": None\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"color\": {\n                  \"type\": \"nominal\",\n                  \"datum\": \"UK government\",\n                  \"scale\": {\n                    \"range\": [\n                      \"#83AFF6\"\n                    ],\n                    \"domain\": [\n                      \"UK government\"\n                    ]\n                  }\n                }\n              },\n              \"transform\": []\n            }\n          ]\n        }\n      ],\n      \"resolve\": {\n        \"scale\": {\n          \"color\": \"independent\"\n        }\n      }\n    }\n  ],\n  \"title\": \"Total funding and year 2000 adjusted funding for the British Library.\",\n  \"config\": {\n    \"legend\": {\n      \"orient\": \"top\",\n      \"disable\": False\n    }\n  },\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"encoding\": {}\n})"
  },
  {
    "objectID": "posts/BL_funding/BL_funding.html#plot-using-deepnote-notebook",
    "href": "posts/BL_funding/BL_funding.html#plot-using-deepnote-notebook",
    "title": "British Library Funding",
    "section": "",
    "text": "No-code plot\n\n\nUse the code below to generate the above plot with Deepnote notebook.\n    _dntk.DeepnoteChart(bl_funding, spec_dict={\n  \"layer\": [\n    {\n      \"layer\": [\n        {\n          \"layer\": [\n            {\n              \"mark\": {\n                \"clip\": True,\n                \"type\": \"circle\",\n                \"color\": \"#CC8BFF\",\n                \"tooltip\": True\n              },\n              \"encoding\": {\n                \"x\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": None,\n                    \"format\": \".0f\"\n                  },\n                  \"sort\": None,\n                  \"type\": \"quantitative\",\n                  \"field\": \"year\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"number\",\n                    \"decimals\": 0\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"y\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": \"GBP (millions)\",\n                    \"format\": {\n                      \"type\": \"default\",\n                      \"decimals\": None\n                    },\n                    \"formatType\": \"numberFormatFromNumberType\"\n                  },\n                  \"type\": \"quantitative\",\n                  \"field\": \"gia_y2000_gbp_millions\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"default\",\n                    \"decimals\": None\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"color\": {\n                  \"type\": \"nominal\",\n                  \"datum\": \"UK government (adjusted)\",\n                  \"scale\": {\n                    \"range\": [\n                      \"#CC8BFF\"\n                    ],\n                    \"domain\": [\n                      \"UK government (adjusted)\"\n                    ]\n                  }\n                }\n              },\n              \"transform\": []\n            }\n          ]\n        },\n        {\n          \"layer\": [\n            {\n              \"mark\": {\n                \"clip\": True,\n                \"type\": \"circle\",\n                \"color\": \"#8018D3\",\n                \"tooltip\": True\n              },\n              \"encoding\": {\n                \"x\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": None,\n                    \"format\": \".0f\"\n                  },\n                  \"sort\": None,\n                  \"type\": \"quantitative\",\n                  \"field\": \"year\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"number\",\n                    \"decimals\": 0\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"y\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": \"GBP (millions)\",\n                    \"format\": {\n                      \"type\": \"default\",\n                      \"decimals\": None\n                    },\n                    \"formatType\": \"numberFormatFromNumberType\"\n                  },\n                  \"type\": \"quantitative\",\n                  \"field\": \"total_y2000_gbp_millions\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"default\",\n                    \"decimals\": None\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"color\": {\n                  \"type\": \"nominal\",\n                  \"datum\": \"Total funding (adjusted)\",\n                  \"scale\": {\n                    \"range\": [\n                      \"#8018D3\"\n                    ],\n                    \"domain\": [\n                      \"Total funding (adjusted)\"\n                    ]\n                  }\n                }\n              },\n              \"transform\": []\n            }\n          ]\n        },\n        {\n          \"layer\": [\n            {\n              \"mark\": {\n                \"clip\": True,\n                \"type\": \"line\",\n                \"color\": \"#2266D3\",\n                \"tooltip\": True\n              },\n              \"encoding\": {\n                \"x\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": None,\n                    \"format\": \".0f\"\n                  },\n                  \"sort\": None,\n                  \"type\": \"quantitative\",\n                  \"field\": \"year\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"number\",\n                    \"decimals\": 0\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"y\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": \"GBP (millions)\",\n                    \"format\": {\n                      \"type\": \"default\",\n                      \"decimals\": None\n                    },\n                    \"formatType\": \"numberFormatFromNumberType\"\n                  },\n                  \"type\": \"quantitative\",\n                  \"field\": \"nominal_gbp_millions\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"default\",\n                    \"decimals\": None\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"color\": {\n                  \"type\": \"nominal\",\n                  \"datum\": \"Total funding\",\n                  \"scale\": {\n                    \"range\": [\n                      \"#2266D3\"\n                    ],\n                    \"domain\": [\n                      \"Total funding\"\n                    ]\n                  }\n                }\n              },\n              \"transform\": []\n            }\n          ]\n        },\n        {\n          \"layer\": [\n            {\n              \"mark\": {\n                \"clip\": True,\n                \"type\": \"line\",\n                \"color\": \"#83AFF6\",\n                \"tooltip\": True\n              },\n              \"encoding\": {\n                \"x\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": None,\n                    \"format\": \".0f\"\n                  },\n                  \"sort\": None,\n                  \"type\": \"quantitative\",\n                  \"field\": \"year\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"number\",\n                    \"decimals\": 0\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"y\": {\n                  \"axis\": {\n                    \"grid\": False,\n                    \"title\": \"GBP (millions)\",\n                    \"format\": {\n                      \"type\": \"default\",\n                      \"decimals\": None\n                    },\n                    \"formatType\": \"numberFormatFromNumberType\"\n                  },\n                  \"type\": \"quantitative\",\n                  \"field\": \"gia_gbp_millions\",\n                  \"scale\": {\n                    \"type\": \"linear\",\n                    \"zero\": False\n                  },\n                  \"format\": {\n                    \"type\": \"default\",\n                    \"decimals\": None\n                  },\n                  \"formatType\": \"numberFormatFromNumberType\"\n                },\n                \"color\": {\n                  \"type\": \"nominal\",\n                  \"datum\": \"UK government\",\n                  \"scale\": {\n                    \"range\": [\n                      \"#83AFF6\"\n                    ],\n                    \"domain\": [\n                      \"UK government\"\n                    ]\n                  }\n                }\n              },\n              \"transform\": []\n            }\n          ]\n        }\n      ],\n      \"resolve\": {\n        \"scale\": {\n          \"color\": \"independent\"\n        }\n      }\n    }\n  ],\n  \"title\": \"Total funding and year 2000 adjusted funding for the British Library.\",\n  \"config\": {\n    \"legend\": {\n      \"orient\": \"top\",\n      \"disable\": False\n    }\n  },\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n  \"encoding\": {}\n})"
  },
  {
    "objectID": "posts/Gas_prices/gas_prices.html",
    "href": "posts/Gas_prices/gas_prices.html",
    "title": "Gas prices in the US",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\n\nweekly_gas_prices = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-01/weekly_gas_prices.csv')\n\n\nweekly_gas_prices['date'] = pd.to_datetime(weekly_gas_prices['date'])\n\n\nweekly_gas_prices\n\n\n\n\n\n\n\n\ndate\nfuel\ngrade\nformulation\nprice\n\n\n\n\n0\n1990-08-20\ngasoline\nregular\nall\n1.191\n\n\n1\n1990-08-20\ngasoline\nregular\nconventional\n1.191\n\n\n2\n1990-08-27\ngasoline\nregular\nall\n1.245\n\n\n3\n1990-08-27\ngasoline\nregular\nconventional\n1.245\n\n\n4\n1990-09-03\ngasoline\nregular\nall\n1.242\n\n\n...\n...\n...\n...\n...\n...\n\n\n22355\n2025-06-23\ngasoline\npremium\nall\n4.128\n\n\n22356\n2025-06-23\ngasoline\npremium\nconventional\n3.950\n\n\n22357\n2025-06-23\ngasoline\npremium\nreformulated\n4.333\n\n\n22358\n2025-06-23\ndiesel\nall\nNaN\n3.775\n\n\n22359\n2025-06-23\ndiesel\nultra_low_sulfur\nNaN\n3.775\n\n\n\n\n22360 rows × 5 columns\n\n\n\n\nweekly_gas_prices['year'] = weekly_gas_prices['date'].dt.year\n\n\n# create a new col fuel_grade by combining values from fuel and grade columns\nweekly_gas_prices['fuel_grade'] = weekly_gas_prices['fuel'] + '-' + weekly_gas_prices['grade']\nweekly_gas_prices\n\n\n\n\n\n\n\n\ndate\nfuel\ngrade\nformulation\nprice\nyear\nfuel_grade\n\n\n\n\n0\n1990-08-20\ngasoline\nregular\nall\n1.191\n1990\ngasoline-regular\n\n\n1\n1990-08-20\ngasoline\nregular\nconventional\n1.191\n1990\ngasoline-regular\n\n\n2\n1990-08-27\ngasoline\nregular\nall\n1.245\n1990\ngasoline-regular\n\n\n3\n1990-08-27\ngasoline\nregular\nconventional\n1.245\n1990\ngasoline-regular\n\n\n4\n1990-09-03\ngasoline\nregular\nall\n1.242\n1990\ngasoline-regular\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n22355\n2025-06-23\ngasoline\npremium\nall\n4.128\n2025\ngasoline-premium\n\n\n22356\n2025-06-23\ngasoline\npremium\nconventional\n3.950\n2025\ngasoline-premium\n\n\n22357\n2025-06-23\ngasoline\npremium\nreformulated\n4.333\n2025\ngasoline-premium\n\n\n22358\n2025-06-23\ndiesel\nall\nNaN\n3.775\n2025\ndiesel-all\n\n\n22359\n2025-06-23\ndiesel\nultra_low_sulfur\nNaN\n3.775\n2025\ndiesel-ultra_low_sulfur\n\n\n\n\n22360 rows × 7 columns"
  },
  {
    "objectID": "posts/Gas_prices/gas_prices.html#tidytuesday-data-for-2025-07-1",
    "href": "posts/Gas_prices/gas_prices.html#tidytuesday-data-for-2025-07-1",
    "title": "Gas prices in the US",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\n\nweekly_gas_prices = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-01/weekly_gas_prices.csv')\n\n\nweekly_gas_prices['date'] = pd.to_datetime(weekly_gas_prices['date'])\n\n\nweekly_gas_prices\n\n\n\n\n\n\n\n\ndate\nfuel\ngrade\nformulation\nprice\n\n\n\n\n0\n1990-08-20\ngasoline\nregular\nall\n1.191\n\n\n1\n1990-08-20\ngasoline\nregular\nconventional\n1.191\n\n\n2\n1990-08-27\ngasoline\nregular\nall\n1.245\n\n\n3\n1990-08-27\ngasoline\nregular\nconventional\n1.245\n\n\n4\n1990-09-03\ngasoline\nregular\nall\n1.242\n\n\n...\n...\n...\n...\n...\n...\n\n\n22355\n2025-06-23\ngasoline\npremium\nall\n4.128\n\n\n22356\n2025-06-23\ngasoline\npremium\nconventional\n3.950\n\n\n22357\n2025-06-23\ngasoline\npremium\nreformulated\n4.333\n\n\n22358\n2025-06-23\ndiesel\nall\nNaN\n3.775\n\n\n22359\n2025-06-23\ndiesel\nultra_low_sulfur\nNaN\n3.775\n\n\n\n\n22360 rows × 5 columns\n\n\n\n\nweekly_gas_prices['year'] = weekly_gas_prices['date'].dt.year\n\n\n# create a new col fuel_grade by combining values from fuel and grade columns\nweekly_gas_prices['fuel_grade'] = weekly_gas_prices['fuel'] + '-' + weekly_gas_prices['grade']\nweekly_gas_prices\n\n\n\n\n\n\n\n\ndate\nfuel\ngrade\nformulation\nprice\nyear\nfuel_grade\n\n\n\n\n0\n1990-08-20\ngasoline\nregular\nall\n1.191\n1990\ngasoline-regular\n\n\n1\n1990-08-20\ngasoline\nregular\nconventional\n1.191\n1990\ngasoline-regular\n\n\n2\n1990-08-27\ngasoline\nregular\nall\n1.245\n1990\ngasoline-regular\n\n\n3\n1990-08-27\ngasoline\nregular\nconventional\n1.245\n1990\ngasoline-regular\n\n\n4\n1990-09-03\ngasoline\nregular\nall\n1.242\n1990\ngasoline-regular\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n22355\n2025-06-23\ngasoline\npremium\nall\n4.128\n2025\ngasoline-premium\n\n\n22356\n2025-06-23\ngasoline\npremium\nconventional\n3.950\n2025\ngasoline-premium\n\n\n22357\n2025-06-23\ngasoline\npremium\nreformulated\n4.333\n2025\ngasoline-premium\n\n\n22358\n2025-06-23\ndiesel\nall\nNaN\n3.775\n2025\ndiesel-all\n\n\n22359\n2025-06-23\ndiesel\nultra_low_sulfur\nNaN\n3.775\n2025\ndiesel-ultra_low_sulfur\n\n\n\n\n22360 rows × 7 columns"
  },
  {
    "objectID": "posts/Gas_prices/gas_prices.html#plotting",
    "href": "posts/Gas_prices/gas_prices.html#plotting",
    "title": "Gas prices in the US",
    "section": "Plotting",
    "text": "Plotting\n\nfig, ax = plt.subplots(figsize=(8, 4))\n\nintervals = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]  \ncolors = ['#D6ECF3', '#87CEEB', '#1E90FF', '#0000CD', 'red']\n\ncmap = mcolors.ListedColormap(colors)\nnorm = mcolors.BoundaryNorm(intervals, cmap.N)\n\ndf_max_price = weekly_gas_prices.groupby(['year', 'fuel_grade'])['price'].max().reset_index()\n\nfuel_cat = ['gasoline-all', 'gasoline-premium', 'gasoline-midgrade', 'gasoline-regular', 'diesel-all', 'diesel-ultra_low_sulfur', 'diesel-low_sulfur']\ndf_max_price['fuel_grade'] = pd.Categorical(df_max_price['fuel_grade'], categories=fuel_cat, ordered=True)\n# sort by year and fuel_grade\ndf_max_price = df_max_price.sort_values(by=['fuel_grade'])\n\n# remove rows where fuel_grade contains \"all\"\ndf_max_price = df_max_price[~df_max_price['fuel_grade'].str.contains('all')]\n\ns1 = ax.scatter(\n    df_max_price['year'], \n    df_max_price['fuel_grade'], \n    c=df_max_price['price'], \n    cmap=cmap, \n    norm=norm,\n    marker='s'\n)\n\nax.invert_yaxis()\n\ncbar_ax = inset_axes(ax,\n                     width=\"50%\",  # Width as a percent of the parent axis\n                     height=\"5%\",  # Height as a percent of the parent axis\n                     loc='lower center',  # Position inside the main plot\n                     bbox_to_anchor=(-0.025, 0.15, 0.5, 0.5),\n                     bbox_transform=ax.transAxes)\n\ncbar = plt.colorbar(s1, cax=cbar_ax, shrink=0.5, orientation='horizontal')\ncbar.set_ticklabels([f'${interval:.0f}' for interval in intervals])\ncbar.outline.set_visible(False)\ncbar.set_label('Price per gallon', fontsize=8)\ncbar.ax.xaxis.set_label_position('top')   \ncbar.ax.tick_params(length=0, labelsize=8)  \n\nfor label in ax.get_yticklabels():\n    if \"diesel\" in label.get_text():\n        label.set_color('#666666')\n\nax.set_yticks(ax.get_yticks())\nax.set_yticklabels([k.get_text().split('-')[1] for k in ax.get_yticklabels()])\n\nax.text(0.01, 0.925, '\\n'.join(\"GASOLINE\"), transform=ax.transAxes, fontsize=10,\n        verticalalignment='top', fontfamily='Consolas')\nax.text(0.01, 0.30, '\\n'.join(\"DIESEL\"), transform=ax.transAxes, fontsize=10,\n        verticalalignment='top', color='#666666', fontfamily='Consolas')\nax.spines[['top', 'right']].set_visible(False)\n\n\n\n# Label max and min for each fuel_grade\nfor grade in df_max_price['fuel_grade'].unique():\n    subset = df_max_price[df_max_price['fuel_grade'] == grade]\n\n    # Skip if empty or NaNs\n    if subset.empty or subset['price'].isnull().all():\n        continue\n\n    max_row = subset.loc[subset['price'].idxmax()]\n    min_row = subset.loc[subset['price'].idxmin()]\n\n    arrow = dict(arrowstyle='&lt;-', color='black', linewidth=0.8)\n\n    # Annotate max\n    ax.annotate(\n        f\"${max_row['price']:.2f}\",\n        xy=(max_row['year'], max_row['fuel_grade']),\n        xytext=(0, 13),  \n        textcoords='offset points',\n        ha='center', va='bottom',\n        fontsize=8, color='red',\n        arrowprops=arrow\n    )\n    # Annotate min\n    ax.annotate(\n        f\"${min_row['price']:.2f}\",\n        xy=(min_row['year'], min_row['fuel_grade']),\n        xytext=(0, -20),  \n        textcoords='offset points',\n        ha='center', va='bottom',\n        fontsize=8, color='blue',\n        arrowprops=arrow\n    )\nfig.suptitle('Year-wise maximum fuel prices in the US across five different categories. \\n For each series, the highest and lowest prices are labeled.',x=0, ha='left', fontfamily='Serif')\nplt.savefig('gas_prices.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "posts/Income_inequality/income_inequality.html",
    "href": "posts/Income_inequality/income_inequality.html",
    "title": "Income Inequality Before and After Taxes",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\nimport textwrap\n\n\nincome_inequality_processed = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-05/income_inequality_processed.csv')\nincome_inequality_raw = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-05/income_inequality_raw.csv')\n\n\nincome_inequality_processed\n\n\n\n\n\n\n\n\nEntity\nCode\nYear\ngini_mi_eq\ngini_dhi_eq\n\n\n\n\n0\nAustralia\nAUS\n1989\n0.431\n0.304\n\n\n1\nAustralia\nAUS\n1995\n0.470\n0.311\n\n\n2\nAustralia\nAUS\n2001\n0.481\n0.320\n\n\n3\nAustralia\nAUS\n2003\n0.469\n0.316\n\n\n4\nAustralia\nAUS\n2004\n0.467\n0.316\n\n\n...\n...\n...\n...\n...\n...\n\n\n942\nVietnam\nVNM\n2005\nNaN\n0.369\n\n\n943\nVietnam\nVNM\n2007\nNaN\n0.401\n\n\n944\nVietnam\nVNM\n2009\nNaN\n0.398\n\n\n945\nVietnam\nVNM\n2011\nNaN\n0.364\n\n\n946\nVietnam\nVNM\n2013\nNaN\n0.350\n\n\n\n\n947 rows × 5 columns\n\n\n\n\nincome_inequality_raw\n\n\n\n\n\n\n\n\nEntity\nCode\nYear\ngini_disposable__age_total\ngini_market__age_total\npopulation_historical\nowid_region\n\n\n\n\n0\nAfghanistan\nAFG\n-10000\nNaN\nNaN\n14737.0\nNaN\n\n\n1\nAfghanistan\nAFG\n-9000\nNaN\nNaN\n20405.0\nNaN\n\n\n2\nAfghanistan\nAFG\n-8000\nNaN\nNaN\n28253.0\nNaN\n\n\n3\nAfghanistan\nAFG\n-7000\nNaN\nNaN\n39120.0\nNaN\n\n\n4\nAfghanistan\nAFG\n-6000\nNaN\nNaN\n54166.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n58920\nZimbabwe\nZWE\n2019\nNaN\nNaN\n15271330.0\nNaN\n\n\n58921\nZimbabwe\nZWE\n2020\nNaN\nNaN\n15526837.0\nNaN\n\n\n58922\nZimbabwe\nZWE\n2021\nNaN\nNaN\n15797165.0\nNaN\n\n\n58923\nZimbabwe\nZWE\n2022\nNaN\nNaN\n16069010.0\nNaN\n\n\n58924\nZimbabwe\nZWE\n2023\nNaN\nNaN\n16340778.0\nAfrica\n\n\n\n\n58925 rows × 7 columns\n\n\n\n\nincome_inequality_processed_noNA = income_inequality_processed.dropna(subset=['gini_mi_eq'])\nincome_inequality_processed_noNA['diff'] = income_inequality_processed_noNA['gini_mi_eq'] - income_inequality_processed_noNA['gini_dhi_eq']\ndf1 = income_inequality_processed_noNA.sort_values('Year').groupby('Entity').last()\ndf1 = df1.sort_values('diff', ascending=False)\ndf1\n\n\n\n\n\n\n\n\nCode\nYear\ngini_mi_eq\ngini_dhi_eq\ndiff\n\n\nEntity\n\n\n\n\n\n\n\n\n\nBelgium\nBEL\n2021\n0.486\n0.255\n0.231\n\n\nItaly\nITA\n2020\n0.563\n0.335\n0.228\n\n\nIreland\nIRL\n2021\n0.514\n0.290\n0.224\n\n\nAustria\nAUT\n2022\n0.494\n0.287\n0.207\n\n\nGermany\nDEU\n2020\n0.506\n0.302\n0.204\n\n\nNorway\nNOR\n2004\n0.452\n0.261\n0.191\n\n\nCzechia\nCZE\n2016\n0.444\n0.254\n0.190\n\n\nDenmark\nDNK\n2022\n0.477\n0.288\n0.189\n\n\nGreece\nGRC\n2021\n0.488\n0.308\n0.180\n\n\nUnited Kingdom\nGBR\n2021\n0.479\n0.302\n0.177\n\n\nSweden\nSWE\n2021\n0.458\n0.285\n0.173\n\n\nRomania\nROU\n2021\n0.463\n0.295\n0.168\n\n\nSpain\nESP\n2022\n0.481\n0.314\n0.167\n\n\nLithuania\nLTU\n2021\n0.518\n0.366\n0.152\n\n\nSlovakia\nSVK\n2018\n0.387\n0.236\n0.151\n\n\nLuxembourg\nLUX\n2021\n0.434\n0.288\n0.146\n\n\nCanada\nCAN\n2021\n0.436\n0.291\n0.145\n\n\nBulgaria\nBGR\n2022\n0.517\n0.373\n0.144\n\n\nNetherlands\nNLD\n2021\n0.413\n0.270\n0.143\n\n\nEstonia\nEST\n2016\n0.456\n0.314\n0.142\n\n\nAustralia\nAUS\n2020\n0.459\n0.325\n0.134\n\n\nIsrael\nISR\n2021\n0.472\n0.345\n0.127\n\n\nFinland\nFIN\n2016\n0.382\n0.258\n0.124\n\n\nJapan\nJPN\n2020\n0.423\n0.305\n0.118\n\n\nUnited States\nUSA\n2023\n0.507\n0.392\n0.115\n\n\nBrazil\nBRA\n2015\n0.555\n0.446\n0.109\n\n\nSwitzerland\nCHE\n2019\n0.401\n0.310\n0.091\n\n\nSouth Africa\nZAF\n2017\n0.706\n0.616\n0.090\n\n\nIceland\nISL\n2017\n0.326\n0.251\n0.075\n\n\nDominican Republic\nDOM\n2007\n0.523\n0.515\n0.008\n\n\n\n\n\n\n\n\n\n\ncutoff = 0.460\nset1 = df1[df1['gini_mi_eq']&gt;=cutoff]\nset2 = df1[df1['gini_mi_eq']&lt;cutoff]\n\n\nx = [0,1]\n\ncategories1 = sorted(set(set1['gini_mi_eq']).union(set(set1['gini_dhi_eq'])))\ncategories2 = sorted(set(set2['gini_mi_eq']).union(set(set2['gini_dhi_eq'])))\ncategories = categories1 + categories2\n\nfig, (ax1,ax2) = plt.subplots(1,2,figsize=(6, 8),sharey=True)\nfor ind, (row_ind, row) in enumerate(set1.iterrows()):\n    ax1.plot(x, [row['gini_mi_eq'], row['gini_dhi_eq']], marker='o', label=row_ind, alpha=0.6)\n    if (row_ind in ['Germany', 'Denmark']):\n        ax1.annotate(row_ind, (0.5, row['gini_dhi_eq']))\n    elif (row_ind in ['Austria']):\n        ax1.annotate(row_ind, (1.01, row['gini_dhi_eq']-0.01))\n    else:\n        ax1.annotate(row_ind, (1.01, row['gini_dhi_eq']))\n\nfor ind, (row_ind, row) in enumerate(set2.iterrows()):\n    ax2.plot(x, [row['gini_mi_eq'], row['gini_dhi_eq']], marker='o', label=row_ind, alpha=0.6)\n    if (row_ind in ['Switzerland', 'Luxembourg', 'Finland', 'Iceland']):\n        ax2.annotate(row_ind, (0.4, row['gini_dhi_eq']))\n    else:\n        ax2.annotate(row_ind, (1.01, row['gini_dhi_eq']))\n\nax1.spines[['top', 'right', 'bottom', 'left']].set_visible(False)\nax2.spines[['top', 'right', 'bottom', 'left']].set_visible(False)\n\nax1.set_xticks([])\nax2.set_xticks([])\n\nax1.tick_params(axis='y', pad=15)\nax1.tick_params(axis='y', length=0)\nax2.tick_params(axis='y', length=0)\n\n\n#ax1.set_ylabel(r'Income inequality $\\longrightarrow\\!\\!\\!\\!\\!\\!&gt;$', fontsize=12)\n\ntitle_text = textwrap.fill(\"Gini coefficient for different countries pre- (left) and post- (right) tax deductions. Data for the most recent year are shown. The plot is split into two columns at the dotted line for better visualization.\",30)\nfig.text(x=.55,y=.75,s=title_text, fontsize=12, fontfamily='Serif')\n\nax1.annotate(\n    '', \n    xy=(-0.05, 1), xycoords='axes fraction',\n    xytext=(-0.05, 0), textcoords='axes fraction',\n    arrowprops=dict(arrowstyle='-&gt;', lw=10, mutation_scale=70, color='lightgrey'),\n    zorder=-1\n)\n\nax1.annotate(\n    'Income inequality', \n    xy=(-0.09, 1), xycoords='axes fraction',\n    xytext=(-0.09, 0.10), textcoords='axes fraction',\n    rotation=90\n)\n\nplt.tight_layout()\n\ny_display = ax1.transData.transform((0, cutoff))[1]\ny_fig = fig.transFigure.inverted().transform((0, y_display))[1]\n\n# Add horizontal line across the figure at the data-level cutoff\nline = Line2D([0.125, 0.9], [y_fig, y_fig],  # X in figure coords, Y is now in figure coords too\n              transform=fig.transFigure,\n              color='lightgrey', linewidth=1, linestyle='dotted')\n\nfig.add_artist(line)\n\n#plt.savefig(\"income_inequality.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "posts/Income_inequality/income_inequality.html#plotting",
    "href": "posts/Income_inequality/income_inequality.html#plotting",
    "title": "Income Inequality Before and After Taxes",
    "section": "",
    "text": "cutoff = 0.460\nset1 = df1[df1['gini_mi_eq']&gt;=cutoff]\nset2 = df1[df1['gini_mi_eq']&lt;cutoff]\n\n\nx = [0,1]\n\ncategories1 = sorted(set(set1['gini_mi_eq']).union(set(set1['gini_dhi_eq'])))\ncategories2 = sorted(set(set2['gini_mi_eq']).union(set(set2['gini_dhi_eq'])))\ncategories = categories1 + categories2\n\nfig, (ax1,ax2) = plt.subplots(1,2,figsize=(6, 8),sharey=True)\nfor ind, (row_ind, row) in enumerate(set1.iterrows()):\n    ax1.plot(x, [row['gini_mi_eq'], row['gini_dhi_eq']], marker='o', label=row_ind, alpha=0.6)\n    if (row_ind in ['Germany', 'Denmark']):\n        ax1.annotate(row_ind, (0.5, row['gini_dhi_eq']))\n    elif (row_ind in ['Austria']):\n        ax1.annotate(row_ind, (1.01, row['gini_dhi_eq']-0.01))\n    else:\n        ax1.annotate(row_ind, (1.01, row['gini_dhi_eq']))\n\nfor ind, (row_ind, row) in enumerate(set2.iterrows()):\n    ax2.plot(x, [row['gini_mi_eq'], row['gini_dhi_eq']], marker='o', label=row_ind, alpha=0.6)\n    if (row_ind in ['Switzerland', 'Luxembourg', 'Finland', 'Iceland']):\n        ax2.annotate(row_ind, (0.4, row['gini_dhi_eq']))\n    else:\n        ax2.annotate(row_ind, (1.01, row['gini_dhi_eq']))\n\nax1.spines[['top', 'right', 'bottom', 'left']].set_visible(False)\nax2.spines[['top', 'right', 'bottom', 'left']].set_visible(False)\n\nax1.set_xticks([])\nax2.set_xticks([])\n\nax1.tick_params(axis='y', pad=15)\nax1.tick_params(axis='y', length=0)\nax2.tick_params(axis='y', length=0)\n\n\n#ax1.set_ylabel(r'Income inequality $\\longrightarrow\\!\\!\\!\\!\\!\\!&gt;$', fontsize=12)\n\ntitle_text = textwrap.fill(\"Gini coefficient for different countries pre- (left) and post- (right) tax deductions. Data for the most recent year are shown. The plot is split into two columns at the dotted line for better visualization.\",30)\nfig.text(x=.55,y=.75,s=title_text, fontsize=12, fontfamily='Serif')\n\nax1.annotate(\n    '', \n    xy=(-0.05, 1), xycoords='axes fraction',\n    xytext=(-0.05, 0), textcoords='axes fraction',\n    arrowprops=dict(arrowstyle='-&gt;', lw=10, mutation_scale=70, color='lightgrey'),\n    zorder=-1\n)\n\nax1.annotate(\n    'Income inequality', \n    xy=(-0.09, 1), xycoords='axes fraction',\n    xytext=(-0.09, 0.10), textcoords='axes fraction',\n    rotation=90\n)\n\nplt.tight_layout()\n\ny_display = ax1.transData.transform((0, cutoff))[1]\ny_fig = fig.transFigure.inverted().transform((0, y_display))[1]\n\n# Add horizontal line across the figure at the data-level cutoff\nline = Line2D([0.125, 0.9], [y_fig, y_fig],  # X in figure coords, Y is now in figure coords too\n              transform=fig.transFigure,\n              color='lightgrey', linewidth=1, linestyle='dotted')\n\nfig.add_artist(line)\n\n#plt.savefig(\"income_inequality.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "posts/Measles_cases/Measles_cases.html",
    "href": "posts/Measles_cases/Measles_cases.html",
    "title": "Measles cases across the world",
    "section": "",
    "text": "TidyTuesday data for 2025-06-24\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncases_month = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-24/cases_month.csv')\ncases_year = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-24/cases_year.csv')\ncases_month\n\n\n\n\n\n\n\n\nregion\ncountry\niso3\nyear\nmonth\nmeasles_suspect\nmeasles_clinical\nmeasles_epi_linked\nmeasles_lab_confirmed\nmeasles_total\nrubella_clinical\nrubella_epi_linked\nrubella_lab_confirmed\nrubella_total\ndiscarded\n\n\n\n\n0\nAFR\nAlgeria\nDZA\n2012\n1\n8.0\n6.0\n0.0\n2.0\n8.0\nNaN\nNaN\nNaN\nNaN\n0.0\n\n\n1\nAFR\nAlgeria\nDZA\n2012\n2\n10.0\n10.0\n0.0\n0.0\n10.0\nNaN\nNaN\nNaN\nNaN\n0.0\n\n\n2\nAFR\nAlgeria\nDZA\n2012\n3\n17.0\n17.0\n0.0\n0.0\n17.0\nNaN\nNaN\nNaN\nNaN\n0.0\n\n\n3\nAFR\nAlgeria\nDZA\n2012\n4\n7.0\n5.0\n0.0\n0.0\n5.0\n0.0\n0.0\n1.0\n1.0\n2.0\n\n\n4\nAFR\nAlgeria\nDZA\n2012\n5\n14.0\n11.0\n0.0\n0.0\n11.0\n0.0\n0.0\n3.0\n3.0\n3.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n22775\nWPR\nViet Nam\nVNM\n2024\n10\n379.0\n19.0\n56.0\n256.0\n331.0\n5.0\n0.0\n5.0\n10.0\n48.0\n\n\n22776\nWPR\nViet Nam\nVNM\n2024\n11\n584.0\n37.0\n125.0\n347.0\n509.0\n0.0\n0.0\n1.0\n1.0\n75.0\n\n\n22777\nWPR\nViet Nam\nVNM\n2024\n12\n588.0\n56.0\n134.0\n338.0\n528.0\n0.0\n0.0\n1.0\n1.0\n60.0\n\n\n22778\nWPR\nViet Nam\nVNM\n2025\n1\n156.0\n7.0\n0.0\n124.0\n131.0\nNaN\nNaN\nNaN\nNaN\n25.0\n\n\n22779\nWPR\nViet Nam\nVNM\n2025\n2\n22.0\n0.0\n0.0\n20.0\n20.0\nNaN\nNaN\nNaN\nNaN\n2.0\n\n\n\n\n22780 rows × 15 columns\n\n\n\n\ncases_year\n\n\n\n\n\n\n\n\nregion\ncountry\niso3\nyear\ntotal_population\nannualized_population_most_recent_year_only\ntotal_suspected_measles_rubella_cases\nmeasles_total\nmeasles_lab_confirmed\nmeasles_epi_linked\nmeasles_clinical\nmeasles_incidence_rate_per_1000000_total_population\nrubella_total\nrubella_lab_confirmed\nrubella_epi_linked\nrubella_clinical\nrubella_incidence_rate_per_1000000_total_population\ndiscarded_cases\ndiscarded_non_measles_rubella_cases_per_100000_total_population\n\n\n\n\n0\nAFRO\nAlgeria\nDZA\n2012\n37646166\n37646166\n76.0\n55\n2\n0\n53\n1.46\n13\n13\n0\n0\n0.35\n8.0\n0.02\n\n\n1\nAFRO\nAlgeria\nDZA\n2013\n38414172\n38414172\n85.0\n0\n0\n0\n0\n0.00\n29\n29\n0\n0\n0.75\n56.0\n0.15\n\n\n2\nAFRO\nAlgeria\nDZA\n2014\n39205031\n39205031\n49.0\n0\n0\n0\n0\n0.00\n3\n3\n0\n0\n0.08\n46.0\n0.12\n\n\n3\nAFRO\nAlgeria\nDZA\n2015\n40019529\n40019529\n109.0\n62\n2\n60\n0\n1.55\n2\n2\n0\n0\n0.05\n45.0\n0.11\n\n\n4\nAFRO\nAlgeria\nDZA\n2016\n40850721\n40850721\n93.0\n49\n21\n27\n1\n1.20\n11\n11\n0\n0\n0.27\n33.0\n0.08\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2377\nWPRO\nViet Nam\nVNM\n2021\n98935099\n98935099\n346.0\n180\n33\n0\n147\n1.82\n6\n5\n0\n1\n0.06\n160.0\n0.16\n\n\n2378\nWPRO\nViet Nam\nVNM\n2022\n99680655\n99680655\n585.0\n38\n10\n1\n27\n0.38\n25\n21\n0\n4\n0.25\n522.0\n0.52\n\n\n2379\nWPRO\nViet Nam\nVNM\n2023\n100352192\n100352192\n499.0\n98\n8\n2\n88\n0.98\n42\n34\n0\n8\n0.42\n359.0\n0.36\n\n\n2380\nWPRO\nViet Nam\nVNM\n2024\n100987687\n100987687\n2807.0\n2105\n1500\n412\n193\n20.84\n41\n22\n0\n19\n0.41\n661.0\n0.65\n\n\n2381\nWPRO\nViet Nam\nVNM\n2025\n101598527\n42332720\n178.0\n151\n144\n0\n7\n3.57\n0\n0\n0\n0\n0.00\n27.0\n0.06\n\n\n\n\n2382 rows × 19 columns\n\n\n\n\ncases_month.groupby(\"country\").size().sort_values(ascending=False)\n\ncountry\nPakistan                            162\nAfghanistan                         161\nThailand                            161\nDemocratic Republic of the Congo    161\nCôte d'Ivoire                       161\n                                   ... \nCabo Verde                            7\nSamoa                                 6\nDominica                              5\nTonga                                 3\nMonaco                                3\nLength: 193, dtype: int64\n\n\n\ndf_grp = cases_month.groupby([\"month\",\"year\"])[[\"measles_total\", \"rubella_total\"]].sum().reset_index()\ndf_grp\n\n\n\n\n\n\n\n\nmonth\nyear\nmeasles_total\nrubella_total\n\n\n\n\n0\n1\n2012\n15412.0\n897.0\n\n\n1\n1\n2013\n18572.0\n3990.0\n\n\n2\n1\n2014\n38442.0\n4378.0\n\n\n3\n1\n2015\n26097.0\n2788.0\n\n\n4\n1\n2016\n22264.0\n3596.0\n\n\n...\n...\n...\n...\n...\n\n\n157\n12\n2020\n1631.0\n352.0\n\n\n158\n12\n2021\n3593.0\n639.0\n\n\n159\n12\n2022\n22938.0\n871.0\n\n\n160\n12\n2023\n29330.0\n493.0\n\n\n161\n12\n2024\n11199.0\n1288.0\n\n\n\n\n162 rows × 4 columns\n\n\n\n\nimport matplotlib.font_manager as fm\nmonospace_font = fm.FontProperties(family='monospace')\n\n\nmonths_order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n                'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n\nfig, ax = plt.subplots(figsize=(10, 6))\njitter = 0.15\ndf_grp['month_measles'] = df_grp['month'] - jitter\ndf_grp['month_rubella'] = df_grp['month'] + jitter\n\ns1 = sns.scatterplot(data=df_grp, y=\"month_measles\", x=\"year\", size=\"measles_total\", sizes=(25,250),\n                hue=\"measles_total\", marker=\"o\", palette=\"Oranges\", legend='brief')\n\nhandles1, labels1 = s1.get_legend_handles_labels()\nlabels1 = [f\"{int(x):,}\" for x in labels1]\nlegend1 = ax.legend(handles1, labels1, title=\"Measles Cases\", loc=\"upper left\", \n                    bbox_to_anchor=(1, 0.6), frameon=False)\nax.add_artist(legend1)  \n\ns2 = sns.scatterplot(data=df_grp, y=\"month_rubella\", x=\"year\", size=\"rubella_total\", sizes=(5,50),\n                hue=\"rubella_total\", marker=\"s\", palette=\"Blues\", legend='brief')\n\nall_handles, all_labels = s2.get_legend_handles_labels()\n\nrubella_handles = all_handles[len(handles1):]\nrubella_labels = all_labels[len(labels1):] #[l for h, l in zip(all_handles, all_labels) if h not in handles1]\nrubella_labels = [f\"{int(x):,}\" for x in rubella_labels]\n\nlegend2 = ax.legend(rubella_handles, rubella_labels, title=\"Rubella Cases\", loc=\"upper left\", \n                    bbox_to_anchor=(1, 0.3), frameon=False)\n\ntitle = \"Timeline of global measles and rubella (M&R) cases.\"\nsub_title = \"Contagious diseases like M&R exhibit seasonality, with incidence peaking during the winter/spring transition.\\nRestrictions imposed due to Covid-19 led to a reduction in the spread of M&R viruses.\"\nplt.title(textwrap.fill(title, width=60), fontfamily=\"Serif\", loc='left',pad=35, fontweight=\"bold\")\n#plt.suptitle(textwrap.fill(sub_title, width=90), fontfamily=\"Serif\", y=0.82, x=.05, ha=\"left\", fontsize=10)\nplt.suptitle(sub_title, fontfamily=\"Serif\", y=0.86, x=.05, ha=\"left\", fontsize=10)\nplt.xlabel(\"\")\nplt.ylabel(\"\")\nplt.yticks(ticks=range(1, 13), labels=months_order,fontfamily='monospace')\nplt.xticks(ticks=range(2012,2026),fontfamily='monospace')\nax.invert_yaxis()\nplt.tight_layout()\n#plt.savefig('measles_cases.png', bbox_inches='tight', pad_inches=0.1, dpi=300)\nplt.show()"
  },
  {
    "objectID": "posts/Netflix_viewing/Netflix_viewing.html",
    "href": "posts/Netflix_viewing/Netflix_viewing.html",
    "title": "Netflix viewing data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport re\n\n\nmovies = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-29/movies.csv')\nshows = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-29/shows.csv')\n\n\nmovies\n\n\n\n\n\n\n\n\nsource\nreport\ntitle\navailable_globally\nrelease_date\nhours_viewed\nruntime\nviews\n\n\n\n\n0\n1_What_We_Watched_A_Netflix_Engagement_Report_...\n2025Jan-Jun\nBack in Action\nYes\n2025-01-17\n313000000.0\n1H 54M 0S\n164700000.0\n\n\n1\n1_What_We_Watched_A_Netflix_Engagement_Report_...\n2025Jan-Jun\nSTRAW\nYes\n2025-06-06\n185200000.0\n1H 48M 0S\n102900000.0\n\n\n2\n1_What_We_Watched_A_Netflix_Engagement_Report_...\n2025Jan-Jun\nThe Life List\nYes\n2025-03-28\n198900000.0\n2H 5M 0S\n95500000.0\n\n\n3\n1_What_We_Watched_A_Netflix_Engagement_Report_...\n2025Jan-Jun\nExterritorial\nYes\n2025-04-30\n159000000.0\n1H 49M 0S\n87500000.0\n\n\n4\n1_What_We_Watched_A_Netflix_Engagement_Report_...\n2025Jan-Jun\nHavoc\nYes\n2025-04-25\n154900000.0\n1H 47M 0S\n86900000.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n36116\n4_What_We_Watched_A_Netflix_Engagement_Report_...\n2023Jul-Dec\nالرجل الرابع\nNo\nNaN\n100000.0\n1H 32M 0S\n100000.0\n\n\n36117\n4_What_We_Watched_A_Netflix_Engagement_Report_...\n2023Jul-Dec\n두근두근 내 인생\nNo\nNaN\n100000.0\n1H 52M 0S\n100000.0\n\n\n36118\n4_What_We_Watched_A_Netflix_Engagement_Report_...\n2023Jul-Dec\n라디오 스타\nNo\nNaN\n100000.0\n1H 56M 0S\n100000.0\n\n\n36119\n4_What_We_Watched_A_Netflix_Engagement_Report_...\n2023Jul-Dec\n선생 김봉두\nNo\nNaN\n100000.0\n1H 57M 0S\n100000.0\n\n\n36120\n4_What_We_Watched_A_Netflix_Engagement_Report_...\n2023Jul-Dec\n표적\nNo\nNaN\n100000.0\n1H 38M 0S\n100000.0\n\n\n\n\n36121 rows × 8 columns\n\n\n\n\nshows\n\n\n\n\n\n\n\n\nsource\nreport\ntitle\navailable_globally\nrelease_date\nhours_viewed\nruntime\nviews\n\n\n\n\n0\n1_What_We_Watched_A_Netflix_Engagement_Report_...\n2025Jan-Jun\nAdolescence: Limited Series\nYes\n2025-03-13\n555100000.0\n3H 50M 0S\n144800000.0\n\n\n1\n1_What_We_Watched_A_Netflix_Engagement_Report_...\n2025Jan-Jun\nSquid Game: Season 2 // 오징어 게임: 시즌 2\nYes\n2024-12-26\n840300000.0\n7H 10M 0S\n117300000.0\n\n\n2\n1_What_We_Watched_A_Netflix_Engagement_Report_...\n2025Jan-Jun\nSquid Game: Season 3 // 오징어 게임: 시즌 3\nYes\n2025-06-27\n438600000.0\n6H 8M 0S\n71500000.0\n\n\n3\n1_What_We_Watched_A_Netflix_Engagement_Report_...\n2025Jan-Jun\nZero Day: Limited Series\nYes\n2025-02-20\n315800000.0\n5H 9M 0S\n61300000.0\n\n\n4\n1_What_We_Watched_A_Netflix_Engagement_Report_...\n2025Jan-Jun\nMissing You: Limited Series\nYes\n2025-01-01\n218600000.0\n3H 46M 0S\n58000000.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n27798\n4_What_We_Watched_A_Netflix_Engagement_Report_...\n2023Jul-Dec\nWe Are Black and British: Season 1\nNo\nNaN\n100000.0\n1H 57M 0S\n100000.0\n\n\n27799\n4_What_We_Watched_A_Netflix_Engagement_Report_...\n2023Jul-Dec\nWhitney Cummings: Can I Touch It?\nYes\n2019-07-30\n100000.0\n59M 0S\n100000.0\n\n\n27800\n4_What_We_Watched_A_Netflix_Engagement_Report_...\n2023Jul-Dec\nWhitney Cummings: Jokes\nNo\n2022-07-26\n100000.0\n1H 0M 0S\n100000.0\n\n\n27801\n4_What_We_Watched_A_Netflix_Engagement_Report_...\n2023Jul-Dec\nWhose Vote Counts, Explained: Limited Series\nYes\n2020-09-28\n100000.0\n1H 15M 0S\n100000.0\n\n\n27802\n4_What_We_Watched_A_Netflix_Engagement_Report_...\n2023Jul-Dec\nZach Galifianakis: Live at the Purple Onion\nNo\nNaN\n100000.0\n1H 1M 0S\n100000.0\n\n\n\n\n27803 rows × 8 columns\n\n\n\n\nmovies[\"release_date\"] = pd.to_datetime(movies[\"release_date\"])\nshows[\"release_date\"] = pd.to_datetime(shows[\"release_date\"])\n\n\nprint(movies[\"hours_viewed\"].sum() / (24 * 365))\nprint(shows[\"hours_viewed\"].sum() / (24 * 365))\n\n11504006.849315068\n31113367.579908676\n\n\n\nmovies.groupby('title').views.sum().sort_values(ascending=False).head(10)\n\ntitle\nThe Boss Baby                  221300000.0\nThe Super Mario Bros. Movie    209900000.0\nLeo                            207200000.0\nLeave the World Behind         205100000.0\nDamsel                         194500000.0\nMinions                        190900000.0\nCarry-On                       185000000.0\nLift                           175800000.0\nSing (2016)                    173700000.0\nDr. Seuss' The Grinch          168300000.0\nName: views, dtype: float64\n\n\n\nshows.groupby('title').views.sum().sort_values(ascending=False).head(10)\n\ntitle\nSquid Game: Season 2 // 오징어 게임: 시즌 2    203800000.0\nAdolescence: Limited Series             144800000.0\nFool Me Once: Limited Series            129000000.0\nBridgerton: Season 3                    123300000.0\nBaby Reindeer: Limited Series           100400000.0\nONE PIECE: Season 1                      98400000.0\nThe Gentlemen: Season 1                  97900000.0\nPeppa Pig: Season 6                      92600000.0\nAvatar The Last Airbender: Season 1      86900000.0\nThe Perfect Couple: Season 1             86400000.0\nName: views, dtype: float64\n\n\n\nmovies = movies.loc[~movies[\"available_globally\"].isin([\"Available Globally?\"])]\nshows = shows.loc[~shows[\"available_globally\"].isin([\"Available Globally?\"])]\n\n\nfig, ax = plt.subplots(1,2, figsize=(10,5), sharey=True)\n\n# Removed 'line_kws' argument as it is not valid for sns.scatterplot\nsns.scatterplot(data=movies, x=\"release_date\", y=\"views\", ax=ax[0], alpha=0.5, hue=\"available_globally\")\n# plot median line\nax[0].axhline(movies[\"views\"].quantile(0.99), color=\"orange\", linestyle=\"--\")\nsns.scatterplot(data=shows, x=\"release_date\", y=\"views\", ax=ax[1], alpha=0.6, hue=\"available_globally\")\nax[1].axhline(shows[\"views\"].quantile(0.99), color=\"orange\", linestyle=\"--\")\n\n# add labels to top 5 movies and shows\ntop5_m = movies.groupby('title').agg({'views': 'sum', 'release_date': 'first'}).sort_values(by='views', ascending=False).head(5)\ntop5_s = shows.groupby('title').agg({'views': 'sum', 'release_date': 'first'}).sort_values(by='views', ascending=False).head(5)\n\n# Filter out rows with None in 'release_date'\ntop5_m = top5_m[top5_m['release_date'].notna()]\ntop5_s = top5_s[top5_s['release_date'].notna()]\n\n#print(top5_m)\n#print(top5_s)\n\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n# make word cloud of movie titles\n\nmovies[\"title_mod\"] = movies[\"title\"].str.replace(\"Movie\", \"\")\nmovies_title = \" \".join([str(x) for x in movies[\"title_mod\"]])\nmovies_wordcloud = WordCloud().generate(movies_title)\nplt.imshow(movies_wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# remove non-english characters\npattern = r'[^a-zA-Z0-9\\s]' \nshows[\"title_mod\"] = shows[\"title\"].str.replace(pattern, '', regex=True)\n\n\nst = shows[\"title_mod\"].str.replace(r\"(Season|Limited Series|Temporada|Serie)\", \"\", regex=True)\n\nshows_title = \" \".join([str(x) for x in st])\nshows_title.replace(\"Season\", \"\")\nshows_wordcloud = WordCloud().generate(shows_title)\nplt.imshow(shows_wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig,ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].imshow(movies_wordcloud, interpolation='bilinear')\nax[1].imshow(shows_wordcloud, interpolation='bilinear')\n\nax[0].axis('off')\nax[1].axis('off')\nplt.tight_layout()\n\nfig.suptitle(\"Word cloud of movie (left) and show (right) titles in the Netflix viewing data \\nfrom late 2023 through the first half of 2025.\", \n                ha=\"left\", x=0.025, y=0.85, fontsize=16, color='white')\nfig.set_facecolor('black')\n#plt.savefig(\"wordcloud.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\n\nwith open(\"wordcloud.svg\", \"w\", encoding=\"utf-8\") as f:\n            f.write(movies_wordcloud.to_svg())"
  },
  {
    "objectID": "posts/Netflix_viewing/Netflix_viewing.html#word-cloud",
    "href": "posts/Netflix_viewing/Netflix_viewing.html#word-cloud",
    "title": "Netflix viewing data",
    "section": "",
    "text": "# make word cloud of movie titles\n\nmovies[\"title_mod\"] = movies[\"title\"].str.replace(\"Movie\", \"\")\nmovies_title = \" \".join([str(x) for x in movies[\"title_mod\"]])\nmovies_wordcloud = WordCloud().generate(movies_title)\nplt.imshow(movies_wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# remove non-english characters\npattern = r'[^a-zA-Z0-9\\s]' \nshows[\"title_mod\"] = shows[\"title\"].str.replace(pattern, '', regex=True)\n\n\nst = shows[\"title_mod\"].str.replace(r\"(Season|Limited Series|Temporada|Serie)\", \"\", regex=True)\n\nshows_title = \" \".join([str(x) for x in st])\nshows_title.replace(\"Season\", \"\")\nshows_wordcloud = WordCloud().generate(shows_title)\nplt.imshow(shows_wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig,ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].imshow(movies_wordcloud, interpolation='bilinear')\nax[1].imshow(shows_wordcloud, interpolation='bilinear')\n\nax[0].axis('off')\nax[1].axis('off')\nplt.tight_layout()\n\nfig.suptitle(\"Word cloud of movie (left) and show (right) titles in the Netflix viewing data \\nfrom late 2023 through the first half of 2025.\", \n                ha=\"left\", x=0.025, y=0.85, fontsize=16, color='white')\nfig.set_facecolor('black')\n#plt.savefig(\"wordcloud.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\n\nwith open(\"wordcloud.svg\", \"w\", encoding=\"utf-8\") as f:\n            f.write(movies_wordcloud.to_svg())"
  },
  {
    "objectID": "posts/Scottish_munro/Scottish_Munro.html",
    "href": "posts/Scottish_munro/Scottish_Munro.html",
    "title": "Scottish Munro Classification",
    "section": "",
    "text": "TidyTuesday dataset of 2025-08-19\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nscottish_munros = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-19/scottish_munros.csv', encoding='latin1')\n\n\nscottish_munros\n\n\n\n\n\n\n\n\nDoBIH_number\nName\nHeight_m\nHeight_ft\nxcoord\nycoord\n1891\n1921\n1933\n1953\n1969\n1974\n1981\n1984\n1990\n1997\n2021\nComments\n\n\n\n\n0\n1\nBen Chonzie\n931.0\n3054.0\n277324.0\n730857.0\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nNaN\n\n\n1\n17\nBen Vorlich\n985.3\n3233.0\n262912.0\n718916.0\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nNaN\n\n\n2\n18\nStuc a' Chroin\n973.0\n3192.0\n261746.0\n717465.0\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nNaN\n\n\n3\n32\nBen Lomond\n973.7\n3195.0\n236707.0\n702863.0\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nNaN\n\n\n4\n26\nBen More\n1174.0\n3852.0\n243276.0\n724417.0\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n599\n1258\nSgurr nan Eag\n926.3\n3039.0\n145705.0\n819536.0\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nNaN\n\n\n600\n1255\nBlabheinn [Bla Bheinn]\n929.0\n3048.0\n152990.0\n821743.0\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\n1891: Blath Bheinn (Blaven), N Top; 1921-1969:...\n\n\n601\n1257\nBlabheinn South Top\n926.5\n3040.0\n152855.0\n821529.0\nMunro Top\nNaN\nNaN\nNaN\nNaN\nNaN\nMunro Top\nMunro Top\nMunro Top\nMunro Top\nMunro Top\n1891: Blath Bheinn (Blaven), S Top; 1981-1990:...\n\n\n602\n1301\nBen More\n966.0\n3169.0\n152576.0\n733078.0\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nMunro\nNaN\n\n\n603\nTen-figure grid references suitable for Garmin...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n604 rows × 18 columns\n\n\n\n\nscottish_munros.columns\n\nIndex(['DoBIH_number', 'Name', 'Height_m', 'Height_ft', 'xcoord', 'ycoord',\n       '1891', '1921', '1933', '1953', '1969', '1974', '1981', '1984', '1990',\n       '1997', '2021', 'Comments'],\n      dtype='object')\n\n\n\nscottish_munros.groupby('1891').count()\n\n\n\n\n\n\n\n\nDoBIH_number\nName\nHeight_m\nHeight_ft\nxcoord\nycoord\n1921\n1933\n1953\n1969\n1974\n1981\n1984\n1990\n1997\n2021\nComments\n\n\n1891\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMunro\n283\n283\n283\n283\n283\n283\n279\n279\n278\n278\n277\n279\n280\n280\n277\n277\n106\n\n\nMunro Top\n255\n255\n255\n255\n255\n255\n242\n242\n242\n242\n233\n196\n194\n194\n191\n190\n142\n\n\n\n\n\n\n\n\nsns.histplot(scottish_munros, x='Height_ft', hue='1891', multiple='stack', bins=20)\nplt.show()\n\n\n\n\n\n\n\n\n\nscottish_munros_long = scottish_munros.melt(id_vars=['DoBIH_number', 'Name', 'Height_m', 'Height_ft', 'xcoord', 'ycoord', 'Comments'], var_name='year', value_name='munro_type')\nmunro_counts = scottish_munros_long.groupby(['year', 'munro_type']).size()\n\n\nsns.set_style(\"dark\", {\n    'axes.facecolor': 'darkgrey',     # background of the plot\n    'figure.facecolor': 'darkgrey',   # background around the plot\n    'axes.edgecolor': 'gainsboro',\n    'font.family': 'monospace',\n})\nsns.set_context(\"notebook\", font_scale=2)\n\ng = sns.displot(data=scottish_munros_long, x='Height_ft', hue='munro_type', col='year', \\\ncol_wrap=6, kind='hist', multiple='dodge', palette=['lightgreen','white'], \\\nheight=3, aspect=1.5, bins=20, edgecolor='none')\n#sns.move_legend(g,\"lower right\", bbox_to_anchor=(0.875, 0.2), title=None, frameon=False)\ng.legend.set_visible(False)\ng.set_axis_labels(\"\", \"\")\ng.figure.suptitle(\"Year-wise distribution of heights for Scottish Munros and Munro Tops\",\\\nx=0.05, y=1.025, ha='left', fontfamily='serif', fontsize=36)\ng.figure.subplots_adjust(wspace=0, hspace=0)\ntext_obj = g.figure.supylabel('Count')\nx, y = text_obj.get_position()   # get current position\ntext_obj.set_position((x - 0.01, y))  \ng.figure.supxlabel('Height (feet)')\n\ncustom_ticks = [3000, 3500, 4000]\ncustom_labels = ['3K', '3.5', '4K']\n\n# Set the ticks and labels on each axis in the grid\nfor ax in g.axes.flat:\n    ax.set_xticks(custom_ticks)\n    ax.set_xticklabels(custom_labels)\n    title_text = ax.get_title()\n    year = title_text.split('=')[-1].strip()\n    year_counts = munro_counts.loc[year]\n    for i, (mt, m_count) in enumerate(year_counts.items()):\n        k=0\n        if(i==0):\n            if(ax == g.axes.flat[-1]):\n                m_count = f'{m_count} ← Total Munros'\n                k = 0.6\n            ax.text(0.9 + k, 0.5 - i * 0.1, f'{m_count}', transform=ax.transAxes, ha='right', va='center', color='lightgreen')\n        else:\n            if(ax == g.axes.flat[-1]):\n                m_count = f'{m_count} ← Total Munro Tops'\n                k = 0.8\n            ax.text(0.9 + k, 0.5 - i * 0.2, f'{m_count}', transform=ax.transAxes, ha='right', va='center', color='white')    \n\ng.set_titles(\"{col_name}\", y=0.7)\ng.figure.text(0.90, 0.01, '1K feet = 304.8 meters', ha='right', color='gainsboro')\n\nsns.despine()\n#g.figure.savefig('Scottish_munros.png', dpi=300, bbox_inches='tight',pad_inches=0.25)\nplt.show()"
  },
  {
    "objectID": "posts/Swim_sites_Sydney/swim_sites.html",
    "href": "posts/Swim_sites_Sydney/swim_sites.html",
    "title": "Water Quality at Sydney Beaches",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggtext)\n\n\nwater_quality &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-20/water_quality.csv')\nwater_quality\n\n# A tibble: 123,530 × 10\n   region         council       swim_site date       time  enterococci_cfu_100ml\n   &lt;chr&gt;          &lt;chr&gt;         &lt;chr&gt;     &lt;date&gt;     &lt;tim&gt;                 &lt;dbl&gt;\n 1 Western Sydney Hawkesbury C… Windsor … 2025-04-28 11:00                   620\n 2 Sydney Harbour North Sydney… Hayes St… 2025-04-28 11:40                    64\n 3 Sydney Harbour Willoughby C… Northbri… 2025-04-28 10:54                   160\n 4 Sydney Harbour Northern Bea… Fairligh… 2025-04-28 09:28                    54\n 5 Western Sydney Hawkesbury C… Yarramun… 2025-04-28 10:35                   720\n 6 Sydney Harbour Northern Bea… Little M… 2025-04-28 09:19                   230\n 7 Sydney Harbour City of Cana… Chiswick… 2025-04-28 13:06                   120\n 8 Sydney Harbour Inner West C… Dawn Fra… 2025-04-28 08:04                   280\n 9 Sydney Harbour Woollahra Mu… Rose Bay… 2025-04-28 08:50                    60\n10 Sydney Harbour Woollahra Mu… Camp Cove 2025-04-28 09:09                   100\n# ℹ 123,520 more rows\n# ℹ 4 more variables: water_temperature_c &lt;dbl&gt;, conductivity_ms_cm &lt;dbl&gt;,\n#   latitude &lt;dbl&gt;, longitude &lt;dbl&gt;\n\n\n\nglimpse(water_quality)\n\nRows: 123,530\nColumns: 10\n$ region                &lt;chr&gt; \"Western Sydney\", \"Sydney Harbour\", \"Sydney Harb…\n$ council               &lt;chr&gt; \"Hawkesbury City Council\", \"North Sydney Council…\n$ swim_site             &lt;chr&gt; \"Windsor Beach\", \"Hayes Street Beach\", \"Northbri…\n$ date                  &lt;date&gt; 2025-04-28, 2025-04-28, 2025-04-28, 2025-04-28,…\n$ time                  &lt;time&gt; 11:00:00, 11:40:00, 10:54:00, 09:28:00, 10:35:0…\n$ enterococci_cfu_100ml &lt;dbl&gt; 620, 64, 160, 54, 720, 230, 120, 280, 60, 100, 1…\n$ water_temperature_c   &lt;dbl&gt; 20, 21, 21, 21, 18, 21, 21, 21, 22, 22, 20, 20, …\n$ conductivity_ms_cm    &lt;dbl&gt; 248, 45250, 48930, 52700, 64, 39140, 4845, 50600…\n$ latitude              &lt;dbl&gt; -33.60448, -33.84172, -33.80604, -33.80073, -33.…\n$ longitude             &lt;dbl&gt; 150.8170, 151.2194, 151.2228, 151.2748, 150.6979…"
  },
  {
    "objectID": "posts/Swim_sites_Sydney/swim_sites.html#tidytuesday-data-for-2025-05-20",
    "href": "posts/Swim_sites_Sydney/swim_sites.html#tidytuesday-data-for-2025-05-20",
    "title": "Water Quality at Sydney Beaches",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggtext)\n\n\nwater_quality &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-20/water_quality.csv')\nwater_quality\n\n# A tibble: 123,530 × 10\n   region         council       swim_site date       time  enterococci_cfu_100ml\n   &lt;chr&gt;          &lt;chr&gt;         &lt;chr&gt;     &lt;date&gt;     &lt;tim&gt;                 &lt;dbl&gt;\n 1 Western Sydney Hawkesbury C… Windsor … 2025-04-28 11:00                   620\n 2 Sydney Harbour North Sydney… Hayes St… 2025-04-28 11:40                    64\n 3 Sydney Harbour Willoughby C… Northbri… 2025-04-28 10:54                   160\n 4 Sydney Harbour Northern Bea… Fairligh… 2025-04-28 09:28                    54\n 5 Western Sydney Hawkesbury C… Yarramun… 2025-04-28 10:35                   720\n 6 Sydney Harbour Northern Bea… Little M… 2025-04-28 09:19                   230\n 7 Sydney Harbour City of Cana… Chiswick… 2025-04-28 13:06                   120\n 8 Sydney Harbour Inner West C… Dawn Fra… 2025-04-28 08:04                   280\n 9 Sydney Harbour Woollahra Mu… Rose Bay… 2025-04-28 08:50                    60\n10 Sydney Harbour Woollahra Mu… Camp Cove 2025-04-28 09:09                   100\n# ℹ 123,520 more rows\n# ℹ 4 more variables: water_temperature_c &lt;dbl&gt;, conductivity_ms_cm &lt;dbl&gt;,\n#   latitude &lt;dbl&gt;, longitude &lt;dbl&gt;\n\n\n\nglimpse(water_quality)\n\nRows: 123,530\nColumns: 10\n$ region                &lt;chr&gt; \"Western Sydney\", \"Sydney Harbour\", \"Sydney Harb…\n$ council               &lt;chr&gt; \"Hawkesbury City Council\", \"North Sydney Council…\n$ swim_site             &lt;chr&gt; \"Windsor Beach\", \"Hayes Street Beach\", \"Northbri…\n$ date                  &lt;date&gt; 2025-04-28, 2025-04-28, 2025-04-28, 2025-04-28,…\n$ time                  &lt;time&gt; 11:00:00, 11:40:00, 10:54:00, 09:28:00, 10:35:0…\n$ enterococci_cfu_100ml &lt;dbl&gt; 620, 64, 160, 54, 720, 230, 120, 280, 60, 100, 1…\n$ water_temperature_c   &lt;dbl&gt; 20, 21, 21, 21, 18, 21, 21, 21, 22, 22, 20, 20, …\n$ conductivity_ms_cm    &lt;dbl&gt; 248, 45250, 48930, 52700, 64, 39140, 4845, 50600…\n$ latitude              &lt;dbl&gt; -33.60448, -33.84172, -33.80604, -33.80073, -33.…\n$ longitude             &lt;dbl&gt; 150.8170, 151.2194, 151.2228, 151.2748, 150.6979…"
  },
  {
    "objectID": "posts/Swim_sites_Sydney/swim_sites.html#data-wrangling",
    "href": "posts/Swim_sites_Sydney/swim_sites.html#data-wrangling",
    "title": "Water Quality at Sydney Beaches",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nwq_grp &lt;- water_quality %&gt;%\n  filter(!is.na(enterococci_cfu_100ml)) %&gt;% \n  mutate(date_y = format(date, \"%Y\")) %&gt;% \n  group_by(date_y, swim_site) %&gt;% \n  summarise(mean_enterococci = mean(enterococci_cfu_100ml, na.rm = TRUE)) %&gt;% \n  slice_min(order_by = mean_enterococci, n = 5)\n\nsite_freq &lt;- wq_grp %&gt;%\n  ungroup() %&gt;% \n  count(swim_site, sort = TRUE)"
  },
  {
    "objectID": "posts/Swim_sites_Sydney/swim_sites.html#plotting",
    "href": "posts/Swim_sites_Sydney/swim_sites.html#plotting",
    "title": "Water Quality at Sydney Beaches",
    "section": "Plotting",
    "text": "Plotting\n\nwq_grp%&gt;% \n  mutate(swim_site = fct_rev(factor(swim_site, levels = site_freq$swim_site))) %&gt;% \n  ggplot(aes(x = date_y, y=swim_site, fill=mean_enterococci)) +\n  scale_fill_gradient(low = \"lightblue\", high = \"blue\") +\n  geom_tile(color=\"white\") +\n  labs(\n    title = \"Year-wise top five swim-sites based on the lowest *Enterococci* concentration\",\n    x = element_blank(),\n    y = element_blank(),\n    fill = \"Average *Enterococci* &lt;br&gt; CFU/100ml\",\n    caption = \"Colony Forming Units (CFU) indicate number of viable bacteria that can form colonies when grown in a lab.\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(angle = 90, hjust = 1),\n    plot.title = element_textbox_simple(\n      padding = margin(5.5, 5.5, 5.5, 5.5),\n      margin = margin(5, 0, 10, 0)),\n    legend.title = element_markdown(hjust = 0.5),\n    legend.position = \"inside\",\n    legend.position.inside = c(0.80, 0.1),\n    legend.title.position = \"top\",\n    plot.caption = element_textbox_simple(\n      margin = margin(0, 0, 10, 175),\n      padding = margin(5.5, 0, 0, 0),\n      hjust = 0,\n      size = 8,\n      color = \"gray40\",\n      lineheight = 1.2\n    ),\n    axis.title.x = element_text(family = \"Consolas\"),\n    ) +\n  guides(fill = guide_colorbar(direction = \"horizontal\", barheight=.5)) +\n   coord_fixed(ratio = 1) \n\n\n\n\n\n\n\n#ggsave(\"swim_sites2.png\", width = 8, height = 6, dpi = 300, bg = \"white\")"
  },
  {
    "objectID": "posts/Weather_attribution/weather_attribution.html",
    "href": "posts/Weather_attribution/weather_attribution.html",
    "title": "Extreme Weather Attribution Studies",
    "section": "",
    "text": "TidyTuesday dataset of 2025-08-12\n\nimport pandas as pd\nimport plotly.express as px\nimport plotly.subplots as sp\nimport textwrap\n\n\nattribution_studies = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-12/attribution_studies.csv')\nattribution_studies_raw = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-12/attribution_studies_raw.csv')\n\n\nattribution_studies\n\n\n\n\n\n\n\n\nevent_name\nevent_period\nevent_year\nstudy_focus\niso_country_code\ncb_region\nevent_type\nclassification\nsummary_statement\npublication_year\ncitation\nsource\nrapid_study\nlink\n\n\n\n\n0\nEuropean summer heatwave\n2003\n2003\nEvent\nNaN\nEurope\nHeat\nMore severe or more likely to occur\n\"We estimate it is very likely (confidence lev...\n2004.0\nStott, P. et al., 2004: Human contribution to ...\nNature\nNo\nhttps://www.nature.com/nature/journal/v432/n70...\n\n\n1\nGlobal temperature extremes\nsince 1950\nNaN\nTrend\nNaN\nGlobal\nHeat\nMore severe or more likely to occur\n\"Comparing these observations with climate mod...\n2005.0\nChristidis, N. et al., 2005: Detection of chan...\nGeophysical Research Letters\nNo\nhttps://agupubs.onlinelibrary.wiley.com/doi/fu...\n\n\n2\nRecord warm autumn in Europe\n2006\n2006\nEvent\nNaN\nEurope\nHeat\nMore severe or more likely to occur\n\"Global warming has made a warm autumn like th...\n2007.0\nVan Oldenborgh, G-J. et al., 2007: How unusual...\nClimate of the Past\nNo\nhttp://www.clim-past.net/3/659/2007/cp-3-659-2...\n\n\n3\nIncreasing frequency of 'very warm' Northern h...\n1860-2009\nNaN\nTrend\nNaN\nNorthern hemisphere\nHeat\nMore severe or more likely to occur\n\"We detect the dominant influence of anthropog...\n2007.0\nJones, G. et al., 2007: Human contribution to ...\nJournal of Geophysical Research: Atmospheres\nNo\nhttps://agupubs.onlinelibrary.wiley.com/doi/fu...\n\n\n4\nMoscow summer heatwave\n2010\n2010\nEvent\nRUS\nEurope\nHeat\nMore severe or more likely to occur\n\"For July temperature in Moscow, we estimate.....\n2011.0\nRahmstorf, S. & Coumou, D. 2011: Increase of e...\nProceedings of the National Academy of Sciences\nNo\nhttp://www.pnas.org/content/108/44/17905.abstract\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n739\nChina growing season hot and dry events\nmid 1990s\nMid-1990s\nEvent\nCHN\nEastern and south-eastern Asia\nCompound\nMore severe or more likely to occur\n\"Anthropogenic forcing changes can explain 60%...\n2024.0\nSu Q. et al., 2024: Anthropogenic Influence on...\nAdvances in Atmospheric Sciences\nNo\nhttps://doi.org/10.1007/s00376-023-2319-z\n\n\n740\nChina hot-wet events\n1979-2014\nNaN\nTrend\nCHN\nEastern and south-eastern Asia\nCompound\nDecrease, less severe or less likely to occur\n\"Over the past 40 years, anthropogenic activit...\n2024.0\nYao H. et al., 2024: Changes caused by human a...\nCommunications Earth & Environment\nNo\nhttps://doi.org/10.1038/s43247-024-01625-y\n\n\n741\nHeat extremes behind bleaching of the Great Ba...\n2020, 2024\n2017, 2020, 2024\nEvent\nAUS\nAustralia and New Zealand\nImpact\nMore severe or more likely to occur\n\"Climate model analysis confirms that human in...\n2024.0\nHenley B.J. et al., 2024: Highest ocean heat i...\nNature\nNo\nhttps://doi.org/10.1038/s41586-024-07672-x\n\n\n742\nHeat-related neonatal deaths in low- and middl...\n2001-2019\nNaN\nTrend\nNaN\nGlobal\nImpact\nMore severe or more likely to occur\n\"Climate change was responsible for 32% (range...\n2024.0\nDimitrova. A. et al., 2024: Temperature-relate...\nNature Communications\nNo\nhttps://www.nature.com/articles/s41467-024-498...\n\n\n743\nHospitalisations in China due to heat\n2000-2019\nNaN\nTrend\nCHN\nEastern and south-eastern Asia\nImpact\nMore severe or more likely to occur\n\"In the 2010s, the heat-related attributable f...\n2024.0\nZhou. L. et al,. 2024: Quantification of the H...\nEnvironmental Health Perspectives\nNo\nhttps://ehp.niehs.nih.gov/doi/10.1289/EHP14057\n\n\n\n\n744 rows × 14 columns\n\n\n\n\nattribution_studies['event_type'].value_counts()\n\nevent_type\nHeat                207\nRain & flooding     177\nDrought             106\nStorm                61\nCold, snow & ice     57\nImpact               33\nCompound             33\nWildfire             31\nOceans               25\nAtmosphere            7\nSunshine              4\nRiver flow            3\nName: count, dtype: int64\n\n\n\nattribution_studies['event_name'].value_counts()\n\nevent_name\nEurope heatwave                          13\nCalifornia drought                        7\nNorthern Europe summer heatwave           7\nDamages from UK flooding                  4\nGlobal temperature extremes               4\n                                         ..\nYangtze river extreme rainfall            1\nExtreme flooding in Missouri              1\nFrance floods                             1\nStorm Desmond heavy rains                 1\nHospitalisations in China due to heat     1\nName: count, Length: 678, dtype: int64\n\n\n\nattribution_studies['classification'].value_counts()\n\nclassification\nMore severe or more likely to occur              554\nNo discernible human influence                    71\nDecrease, less severe or less likely to occur     66\nInsufficient data/inconclusive                    53\nName: count, dtype: int64\n\n\n\nclassification_labels = {\n    'More severe or more likely to occur': 'More Severe/Likely',\n    'Decrease, less severe or less likely to occur': 'Less Severe/Likely',\n    'No discernible human influence': 'No human influence',\n    'Insufficient data/inconclusive': 'Insufficient Data'\n}\n\n# sunburst plots\nsunburst_data = attribution_studies.groupby(['study_focus', 'classification', 'event_type']).size().reset_index(name='count')\nsunburst_data['classification'] = sunburst_data['classification'].map(classification_labels)\n\nsunburst_data = sunburst_data.dropna(subset=['classification'])\n\nevent_focus_count = sunburst_data[sunburst_data['study_focus'] == 'Event']['count'].sum()\ntrend_focus_count = sunburst_data[sunburst_data['study_focus'] == 'Trend']['count'].sum()\n\nfig = sp.make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]],\n                       subplot_titles=(f'Event Focus ({event_focus_count})', f'Trend Focus ({trend_focus_count})'))\n\nfig.add_trace(px.sunburst(sunburst_data[sunburst_data['study_focus'] == 'Event'], \n                          path=['classification', 'event_type'], values='count').data[0],\n              row=1, col=1)\n\nfig.add_trace(px.sunburst(sunburst_data[sunburst_data['study_focus'] == 'Trend'], \n                          path=['classification', 'event_type'], values='count').data[0],\n              row=1, col=2)\n\nfig.update_layout(title_text='Effect of climate change on extreme weather attributions &#8211; &lt;br&gt;&lt;span style=\"color:blue\"&gt;More severe&lt;/span&gt;, &lt;span style=\"color:green\"&gt;Less severe&lt;/span&gt;, &lt;span style=\"color:red\"&gt;No human influence&lt;/span&gt;, and &lt;span style=\"color:purple\"&gt;Insufficient data&lt;/span&gt;.')\n\n# Save figure\nfig.write_html('sunburst_plots_by_study_focus.html')\n#fig.write_image(\"weather_attribution.png\")\nfig.show()\n\n                                                \n\n\nwrite_image didn’t work. HTML file was processed in inkscape to generate high resolution image."
  }
]